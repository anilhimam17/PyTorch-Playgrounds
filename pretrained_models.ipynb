{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22a8b342",
   "metadata": {},
   "source": [
    "## PyTorch a Quick Start\n",
    "\n",
    "- By default PyTorch has two primitives to work with data\n",
    "    - The `torch.utils.data.Dataset` is used to store the samples and the corresponding labels.\n",
    "    - The `torch.utils.data.DataLoader` is used to provide a highly custom wrapper around the dataset to aid with loading the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ebb985c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1df2ab32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n",
      "100.0%\n",
      "100.0%\n",
      "100.0%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data:  Dataset FashionMNIST\n",
      "    Number of datapoints: 60000\n",
      "    Root location: data\n",
      "    Split: Train\n",
      "    StandardTransform\n",
      "Transform: ToTensor()\n",
      "Testing Data:  Dataset FashionMNIST\n",
      "    Number of datapoints: 10000\n",
      "    Root location: data\n",
      "    Split: Test\n",
      "    StandardTransform\n",
      "Transform: ToTensor()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Loading the Datasets\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\", train=True, download=True, transform=ToTensor()\n",
    ")\n",
    "print(\"Training Data: \", training_data)\n",
    "\n",
    "testing_data = datasets.FashionMNIST(\n",
    "    root=\"data\", train=False, download=True, transform=ToTensor()\n",
    ")\n",
    "print(\"Testing Data: \", testing_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1fe47d2",
   "metadata": {},
   "source": [
    "## DataLoaders\n",
    "- DataLoaders support automatic batching, sampling, shuffling, and multiprocess data loading.\n",
    "- They wrap the dataset over an iterable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "909154ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loader:  <torch.utils.data.dataloader.DataLoader object at 0x133006f00>\n",
      "Testing Loader:  <torch.utils.data.dataloader.DataLoader object at 0x133005fd0>\n"
     ]
    }
   ],
   "source": [
    "# Fixed Batch Size\n",
    "batch_size = 64\n",
    "\n",
    "# DataLoaders\n",
    "train_loader = DataLoader(\n",
    "    dataset=training_data, batch_size=batch_size\n",
    ")\n",
    "print(\"Training Loader: \", train_loader)\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset=testing_data, batch_size=batch_size\n",
    ")\n",
    "print(\"Testing Loader: \", test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b0346ef",
   "metadata": {},
   "source": [
    "## Building the Model\n",
    "- Every neural network built using PyTorch inherits its properties for the `nn` module.\n",
    "- All the layers of the model are defined inside the constructor of the class.\n",
    "- The propagation of the data through the layers is defined in the forward function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8c57b50b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Accelerator: mps\n"
     ]
    }
   ],
   "source": [
    "if torch.accelerator.is_available():\n",
    "    acc = torch.accelerator.current_accelerator(check_available=True).type  # type: ignore\n",
    "else:\n",
    "    acc = \"cpu\"\n",
    "\n",
    "print(f\"Available Accelerator: {acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "51ccb107",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FirstPyTorchNN(nn.Module):\n",
    "    \"\"\"This class implements a neural network in PyTorch.\"\"\"\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.sequential = nn.Sequential(\n",
    "            nn.Linear(28 * 28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "            nn.Softmax()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Implements the forward propagation of the model.\"\"\"\n",
    "        x = self.flatten(x)\n",
    "        logits = self.sequential(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "284eed73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FirstPyTorchNN(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (sequential): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "    (5): Softmax(dim=None)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Loading the Model on the Accelerator, Much more relevant in respect the workings of a GPU\n",
    "first_model = FirstPyTorchNN().to(device=acc)\n",
    "print(first_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0690036e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other Hyper Parameters\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(params=first_model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b647ee6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/raidenshogun/Development/PyTorch-Playgrounds/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1773: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.6955392360687256  [64/60000]\n",
      "loss: 1.6377341747283936  [6464/60000]\n",
      "loss: 1.7271332740783691  [12864/60000]\n",
      "loss: 1.90253746509552  [19264/60000]\n",
      "loss: 1.8375917673110962  [25664/60000]\n",
      "loss: 1.75740647315979  [32064/60000]\n",
      "loss: 1.7461334466934204  [38464/60000]\n",
      "loss: 1.7571625709533691  [44864/60000]\n",
      "loss: 1.7730157375335693  [51264/60000]\n",
      "loss: 1.8529824018478394  [57664/60000]\n",
      "loss: 1.6950182914733887  [64/60000]\n",
      "loss: 1.6778686046600342  [6464/60000]\n",
      "loss: 1.7261576652526855  [12864/60000]\n",
      "loss: 1.833077311515808  [19264/60000]\n",
      "loss: 1.7779645919799805  [25664/60000]\n",
      "loss: 1.7593181133270264  [32064/60000]\n",
      "loss: 1.7288976907730103  [38464/60000]\n",
      "loss: 1.7537496089935303  [44864/60000]\n",
      "loss: 1.8360141515731812  [51264/60000]\n",
      "loss: 1.7953654527664185  [57664/60000]\n",
      "loss: 1.6950538158416748  [64/60000]\n",
      "loss: 1.6871020793914795  [6464/60000]\n",
      "loss: 1.7565385103225708  [12864/60000]\n",
      "loss: 1.8749682903289795  [19264/60000]\n",
      "loss: 1.8169257640838623  [25664/60000]\n",
      "loss: 1.8546746969223022  [32064/60000]\n",
      "loss: 1.7340257167816162  [38464/60000]\n",
      "loss: 1.7751197814941406  [44864/60000]\n",
      "loss: 1.7458001375198364  [51264/60000]\n",
      "loss: 1.7987728118896484  [57664/60000]\n",
      "loss: 1.673379898071289  [64/60000]\n",
      "loss: 1.7183197736740112  [6464/60000]\n",
      "loss: 1.7261279821395874  [12864/60000]\n",
      "loss: 1.7983450889587402  [19264/60000]\n",
      "loss: 1.7443571090698242  [25664/60000]\n",
      "loss: 1.7806745767593384  [32064/60000]\n",
      "loss: 1.741379737854004  [38464/60000]\n",
      "loss: 1.734504222869873  [44864/60000]\n",
      "loss: 1.7574084997177124  [51264/60000]\n",
      "loss: 1.8311288356781006  [57664/60000]\n",
      "loss: 1.6641513109207153  [64/60000]\n",
      "loss: 1.710179328918457  [6464/60000]\n",
      "loss: 1.7263336181640625  [12864/60000]\n",
      "loss: 1.8310885429382324  [19264/60000]\n",
      "loss: 1.7841911315917969  [25664/60000]\n",
      "loss: 1.8221607208251953  [32064/60000]\n",
      "loss: 1.7420432567596436  [38464/60000]\n",
      "loss: 1.754945993423462  [44864/60000]\n",
      "loss: 1.7422473430633545  [51264/60000]\n",
      "loss: 1.816279411315918  [57664/60000]\n",
      "loss: 1.6769318580627441  [64/60000]\n",
      "loss: 1.6170430183410645  [6464/60000]\n",
      "loss: 1.7378872632980347  [12864/60000]\n",
      "loss: 1.83624267578125  [19264/60000]\n",
      "loss: 1.8051804304122925  [25664/60000]\n",
      "loss: 1.7446352243423462  [32064/60000]\n",
      "loss: 1.7504048347473145  [38464/60000]\n",
      "loss: 1.781752109527588  [44864/60000]\n",
      "loss: 1.7778104543685913  [51264/60000]\n",
      "loss: 1.8740308284759521  [57664/60000]\n",
      "loss: 1.66398024559021  [64/60000]\n",
      "loss: 1.69728684425354  [6464/60000]\n",
      "loss: 1.7266716957092285  [12864/60000]\n",
      "loss: 1.839187502861023  [19264/60000]\n",
      "loss: 1.7571730613708496  [25664/60000]\n",
      "loss: 1.805097222328186  [32064/60000]\n",
      "loss: 1.7263526916503906  [38464/60000]\n",
      "loss: 1.8203552961349487  [44864/60000]\n",
      "loss: 1.7953765392303467  [51264/60000]\n",
      "loss: 1.7993004322052002  [57664/60000]\n",
      "loss: 1.6639904975891113  [64/60000]\n",
      "loss: 1.6317903995513916  [6464/60000]\n",
      "loss: 1.711147665977478  [12864/60000]\n",
      "loss: 1.8334726095199585  [19264/60000]\n",
      "loss: 1.7462499141693115  [25664/60000]\n",
      "loss: 1.7475171089172363  [32064/60000]\n",
      "loss: 1.7434132099151611  [38464/60000]\n",
      "loss: 1.7428730726242065  [44864/60000]\n",
      "loss: 1.7579536437988281  [51264/60000]\n",
      "loss: 1.8230924606323242  [57664/60000]\n",
      "loss: 1.6489019393920898  [64/60000]\n",
      "loss: 1.6173808574676514  [6464/60000]\n",
      "loss: 1.7173866033554077  [12864/60000]\n",
      "loss: 1.8512637615203857  [19264/60000]\n",
      "loss: 1.7420871257781982  [25664/60000]\n",
      "loss: 1.7736504077911377  [32064/60000]\n",
      "loss: 1.7238543033599854  [38464/60000]\n",
      "loss: 1.7554796934127808  [44864/60000]\n",
      "loss: 1.7399967908859253  [51264/60000]\n",
      "loss: 1.7783896923065186  [57664/60000]\n",
      "loss: 1.6724584102630615  [64/60000]\n",
      "loss: 1.6578922271728516  [6464/60000]\n",
      "loss: 1.758080005645752  [12864/60000]\n",
      "loss: 1.820427417755127  [19264/60000]\n",
      "loss: 1.7575260400772095  [25664/60000]\n",
      "loss: 1.7743213176727295  [32064/60000]\n",
      "loss: 1.7574260234832764  [38464/60000]\n",
      "loss: 1.7890760898590088  [44864/60000]\n",
      "loss: 1.763563871383667  [51264/60000]\n",
      "loss: 1.7859854698181152  [57664/60000]\n",
      "loss: 1.6798837184906006  [64/60000]\n",
      "loss: 1.648633599281311  [6464/60000]\n",
      "loss: 1.7270865440368652  [12864/60000]\n",
      "loss: 1.8356266021728516  [19264/60000]\n",
      "loss: 1.7735025882720947  [25664/60000]\n",
      "loss: 1.7606561183929443  [32064/60000]\n",
      "loss: 1.7237297296524048  [38464/60000]\n",
      "loss: 1.7800875902175903  [44864/60000]\n",
      "loss: 1.7111499309539795  [51264/60000]\n",
      "loss: 1.8055169582366943  [57664/60000]\n",
      "loss: 1.6865301132202148  [64/60000]\n",
      "loss: 1.6651192903518677  [6464/60000]\n",
      "loss: 1.7267543077468872  [12864/60000]\n",
      "loss: 1.847611427307129  [19264/60000]\n",
      "loss: 1.7267560958862305  [25664/60000]\n",
      "loss: 1.7578684091567993  [32064/60000]\n",
      "loss: 1.7265969514846802  [38464/60000]\n",
      "loss: 1.7580251693725586  [44864/60000]\n",
      "loss: 1.7580376863479614  [51264/60000]\n",
      "loss: 1.8048511743545532  [57664/60000]\n",
      "loss: 1.679898738861084  [64/60000]\n",
      "loss: 1.6676920652389526  [6464/60000]\n",
      "loss: 1.7106661796569824  [12864/60000]\n",
      "loss: 1.820490837097168  [19264/60000]\n",
      "loss: 1.7892098426818848  [25664/60000]\n",
      "loss: 1.7726099491119385  [32064/60000]\n",
      "loss: 1.7440543174743652  [38464/60000]\n",
      "loss: 1.7423803806304932  [44864/60000]\n",
      "loss: 1.773380994796753  [51264/60000]\n",
      "loss: 1.7909321784973145  [57664/60000]\n",
      "loss: 1.6799004077911377  [64/60000]\n",
      "loss: 1.648379921913147  [6464/60000]\n",
      "loss: 1.7111557722091675  [12864/60000]\n",
      "loss: 1.8355481624603271  [19264/60000]\n",
      "loss: 1.7466135025024414  [25664/60000]\n",
      "loss: 1.7424001693725586  [32064/60000]\n",
      "loss: 1.758023977279663  [38464/60000]\n",
      "loss: 1.7736369371414185  [44864/60000]\n",
      "loss: 1.804053783416748  [51264/60000]\n",
      "loss: 1.7736525535583496  [57664/60000]\n",
      "loss: 1.6642752885818481  [64/60000]\n",
      "loss: 1.6747472286224365  [6464/60000]\n",
      "loss: 1.711599349975586  [12864/60000]\n",
      "loss: 1.8352723121643066  [19264/60000]\n",
      "loss: 1.7110432386398315  [25664/60000]\n",
      "loss: 1.7575069665908813  [32064/60000]\n",
      "loss: 1.7578669786453247  [38464/60000]\n",
      "loss: 1.759843349456787  [44864/60000]\n",
      "loss: 1.726796269416809  [51264/60000]\n",
      "loss: 1.7891063690185547  [57664/60000]\n",
      "loss: 1.6488122940063477  [64/60000]\n",
      "loss: 1.668161392211914  [6464/60000]\n",
      "loss: 1.7104434967041016  [12864/60000]\n",
      "loss: 1.8204398155212402  [19264/60000]\n",
      "loss: 1.804877519607544  [25664/60000]\n",
      "loss: 1.8045812845230103  [32064/60000]\n",
      "loss: 1.7113564014434814  [38464/60000]\n",
      "loss: 1.8045799732208252  [44864/60000]\n",
      "loss: 1.7580251693725586  [51264/60000]\n",
      "loss: 1.7892721891403198  [57664/60000]\n",
      "loss: 1.6641781330108643  [64/60000]\n",
      "loss: 1.683115005493164  [6464/60000]\n",
      "loss: 1.7423908710479736  [12864/60000]\n",
      "loss: 1.8203346729278564  [19264/60000]\n",
      "loss: 1.773648738861084  [25664/60000]\n",
      "loss: 1.7425603866577148  [32064/60000]\n",
      "loss: 1.7579045295715332  [38464/60000]\n",
      "loss: 1.7580249309539795  [44864/60000]\n",
      "loss: 1.7426648139953613  [51264/60000]\n",
      "loss: 1.8361161947250366  [57664/60000]\n",
      "loss: 1.6636171340942383  [64/60000]\n",
      "loss: 1.647705316543579  [6464/60000]\n",
      "loss: 1.7111501693725586  [12864/60000]\n",
      "loss: 1.8508484363555908  [19264/60000]\n",
      "loss: 1.7580251693725586  [25664/60000]\n",
      "loss: 1.7580219507217407  [32064/60000]\n",
      "loss: 1.7736457586288452  [38464/60000]\n",
      "loss: 1.758021354675293  [44864/60000]\n",
      "loss: 1.7580246925354004  [51264/60000]\n",
      "loss: 1.8049001693725586  [57664/60000]\n",
      "loss: 1.6642751693725586  [64/60000]\n",
      "loss: 1.6734477281570435  [6464/60000]\n",
      "loss: 1.7105107307434082  [12864/60000]\n",
      "loss: 1.8361601829528809  [19264/60000]\n",
      "loss: 1.7736324071884155  [25664/60000]\n",
      "loss: 1.7736501693725586  [32064/60000]\n",
      "loss: 1.7427597045898438  [38464/60000]\n",
      "loss: 1.773673176765442  [44864/60000]\n",
      "loss: 1.7426443099975586  [51264/60000]\n",
      "loss: 1.8045735359191895  [57664/60000]\n",
      "loss: 1.6798999309539795  [64/60000]\n",
      "loss: 1.6636826992034912  [6464/60000]\n",
      "loss: 1.7154691219329834  [12864/60000]\n",
      "loss: 1.8516864776611328  [19264/60000]\n",
      "loss: 1.758711338043213  [25664/60000]\n",
      "loss: 1.7577548027038574  [32064/60000]\n",
      "loss: 1.7722572088241577  [38464/60000]\n",
      "loss: 1.7745089530944824  [44864/60000]\n",
      "loss: 1.773650050163269  [51264/60000]\n",
      "loss: 1.7580087184906006  [57664/60000]\n"
     ]
    }
   ],
   "source": [
    "# Training Method\n",
    "def train(dataloader: DataLoader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(acc), y.to(acc)\n",
    "\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss}  [{current}/{size}]\")\n",
    "\n",
    "for epoch in range(20):\n",
    "    train(train_loader, first_model, loss_fn, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb04e18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-playgrounds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
