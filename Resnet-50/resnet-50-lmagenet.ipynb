{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":2491748,"sourceType":"datasetVersion","datasetId":1500837}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Imports","metadata":{}},{"cell_type":"code","source":"from torchvision.datasets import DatasetFolder, ImageFolder\nfrom torchvision.transforms import v2\nfrom torch.utils.data import DataLoader, Subset, ConcatDataset\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nimport torch\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\n\nfrom collections import Counter\nfrom pathlib import Path\nimport time\nimport subprocess\nimport shutil\nimport json","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-10T08:56:35.959248Z","iopub.execute_input":"2025-09-10T08:56:35.959932Z","iopub.status.idle":"2025-09-10T08:56:35.964568Z","shell.execute_reply.started":"2025-09-10T08:56:35.959906Z","shell.execute_reply":"2025-09-10T08:56:35.964024Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Data Preprocessing","metadata":{}},{"cell_type":"code","source":"# Kaggle Loaded Dataset Path (readonly)\nroot_input = Path(\"/kaggle/input/imagenet100\")\n\n# Working Dataset Path to modify the default\nroot_working = Path(\"/kaggle/working/reduced_imagenet\")\nroot_working.mkdir(exist_ok=True)\n\nif root_working.exists():\n    print(\"The working directory was created successfully.\")\nelse:\n    print(\"There seems to be an issue.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-10T08:56:36.944422Z","iopub.execute_input":"2025-09-10T08:56:36.945097Z","iopub.status.idle":"2025-09-10T08:56:36.950373Z","shell.execute_reply.started":"2025-09-10T08:56:36.945071Z","shell.execute_reply":"2025-09-10T08:56:36.949655Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Loading all the labels\nlabels_path = Path(root_input / \"Labels.json\")\n\n# Reading the Raw JSON Labels\nlabels = json.loads(labels_path.read_text())\nprint(json.dumps(labels, indent=4))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-10T08:56:38.551369Z","iopub.execute_input":"2025-09-10T08:56:38.551865Z","iopub.status.idle":"2025-09-10T08:56:38.572955Z","shell.execute_reply.started":"2025-09-10T08:56:38.551834Z","shell.execute_reply":"2025-09-10T08:56:38.572326Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Modifying the labels to retain only the first name\nfor index, item in enumerate(labels.items()):\n    key, value = item\n    class_name = value.split(\",\")[0]\n    labels[key] = class_name\n\n# Modified Labels\nmod_labels = json.dumps(labels, indent=4)\nprint(\"All Labels:\\n\", mod_labels)\n\n# Saving the Reduced Labels\nwith open(root_working / \"labels.json\", \"w\") as file:\n    json.dump(labels, fp=file, indent=4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-10T08:56:40.849997Z","iopub.execute_input":"2025-09-10T08:56:40.850563Z","iopub.status.idle":"2025-09-10T08:56:40.856527Z","shell.execute_reply.started":"2025-09-10T08:56:40.850542Z","shell.execute_reply":"2025-09-10T08:56:40.855828Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"The copying process has started\")\n\n# Iterating over the directories at root\nfor parent_node in root_input.iterdir():\n\n    # Avoiding the Label File\n    if not parent_node.is_dir():\n        continue\n\n    # Retrieving the Split Name\n    split_name = parent_node.stem.split(\".\")[0]\n\n    # Executing the Copy based on the Split Name\n    if split_name == \"train\" :\n        dest_path = root_working / \"train\"\n    elif split_name == \"val\":\n        dest_path = root_working / \"valid\"\n        \n    # Creating the directory\n    dest_path.mkdir(exist_ok=True)\n    \n    # Iterating through the classes on sub-nodes\n    for sub_node in parent_node.iterdir():\n\n        # If Class ID in Reduced => Backtrack the name and copy\n        if sub_node.stem in labels.keys():\n            class_name = labels[sub_node.stem]\n            completed_path = dest_path / class_name\n            shutil.copytree(sub_node, completed_path, dirs_exist_ok=True)\n\nprint(\"Copy Process Completed\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-10T08:56:43.469438Z","iopub.execute_input":"2025-09-10T08:56:43.469986Z","iopub.status.idle":"2025-09-10T08:57:17.172471Z","shell.execute_reply.started":"2025-09-10T08:56:43.469964Z","shell.execute_reply":"2025-09-10T08:57:17.171626Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Data Handling","metadata":{}},{"cell_type":"code","source":"# Dataset Working Path\ndataset_path = Path(\"/kaggle/working/reduced_imagenet\")\n\n\nclass DataHandler:\n    \"\"\"This class is responsible for loading the datasets.\"\"\"\n    def __init__(self, root_dir: Path) -> None:\n        self.root_dir = root_dir\n        self.norm_means = [0.485, 0.456, 0.406]\n        self.norm_stds = [0.229, 0.224, 0.225]\n\n        self.apply_transforms = v2.Compose([\n            v2.Resize(size=256, interpolation=v2.InterpolationMode.BICUBIC),  # Maintaining the Aspect Ratio\n            v2.CenterCrop(size=(224, 224)),  # Crop the Image to the Subject\n            v2.ToImage(),  # Converts PIL Image to Tensor\n            v2.ToDtype(torch.float32, scale=True),  # Converting the Dtype for Normalisation\n            v2.Normalize(mean=self.norm_means, std=self.norm_stds),  # Applies Normalisation\n        ])\n\n    def load_set(self, set_name: str) -> DatasetFolder:\n        \"\"\"Loads the set by the specified set_name.\"\"\"\n\n        if set_name == \"train\":\n            dataset_path = self.root_dir / \"train\"\n        elif set_name == \"valid\":\n            dataset_path = self.root_dir / \"valid\"\n        else:\n            raise UnboundLocalError(\"Invalid set name provided.\")\n\n        dataset = ImageFolder(root=dataset_path, transform=self.apply_transforms)\n        self.class_names = dataset.classes\n        return dataset\n\n    def move_samples_from_train(\n        self, train_set: DatasetFolder, \n        move_percent: float = 0.12\n    ) -> tuple[Subset, Subset]:\n        \"\"\"Moves a fixed number of samples from train to valid for better split.\"\"\"\n\n        targets = np.array(train_set.targets)\n        indices = range(len(train_set))\n        \n        # Stratified Sampling of the Train Set\n        train_indices, valid_indices = train_test_split(\n            indices, test_size=move_percent, stratify=targets\n        )\n        \n        # Subsets\n        train_set_pre_prep = Subset(train_set, train_indices)\n        valid_set_pre_prep = Subset(train_set, valid_indices)\n\n        return train_set_pre_prep, valid_set_pre_prep\n    \n    def prepare_dataset(self, dataset: DatasetFolder, batch_size: int=64, shuffle: bool=True) -> DataLoader:\n        \"\"\"Returns the prepared and loaded dataset.\"\"\"\n\n        return DataLoader(\n            dataset=dataset, \n            batch_size=batch_size,\n            shuffle=shuffle,\n            num_workers=4\n        )\n    \n    def view_images(self, loaded_set: DataLoader) -> None:\n        \"\"\"Helper function just to view the images.\"\"\"\n        images, targets = next(iter(loaded_set))\n\n        plt.figure(figsize=(12, 10))\n        for i in range(20):\n            img = images[i].squeeze()\n            label = targets[i]\n\n            plt.subplot(5, 4, i + 1)\n            plt.title(f\"{self.class_names[label]}\", fontdict={\"size\": 7})\n            plt.imshow(img.permute(1, 2, 0))\n            plt.axis(\"off\")\n        \n        plt.tight_layout()\n        plt.show()\n\n\n# Testing\ndata_handle = DataHandler(root_dir=dataset_path)\ntrain_loaded = data_handle.load_set(\"train\")\ntrain_prep = data_handle.prepare_dataset(dataset=train_loaded)\nclass_names = data_handle.class_names\ndata_handle.view_images(loaded_set=train_prep)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-10T08:16:10.174604Z","iopub.execute_input":"2025-09-10T08:16:10.175304Z","iopub.status.idle":"2025-09-10T08:16:13.936840Z","shell.execute_reply.started":"2025-09-10T08:16:10.175275Z","shell.execute_reply":"2025-09-10T08:16:13.936042Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Important**\n- The images are a little weird as the norm is turned on should be fine with it off just for visualisation purposes.","metadata":{}},{"cell_type":"code","source":"# Class Names as in the dataset\nprint(\"Total No of Classes: \", len(class_names))\nprint(\"\\nClass Names:\\n\", class_names)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-10T08:16:13.938619Z","iopub.execute_input":"2025-09-10T08:16:13.938884Z","iopub.status.idle":"2025-09-10T08:16:13.943567Z","shell.execute_reply.started":"2025-09-10T08:16:13.938863Z","shell.execute_reply":"2025-09-10T08:16:13.942743Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## The Residual Block","metadata":{}},{"cell_type":"code","source":"class ResidualLearningBlock(torch.nn.Module):\n    \"\"\"Class Implements a Residual Learning Block to build the ResNet.\"\"\"\n\n    def __init__(self, in_channels: int, stride: int, no_of_filters: int) -> None:\n\n        # Loading all the properties from the super class\n        super().__init__()\n        \n        # Defining the layers for each block\n        self.sequential_block = torch.nn.Sequential(\n            torch.nn.Conv2d(in_channels=in_channels, out_channels=no_of_filters, kernel_size=1),  # 1x1 Convolution, Stride: 1\n            torch.nn.BatchNorm2d(num_features=no_of_filters),\n            torch.nn.ReLU(),\n\n            torch.nn.Conv2d(in_channels=no_of_filters, out_channels=no_of_filters, padding=1, kernel_size=3, stride=stride),  # 3x3 Convolution, Stride: 1\n            torch.nn.BatchNorm2d(num_features=no_of_filters),\n            torch.nn.ReLU(),\n\n            torch.nn.Conv2d(in_channels=no_of_filters, out_channels=no_of_filters*4, kernel_size=1),  # 1x1 Convolution, Stride: Stride\n            torch.nn.BatchNorm2d(num_features=no_of_filters*4)\n        )\n\n        # Skip Connection Layer\n        if stride > 1 or in_channels != no_of_filters * 4:\n            self.skip_block = torch.nn.Sequential(\n                torch.nn.Conv2d(in_channels=in_channels, out_channels=no_of_filters*4, kernel_size=1, stride=stride),\n                torch.nn.BatchNorm2d(num_features=no_of_filters*4)\n            )\n        else:\n            self.skip_block = torch.nn.Identity()\n\n    def forward(self, X: torch.Tensor) -> torch.Tensor:\n        \"\"\"Implements the forward propagation of the residual network along with skip connection.\"\"\"\n\n        # Copying the Inputs for the Block\n        inputs = X\n\n        # Block Bottleneck Propagation\n        x = self.sequential_block(X)\n\n        # Skip Connection Propagation\n        inputs = self.skip_block(inputs)\n\n        # Concatenation of the Skip Connection\n        concat = x + inputs\n\n        # Activating the parameters\n        concat = torch.nn.ReLU()(concat)\n\n        return concat\n\n\n# Testing\nfirst_residual_block = ResidualLearningBlock(256, 2, 512)\nprint(first_residual_block)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-10T08:35:27.651220Z","iopub.execute_input":"2025-09-10T08:35:27.651716Z","iopub.status.idle":"2025-09-10T08:35:27.697106Z","shell.execute_reply.started":"2025-09-10T08:35:27.651693Z","shell.execute_reply":"2025-09-10T08:35:27.696500Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## The Resnet-50 Model","metadata":{}},{"cell_type":"code","source":"class Resnet50(torch.nn.Module):\n    \"\"\"This class implements the complete Resnet-50 model architecture using ResidualLearningBlocks.\"\"\"\n\n    initial_fmaps = 64\n    initial_kernel_size = 7\n    initial_stride = 2\n    initial_pool_size = 3\n    \n    def __init__(self, input_channels: int = 3, fc_size: int = 1000, dropout_rate: float = 0.2) -> None:\n\n        # Loading all the properties from the super class\n        super().__init__()\n\n        # Intial Convolution Block\n        self.model_conv_layers = torch.nn.Sequential(\n            torch.nn.Conv2d(\n                in_channels=input_channels,\n                out_channels=self.initial_fmaps,\n                kernel_size=self.initial_kernel_size,\n                stride=self.initial_stride,\n                padding=3   \n            ),\n            torch.nn.MaxPool2d(\n                kernel_size=self.initial_pool_size,\n                stride=2,\n                padding=1\n            )\n        )\n\n        # No of channel for the deep convolutional layers\n        self.channel_config = [(64, 3), (128, 4), (256, 6), (512, 3)]\n        prev_out_channels = self.channel_config[0][0]\n\n        # Constructing the remaining model architecture\n        for layer_idx, (no_channels, no_of_blocks) in enumerate(self.channel_config):\n\n            # Creating each Conv_nX sequence\n            for block_no in range(no_of_blocks):\n\n                # Default stride value\n                stride = 1\n\n                # If the block is the first in the sequence\n                if block_no == 0:\n                    \n                    # Updating Stride for each sequence block\n                    stride = 1 if layer_idx == 0 else 2\n                    \n                    # Appending the Residual Block Sequences into the main Resnet Module\n                    self.model_conv_layers.append(\n                        ResidualLearningBlock(\n                            in_channels=prev_out_channels,\n                            stride=stride,\n                            no_of_filters=no_channels\n                        )\n                    )\n                else:\n                    # Appending the Residual Block Sequences into the main Resnet Module\n                    self.model_conv_layers.append(\n                        ResidualLearningBlock(\n                            in_channels=prev_out_channels,\n                            stride=stride,\n                            no_of_filters=no_channels\n                        )\n                    )\n    \n                # Updating the no of previous channels\n                prev_out_channels = no_channels * 4\n\n            # Adding the Dropout Layer after each block\n            self.model_conv_layers.append(\n                torch.nn.Dropout(p=dropout_rate)\n            )\n        \n        # Adding the Average Pool Layer on successful execution\n        else:\n            self.model_conv_layers.append(\n                torch.nn.AvgPool2d(kernel_size=self.initial_kernel_size)\n            )\n\n        # Downstream Layers\n        self.dropout_fc = torch.nn.Dropout(p=dropout_rate)\n        self.fc = torch.nn.Linear(in_features=2048 * 1, out_features=fc_size)\n\n    def forward(self, X: torch.Tensor) -> torch.Tensor:\n        \"\"\"Implements the forward propagation of the complete Resnet Model.\"\"\"\n\n        avg_pool_scores = self.model_conv_layers(X)\n        avg_pool_flatten = torch.nn.Flatten()(avg_pool_scores)\n        avg_pool_dropout = self.dropout_fc(avg_pool_flatten)\n        logits = self.fc(avg_pool_dropout)\n\n        return logits\n\n\n# Testing\nfirst_resnet_50 = Resnet50()\nprint(first_resnet_50)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-10T08:35:37.464588Z","iopub.execute_input":"2025-09-10T08:35:37.465157Z","iopub.status.idle":"2025-09-10T08:35:37.703153Z","shell.execute_reply.started":"2025-09-10T08:35:37.465133Z","shell.execute_reply":"2025-09-10T08:35:37.702557Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## The Optimizer","metadata":{}},{"cell_type":"code","source":"CHECKPOINT_PATH = \"/kaggle/working/models\"\n\n\nclass TrainingLoop:\n    \"\"\"This class handles the training loop for the models.\"\"\"\n    def __init__(self, learning_rate: float, model: torch.nn.Module):\n        self.model = model\n        self.optim = torch.optim.AdamW(\n            params=self.model.parameters(),\n            lr=learning_rate\n        )\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        self.loss_fn = torch.nn.CrossEntropyLoss()\n\n        # Learning rate scheduler\n        self.lr_schedule = ReduceLROnPlateau(\n            optimizer=self.optim, mode=\"min\", factor=0.1,patience=5, min_lr=1e-6\n        )\n\n        # Creating the Checkpoint Storage Directory\n        self.model_dir = Path(CHECKPOINT_PATH)\n        if not self.model_dir.exists():\n            self.model_dir.mkdir()\n    \n    def train_model(\n            self, epochs: int,\n            train_set: DataLoader, valid_set: DataLoader\n        ) -> tuple[list[float], list[float]]:\n        \"\"\"Trains the model for the given number of epochs.\"\"\"\n\n        # Cache Losses\n        train_losses = []\n        valid_losses = []\n\n        # Mean Training Variables\n        mean_loss_train = 0\n        mean_loss_valid = 0\n\n        # Training Checkpoint\n        best_valid_loss = torch.inf\n        patience = 5\n        patience_counter = 0\n\n        # Training Loop\n        print(\"The training process has started\")\n        for i in range(epochs):\n\n            # Average Epoch Time tracking\n            start = time.time()\n\n            # ==== Training Step ====\n\n            # Completing a single epoch\n            for batch, (X, y) in enumerate(train_set):\n\n                # Moving the batches to GPU\n                X, y = X.to(self.device), y.to(self.device)\n            \n                # Training Step\n                logits = self.model(X)\n\n                # Loss Calculation\n                train_loss = self.loss_fn(input=logits, target=y)\n                train_loss = train_loss.sum()\n                mean_loss_train += train_loss.item()\n\n                # Backpropagation\n                self.optim.zero_grad()\n                train_loss.backward()\n                self.optim.step()\n\n            # Completion of Epoch\n            end = time.time()\n\n            # ==== Validation Step ====\n\n            # Turning on the Eval mode on the model for the BN-Layers\n            self.model.eval()\n            with torch.no_grad():\n                for batch, (X, y) in enumerate(valid_set):\n\n                    # Moving the batches to GPU\n                    X, y = X.to(self.device), y.to(self.device)\n\n                    # Validation Calculation\n                    logits = self.model(X)\n\n                    # Loss Calculation\n                    valid_loss = self.loss_fn(input=logits, target=y)\n                    valid_loss = valid_loss.sum()\n                    mean_loss_valid += valid_loss.item()\n            \n            # Switching the model back to training mode\n            self.model.train()\n\n            # ==== End of Epoch Metrics & Model Checkpointing ====\n            mean_loss_train /= len(train_set)\n            mean_loss_valid /= len(valid_set)\n            time_epoch = end - start\n\n            # Updating the LR Scheduler\n            self.lr_schedule.step(mean_loss_valid)\n            \n            # Updating the Caches\n            train_losses.append(mean_loss_train)\n            valid_losses.append(mean_loss_valid)\n\n            print(f\"Epoch {i + 1}: Train Loss -> {mean_loss_train} | Valid Loss -> {mean_loss_valid} | Time Epoch -> {time_epoch} | Learning Rate -> {self.lr_schedule.get_last_lr()}\")\n\n            # Updating the best validation loss so far\n            if mean_loss_valid < best_valid_loss:\n                best_valid_loss = mean_loss_valid\n\n                # Saving the Model by weights\n                torch.save(obj=self.model.state_dict(), f=self.model_dir / \"resnet_50_imagenet.pth\")\n                print(\"New best model was saved\")\n                patience_counter = 0\n            else:\n                patience_counter += 1\n                print(f\"No improvement: {patience_counter} / {patience}\")\n                if patience_counter >= patience:\n                    print(\"Early Stopping\")\n                    break\n\n            # ==== Reset the Training Loop Metrics ====\n            mean_loss_train, mean_loss_valid = 0, 0\n        \n        return train_losses, valid_losses\n\n\n# Testing\noptim = TrainingLoop(learning_rate=1e-4, model=first_resnet_50)\nprint(optim)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-10T08:39:06.702209Z","iopub.execute_input":"2025-09-10T08:39:06.702827Z","iopub.status.idle":"2025-09-10T08:39:06.798776Z","shell.execute_reply.started":"2025-09-10T08:39:06.702796Z","shell.execute_reply":"2025-09-10T08:39:06.798089Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## The Main Function","metadata":{}},{"cell_type":"code","source":"# Learning Curve Asset Path\nASSET_PATH = Path(\"/kaggle/working/assets\")\n\n# Accelerator Device\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Accelerator Available: {DEVICE}\")\nprint(f\"Units: {torch.cuda.device_count()}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-10T08:39:12.873710Z","iopub.execute_input":"2025-09-10T08:39:12.874292Z","iopub.status.idle":"2025-09-10T08:39:12.904836Z","shell.execute_reply.started":"2025-09-10T08:39:12.874271Z","shell.execute_reply":"2025-09-10T08:39:12.904188Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def main():\n    \"\"\"The main function for the Resnet-50.\"\"\"\n    print(\"Hello, Residual Network - 50\")\n    print(\"\\n ------- \\n\")\n\n    # Initialising the Data Handler\n    data_handle = DataHandler(root_dir=dataset_path)\n\n    # Loading the sets from disk\n    train_set = data_handle.load_set(\"train\")\n    valid_set = data_handle.load_set(\"valid\")\n\n    # Initial sample sizes\n    print(\"Initial Sample Sizes:\")\n    train_count = Counter(train_set.targets)\n    valid_count = Counter(valid_set.targets)\n    print(f\"Train Set:\\n{train_count}\")\n    print(f\"\\nValid Set:\\n{valid_count}\")\n    print(\"\\n ------- \\n\")\n\n    # Moving Samples\n    train_set_pre_prep, valid_set_pre_prep = data_handle.move_samples_from_train(train_set, move_percent=0.15)\n    \n    print(\"Moved Sample Sizes:\")\n    train_targets = [train_set.targets[i] for i in train_set_pre_prep.indices]\n    valid_targets = [train_set.targets[i] for i in valid_set_pre_prep.indices]\n    train_count = Counter(train_targets)\n    valid_count = Counter(valid_targets)\n    print(f\"Train Set:\\n{train_count}\")\n    print(f\"\\nValid Set:\\n{valid_count}\")\n    print(\"\\n ------- \\n\")\n\n    # Preparing the sets\n    train_prep = data_handle.prepare_dataset(train_set_pre_prep)\n    valid_set_pre_prep = ConcatDataset([valid_set, valid_set_pre_prep])\n    valid_prep = data_handle.prepare_dataset(valid_set_pre_prep)\n\n    # Loading the Model\n    model_base = Resnet50().to(device=DEVICE)\n    resnet_50 = torch.nn.DataParallel(model_base)\n    \n    # Loading the Training Loop Handler\n    optimizer = TrainingLoop(learning_rate=5e-4, model=resnet_50)\n    \n    # Training the model\n    train_losses, valid_losses = optimizer.train_model(30, train_prep, valid_prep)\n\n    # Plotting the losses\n    plt.figure(figsize=(10, 8))\n    plt.title(\"Learning Curve\")\n    plt.plot(range(1, len(train_losses) + 1), train_losses, c=\"b\", ls=\"-\", label=\"Train Loss\")\n    plt.plot(range(1, len(valid_losses) + 1), valid_losses, c=\"g\", ls=\"-.\", label=\"Valid Loss\")\n    plt.legend(loc=\"upper right\")\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(\"Loss\")\n\n    # Storing the Learning Curve\n    if not ASSET_PATH.exists():\n        ASSET_PATH.mkdir()\n    plt.savefig(ASSET_PATH / \"learning_curve.png\")\n\n    # Rendering the plot\n    plt.show()\n\n\n# ==== Driver Code ====\nmain()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-10T08:39:15.601100Z","iopub.execute_input":"2025-09-10T08:39:15.601733Z","iopub.status.idle":"2025-09-10T08:55:46.519337Z","shell.execute_reply.started":"2025-09-10T08:39:15.601713Z","shell.execute_reply":"2025-09-10T08:55:46.518259Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}