{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13063996,"sourceType":"datasetVersion","datasetId":8273118}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Imports","metadata":{}},{"cell_type":"code","source":"from torchvision.datasets import DatasetFolder, ImageFolder\nfrom torchvision.transforms import v2\nfrom torch.utils.data import DataLoader, Subset, ConcatDataset\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nimport torch\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\n\nfrom collections import Counter\nfrom pathlib import Path\nimport time\nimport subprocess\nimport shutil\nimport json","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T10:51:03.783931Z","iopub.execute_input":"2025-09-15T10:51:03.784208Z","iopub.status.idle":"2025-09-15T10:51:16.718042Z","shell.execute_reply.started":"2025-09-15T10:51:03.784185Z","shell.execute_reply":"2025-09-15T10:51:16.717173Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Data Preprocessing","metadata":{}},{"cell_type":"code","source":"# Kaggle Loaded Dataset Path (readonly)\nroot_input = Path(\"/kaggle/input/imagenet-reduced-100/reduced_imagenet/\")\n\n# Working Dataset Path to modify the default\nroot_working = Path(\"/kaggle/working/reduced_imagenet\")\nroot_working.mkdir(exist_ok=True)\n\nif root_working.exists():\n    print(\"The working directory was created successfully.\")\nelse:\n    print(\"There seems to be an issue.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T10:51:16.719213Z","iopub.execute_input":"2025-09-15T10:51:16.719562Z","iopub.status.idle":"2025-09-15T10:51:16.724534Z","shell.execute_reply.started":"2025-09-15T10:51:16.719542Z","shell.execute_reply":"2025-09-15T10:51:16.723950Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Loading all the labels\nlabels_path = Path(root_input / \"labels.json\")\n\n# Reading the Raw JSON Labels\nlabels = json.loads(labels_path.read_text())\nprint(json.dumps(labels, indent=4))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T10:51:16.725325Z","iopub.execute_input":"2025-09-15T10:51:16.725560Z","iopub.status.idle":"2025-09-15T10:51:16.768272Z","shell.execute_reply.started":"2025-09-15T10:51:16.725537Z","shell.execute_reply":"2025-09-15T10:51:16.767654Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# No of Classes to be trained\nn_classes = 40\n\n# Selecting the First N_Classes from the Labels\nreduced_labels = dict(list(labels.items())[:n_classes])\nreduced_labels","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T11:01:32.908184Z","iopub.execute_input":"2025-09-15T11:01:32.908817Z","iopub.status.idle":"2025-09-15T11:01:32.914629Z","shell.execute_reply.started":"2025-09-15T11:01:32.908792Z","shell.execute_reply":"2025-09-15T11:01:32.914007Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Copy Process Started\")\n\nfor parent_node in root_input.iterdir():\n     # Avoiding the Label File\n    if not parent_node.is_dir():\n        continue\n\n    # Retrieving the Split Name\n    split_name = parent_node.stem\n\n    # Executing the Copy based on the Split Name\n    if split_name == \"train\" :\n        dest_path = root_working / \"train\"\n    elif split_name == \"valid\":\n        dest_path = root_working / \"valid\"\n    else:\n        print(f\"Skipping unrecognised branch: {parent_node}\")\n        continue\n\n    # Creating the directory\n    dest_path.mkdir(exist_ok=True)\n\n    # Iterating through the classes on sub-nodes\n    for sub_node in parent_node.iterdir():\n\n        # If Class ID in Reduced => Backtrack the name and copy\n        if sub_node.stem in reduced_labels.values():\n            class_name = sub_node.stem\n            completed_path = dest_path / class_name\n            shutil.copytree(sub_node, completed_path, dirs_exist_ok=True)\n\nprint(\"Copy Process Completed\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T10:51:16.775942Z","iopub.execute_input":"2025-09-15T10:51:16.776112Z","iopub.status.idle":"2025-09-15T10:52:37.982437Z","shell.execute_reply.started":"2025-09-15T10:51:16.776098Z","shell.execute_reply":"2025-09-15T10:52:37.981700Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## The Data Handler Object","metadata":{}},{"cell_type":"code","source":"# Dataset Working Path\ndataset_path = root_working\n\n\nclass DataHandler:\n    \"\"\"This class is responsible for loading the datasets.\"\"\"\n    def __init__(self, root_dir: Path) -> None:\n        self.root_dir = root_dir\n        self.norm_means = [0.485, 0.456, 0.406]\n        self.norm_stds = [0.229, 0.224, 0.225]\n\n        self.apply_transforms = v2.Compose([\n            v2.Resize(size=256, interpolation=v2.InterpolationMode.BICUBIC),  # Maintaining the Aspect Ratio\n            v2.CenterCrop(size=(224, 224)),  # Crop the Image to the Subject\n            v2.ToImage(),  # Converts PIL Image to Tensor\n            v2.ToDtype(torch.float32, scale=True),  # Converting the Dtype for Normalisation\n            v2.Normalize(mean=self.norm_means, std=self.norm_stds),  # Applies Normalisation\n        ])\n\n    def load_set(self, set_name: str) -> DatasetFolder:\n        \"\"\"Loads the set by the specified set_name.\"\"\"\n\n        if set_name == \"train\":\n            dataset_path = self.root_dir / \"train\"\n        elif set_name == \"valid\":\n            dataset_path = self.root_dir / \"valid\"\n        else:\n            raise UnboundLocalError(\"Invalid set name provided.\")\n\n        dataset = ImageFolder(root=dataset_path, transform=self.apply_transforms)\n        self.class_names = dataset.classes\n        return dataset\n\n    def move_samples_from_train(\n        self, train_set: DatasetFolder, \n        move_percent: float = 0.12\n    ) -> tuple[Subset, Subset]:\n        \"\"\"Moves a fixed number of samples from train to valid for better split.\"\"\"\n\n        targets = np.array(train_set.targets)\n        indices = range(len(train_set))\n        \n        # Stratified Sampling of the Train Set\n        train_indices, valid_indices = train_test_split(\n            indices, test_size=move_percent, stratify=targets\n        )\n        \n        # Subsets\n        train_set_pre_prep = Subset(train_set, train_indices)\n        valid_set_pre_prep = Subset(train_set, valid_indices)\n\n        return train_set_pre_prep, valid_set_pre_prep\n    \n    def prepare_dataset(self, dataset: DatasetFolder, batch_size: int=64, shuffle: bool=True) -> DataLoader:\n        \"\"\"Returns the prepared and loaded dataset.\"\"\"\n\n        return DataLoader(\n            dataset=dataset, \n            batch_size=batch_size,\n            shuffle=shuffle,\n            num_workers=4\n        )\n    \n    def view_images(self, loaded_set: DataLoader) -> None:\n        \"\"\"Helper function just to view the images.\"\"\"\n        images, targets = next(iter(loaded_set))\n\n        plt.figure(figsize=(12, 10))\n        for i in range(20):\n            img = images[i].squeeze()\n            label = targets[i]\n\n            plt.subplot(5, 4, i + 1)\n            plt.title(f\"{self.class_names[label]}\", fontdict={\"size\": 7})\n            plt.imshow(img.permute(1, 2, 0))\n            plt.axis(\"off\")\n        \n        plt.tight_layout()\n        plt.show()\n\n\n# Testing\ndata_handle = DataHandler(root_dir=dataset_path)\ntrain_loaded = data_handle.load_set(\"train\")\ntrain_prep = data_handle.prepare_dataset(dataset=train_loaded)\nclass_names = data_handle.class_names\ndata_handle.view_images(loaded_set=train_prep)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T10:52:37.983293Z","iopub.execute_input":"2025-09-15T10:52:37.983579Z","iopub.status.idle":"2025-09-15T10:52:41.316016Z","shell.execute_reply.started":"2025-09-15T10:52:37.983554Z","shell.execute_reply":"2025-09-15T10:52:41.315127Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Class Names as in the dataset\nprint(\"Total No of Classes: \", n_classes)\nprint(\"\\nClass Names:\\n\", class_names)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T10:52:41.317127Z","iopub.execute_input":"2025-09-15T10:52:41.317379Z","iopub.status.idle":"2025-09-15T10:52:41.322869Z","shell.execute_reply.started":"2025-09-15T10:52:41.317353Z","shell.execute_reply":"2025-09-15T10:52:41.322006Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## MultiHead Self-Attention Block","metadata":{}},{"cell_type":"code","source":"class MultiHeadSelfAttention(torch.nn.Module):\n    \"\"\"This class implements the MultiHead Self-Attention Mechanism that is central to Transformers.\"\"\"\n\n    def __init__(self, embed_dims: int = 768, n_heads: int = 12, in_features: int = 768, dropout_rate: float = 0.2) -> None:\n        \n        # Loading all the properties from the Super Class\n        super().__init__()\n\n        # Instance Variables of the MHSA\n        self.embed_dims = embed_dims\n        self.n_heads = n_heads\n\n        # Head Size\n        self.head_size = self.embed_dims // self.n_heads\n\n        # Attention Matrices\n        self.queries_full = torch.nn.Linear(in_features=in_features, out_features=self.embed_dims, bias=False)\n        self.keys_full = torch.nn.Linear(in_features=in_features, out_features=self.embed_dims, bias=False)\n        self.values_full = torch.nn.Linear(in_features=in_features, out_features=self.embed_dims, bias=False)\n\n        # Context Sharing Layer\n        self.context_share = torch.nn.Linear(in_features=in_features, out_features=embed_dims, bias=True)\n\n        # Dropout Layer\n        self.dropout = torch.nn.Dropout(p=dropout_rate)\n\n    def forward(self, X: torch.Tensor) -> torch.Tensor:\n        \"\"\"Applies the forward propagation for the entire MultiHead Self-Attention Block.\n        \n        For implementation efficiency it calculates the Linear Transformations for the Q, K, V on the full input.\n        It then permutes the tensors into individual heads to calculate the attention scores and provide the output.\n        \n        args:\n        - X: torch.Tensor -> Expected Batch_Size, N_Patch + 1 [CLS Token], Embedding Dim.\n        \n        returns:\n        - torch.Tensor -> Batch_Size, N_Patch + 1 [CLS Token], Embedding Dim.\"\"\"\n\n        # Input Dimensions\n        B, T, C = X.shape\n\n        # Scaling Constant for attention calculation\n        scaling_const = self.head_size ** -0.5\n\n        # Calculating all the Linear Projections\n        queries: torch.Tensor = self.queries_full(X)  # 197, 768 @ 768, 768 => (Batch_Size, 197, 768)\n        keys: torch.Tensor = self.keys_full(X)        # 197, 768 @ 768, 768 => (Batch_Size, 197, 768)\n        values: torch.Tensor = self.values_full(X)    # 197, 768 @ 768, 768 => (Batch_Size, 197, 768)\n\n        # Reshaping the tensor to Self Attention Head Sizes for Calculation\n        queries = queries.reshape((B, T, self.n_heads, self.head_size))  # Batch_Size, 197, 12, 64\n        keys = keys.reshape((B, T, self.n_heads, self.head_size))        # Batch_Size, 197, 12, 64\n        values = values.reshape((B, T, self.n_heads, self.head_size))    # Batch_Size, 197, 12, 64\n\n        # Permuting the Tensors for MultiHead Attention Calculation\n        queries_mhsa = torch.permute(input=queries, dims=[0, 2, 1, 3])   # Batch_Size, 12, 197, 64\n        keys_mhsa = torch.permute(input=keys, dims=[0, 2, 1, 3])         # Batch_Size, 12, 197, 64\n        values_mhsa = torch.permute(input=values, dims=[0, 2, 1, 3])     # Batch_Size, 12, 197, 64\n\n        # MultiHead Attention Pattern Calculation for each Attention Head\n        attention_pattern_mhsa = queries_mhsa @ keys_mhsa.transpose(-2, -1) * scaling_const  # 197, 64 @ 64, 197 => Batch_Size, 12, 197, 197\n        attention_pattern_mhsa = torch.nn.functional.softmax(attention_pattern_mhsa, dim=-1)\n\n        # Weighted Score\n        attended_embeddings_mhsa = attention_pattern_mhsa @ values_mhsa  # 197, 197 @ 197, 64 => Batch_Size, 12, 197, 64\n\n        # Resized Attension Scores\n        attended_embeddings = attended_embeddings_mhsa.permute(dims=[0, 2, 1, 3])   # Batch_Size, 197, 12, 64\n        attended_embeddings = attended_embeddings.flatten(start_dim=2, end_dim=-1)  # Batch_Size, 197, 768\n\n        # Context Sharing\n        rich_embeddings = self.context_share(attended_embeddings)  # 197, 768 @ 768, 768 => 197, 786\n\n        # Dropout Reg for better Generalization of the Attention Scores\n        regularized_rich_embeddings = self.dropout(rich_embeddings)\n\n        return regularized_rich_embeddings\n\n\n# Testing\nmhsa = MultiHeadSelfAttention()\nprint(mhsa)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T10:52:41.323731Z","iopub.execute_input":"2025-09-15T10:52:41.323978Z","iopub.status.idle":"2025-09-15T10:52:41.362937Z","shell.execute_reply.started":"2025-09-15T10:52:41.323956Z","shell.execute_reply":"2025-09-15T10:52:41.362297Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Feedforward Block","metadata":{}},{"cell_type":"code","source":"class FeedForward(torch.nn.Module):\n    \"\"\"This class implements a position-wise feed-forward network applied to each token in the sequence.\"\"\"\n\n    def __init__(self, embed_dims: int = 768) -> None:\n\n        # Inheriting all the properties of the Super Class\n        super().__init__()\n\n        # Sequential Feedfoward Block\n        self.feed_forward_block = torch.nn.Sequential(\n            torch.nn.Linear(in_features=embed_dims, out_features=embed_dims * 4, bias=True),  # Up-Projection from Attention Embeddings\n            torch.nn.GELU(),\n            torch.nn.Linear(in_features=embed_dims * 4, out_features=embed_dims, bias=True),  # Down-Projection for Residual Connection\n        )\n\n    def forward(self, X: torch.Tensor) -> torch.Tensor:\n        \"\"\"Implements the forward propagation of the simple feedfoward block.\n        \n        args:\n        - X: torch.Tensor -> Batch_Size, N_Patch, Embedding Dims\n        \n        returns:\n        - torch.Tensor -> Batch_Size, N_Patch, Embedding Dims\"\"\"\n\n        return self.feed_forward_block(X)\n\n\n# Testing\nffwd = FeedForward()\nprint(ffwd)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T10:52:41.363777Z","iopub.execute_input":"2025-09-15T10:52:41.364050Z","iopub.status.idle":"2025-09-15T10:52:41.412021Z","shell.execute_reply.started":"2025-09-15T10:52:41.364024Z","shell.execute_reply":"2025-09-15T10:52:41.411471Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Vision Encoder Block","metadata":{}},{"cell_type":"code","source":"class VisionEncoder(torch.nn.Module):\n    \"\"\"This class implements the complete Vision Encoder block for the Vision Transformer.\"\"\"\n\n    def __init__(self, embed_dims: int = 768, n_heads: int = 12, in_features: int = 768, dropout_rate: float = 0.2):\n\n        # Loading all the properties from the Super Class\n        super().__init__()\n\n        # MultiHead Self-Attention Block\n        self.mhsa_block = MultiHeadSelfAttention(\n            embed_dims=embed_dims,\n            n_heads=n_heads,\n            in_features=in_features,\n            dropout_rate=dropout_rate\n        )\n\n        # Feedfoward Block\n        self.ff_block = FeedForward(embed_dims=embed_dims)\n\n        # Normalization Layers\n        self.ln1 = torch.nn.LayerNorm(embed_dims)\n        self.ln2 = torch.nn.LayerNorm(embed_dims)\n\n    def forward(self, X: torch.Tensor) -> torch.Tensor:\n        \"\"\"Implements the complete forward propagation of a single Vision Encoder block.\"\"\"\n\n        # MultiHead Self-Attention Embeddings Calculation\n        attention_out = self.mhsa_block(self.ln1(X))\n        residual_attention_scores = attention_out + X\n\n        # Position-Wise Feedfoward Calculation\n        ff_logits = self.ff_block(self.ln2(residual_attention_scores))\n        final_residual_scores = ff_logits + residual_attention_scores\n\n        return final_residual_scores\n\n\n# Testing\nvision_encoder = VisionEncoder()\nprint(vision_encoder)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T10:52:41.414128Z","iopub.execute_input":"2025-09-15T10:52:41.414426Z","iopub.status.idle":"2025-09-15T10:52:41.483950Z","shell.execute_reply.started":"2025-09-15T10:52:41.414403Z","shell.execute_reply":"2025-09-15T10:52:41.483213Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## The Vision Transformer","metadata":{}},{"cell_type":"code","source":"class VisionTransformer(torch.nn.Module):\n    \"\"\"This class implements the complete Vision Transformer from scratch.\"\"\"\n\n    def __init__(\n            self, n_classes: int, n_layers: int = 12, image_size: int = 224, \n            patch_size: int = 16, embed_dims: int = 768, n_heads: int = 12, \n            in_features: int = 768, dropout_rate: float = 0.2\n        ) -> None:\n\n        # Loading all the properties from the Super Class\n        super().__init__()\n\n        # Vision Transformer Properties\n        self.n_patches = image_size // patch_size\n\n        # Initial Convolution Layer\n        self.patch_conv = torch.nn.Conv2d(\n            in_channels=3,\n            out_channels=embed_dims,\n            kernel_size=patch_size,\n            stride=patch_size\n        )\n\n        # Positional Embeddings\n        self.positional_embeddings = torch.nn.Embedding(\n            num_embeddings=(self.n_patches ** 2) + 1,\n            embedding_dim=embed_dims\n        )\n\n        # Vision Transformer Encoder Blocks\n        self.deep_encoder_blocks = torch.nn.Sequential(\n            *[\n                VisionEncoder(\n                    embed_dims=embed_dims,\n                    n_heads=n_heads,\n                    in_features=in_features,\n                    dropout_rate=dropout_rate\n                )\n                for _ in range(n_layers)\n            ]\n        )\n\n        # Final Layer Norm before MLP Head\n        self.final_ln = torch.nn.LayerNorm(embed_dims)\n\n        # MLP Head\n        self.mlp_head = torch.nn.Linear(\n            in_features=embed_dims,\n            out_features=n_classes,\n            bias=True\n        )\n\n        # Learnable Class Token a small random value\n        self.cls_token = torch.nn.Parameter(\n            data=torch.randn(1, 1, embed_dims) * 0.02\n        )\n\n    def forward(self, X: torch.Tensor) -> torch.Tensor:\n        \"\"\"Implements the forward propagation of the complete Vision Transformer.\"\"\"\n\n        # Initial Conv\n        image_conv: torch.Tensor = self.patch_conv(X)  # Batch_Size, 768, 14, 14\n\n        # Patch Embeddings\n        image_tensors: torch.Tensor = image_conv.flatten(start_dim=-2, end_dim=-1)  # Batch_Size, 768, 196\n        image_patches: torch.Tensor = image_tensors.permute(dims=[0, 2, 1])         # Batch_Size, 196, 768\n\n        # Prepending the Learnable CLS Token\n        image_patches = torch.cat(\n            [\n                self.cls_token.expand(X.shape[0], -1, -1),\n                image_patches\n            ],\n            dim=1\n        )\n\n        # Position Embedding\n        pos_scores = self.positional_embeddings(\n            torch.arange(\n                start=0, \n                end=(self.n_patches ** 2) + 1,\n                device=torch.accelerator.current_accelerator()\n            )\n        )\n\n        # Image Patch Embeddings\n        image_patch_embeddings = image_patches + pos_scores\n\n        # Deep Vision Encoder blocks\n        deep_logits = self.deep_encoder_blocks(image_patch_embeddings)\n\n        # MLP Head\n        final_logits = self.mlp_head(self.final_ln(deep_logits[:, 0]))\n\n        return final_logits\n\n\n# Testing\nfirst_vit = VisionTransformer(n_classes=n_classes)\nprint(\"Model Architecture:\\n\")\nprint(first_vit, end=\"\\n\\n\")\n\nnum_params = sum(module.numel() for module in first_vit.parameters() if module.requires_grad)\nprint(f\"Total no of learnable params: {num_params}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T10:52:41.484701Z","iopub.execute_input":"2025-09-15T10:52:41.484935Z","iopub.status.idle":"2025-09-15T10:52:42.226389Z","shell.execute_reply.started":"2025-09-15T10:52:41.484918Z","shell.execute_reply":"2025-09-15T10:52:42.225732Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Training Loop Handler","metadata":{}},{"cell_type":"code","source":"CHECKPOINT_PATH = \"./models\"\n\n\nclass TrainingLoop:\n    \"\"\"This class handles the training loop for the models.\"\"\"\n    def __init__(self, learning_rate: float, model: torch.nn.Module):\n        self.model = model\n        self.optim = torch.optim.AdamW(\n            params=self.model.parameters(),\n            lr=learning_rate\n        )\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        self.loss_fn = torch.nn.CrossEntropyLoss()\n\n        # Learning Rate Scheduler\n        self.lr_schedule = ReduceLROnPlateau(\n            optimizer=self.optim, mode=\"min\", patience=2, min_lr=1e-7\n        )\n\n        # Creating the Checkpoint Storage Directory\n        self.model_dir = Path(CHECKPOINT_PATH)\n        if not self.model_dir.exists():\n            self.model_dir.mkdir()\n    \n    def train_model(\n            self, epochs: int,\n            train_set: DataLoader, valid_set: DataLoader\n        ) -> tuple[list[float], list[float]]:\n        \"\"\"Trains the model for the given number of epochs.\"\"\"\n\n        # Cache Losses\n        train_losses = []\n        valid_losses = []\n\n        # Mean Training Variables\n        mean_loss_train = 0\n        mean_loss_valid = 0\n\n        # Training Checkpoint\n        best_valid_loss = torch.inf\n        patience = 5\n        patience_counter = 0\n\n        # Training Loop\n        print(\"The training process has started\")\n        for i in range(epochs):\n\n            # Average Epoch Time tracking\n            start = time.time()\n\n            # ==== Training Step ====\n\n            # Completing a single epoch\n            for X, y in train_set:\n\n                # Moving the batches to GPU\n                X, y = X.to(self.device), y.to(self.device)\n            \n                # Training Step\n                logits = self.model(X)\n\n                # Loss Calculation\n                train_loss = self.loss_fn(input=logits, target=y)\n                train_loss = train_loss.sum()\n                mean_loss_train += train_loss.item()\n\n                # Backpropagation\n                self.optim.zero_grad()\n                train_loss.backward()\n                self.optim.step()\n\n            # Completion of Epoch\n            end = time.time()\n\n            # ==== Validation Step ====\n\n            # Turning on the Eval mode on the model for the BN-Layers\n            self.model.eval()\n            with torch.no_grad():\n                for X, y in valid_set:\n\n                    # Moving the batches to GPU\n                    X, y = X.to(self.device), y.to(self.device)\n\n                    # Validation Calculation\n                    logits = self.model(X)\n\n                    # Loss Calculation\n                    valid_loss = self.loss_fn(input=logits, target=y)\n                    valid_loss = valid_loss.sum()\n                    mean_loss_valid += valid_loss.item()\n            \n            # Switching the model back to training mode\n            self.model.train()\n\n            # ==== End of Epoch Metrics & Model Checkpointing ====\n            mean_loss_train /= len(train_set)\n            mean_loss_valid /= len(valid_set)\n            time_epoch = end - start\n\n            # Update the LR-Schedule\n            self.lr_schedule.step(mean_loss_valid)\n\n            # Updating the Caches\n            train_losses.append(mean_loss_train)\n            valid_losses.append(mean_loss_valid)\n\n            print(f\"Epoch {i + 1}: Train Loss -> {mean_loss_train:.4f} | Valid Loss -> {mean_loss_valid:.4f} | Time Epoch -> {time_epoch:.4f}\")\n\n            # Updating the best validation loss so far\n            if mean_loss_valid < best_valid_loss:\n                best_valid_loss = mean_loss_valid\n\n                # Saving the Model by weights\n                torch.save(obj=self.model.state_dict(), f=self.model_dir / \"vit_base_16.pth\")\n                print(\"New best model was saved\")\n                patience_counter = 0\n            else:\n                patience_counter += 1\n                print(f\"No improvement: {patience_counter} / {patience}\")\n                if patience_counter >= patience:\n                    print(\"Early Stopping\")\n                    break\n\n            # ==== Reset the Training Loop Metrics ====\n            mean_loss_train, mean_loss_valid = 0, 0\n        \n        return train_losses, valid_losses\n\n\n# Testing\noptim = TrainingLoop(learning_rate=1e-4, model=first_vit)\nprint(optim)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T10:52:42.227074Z","iopub.execute_input":"2025-09-15T10:52:42.227248Z","iopub.status.idle":"2025-09-15T10:52:42.340815Z","shell.execute_reply.started":"2025-09-15T10:52:42.227232Z","shell.execute_reply":"2025-09-15T10:52:42.339819Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## The Main Function","metadata":{}},{"cell_type":"code","source":"# Learning Curve Asset Path\nASSET_PATH = Path(\"/kaggle/working/assets\")\n\n# Accelerator Device\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Accelerator Available: {DEVICE}\")\nprint(f\"Units: {torch.cuda.device_count()}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T10:52:42.341780Z","iopub.execute_input":"2025-09-15T10:52:42.341985Z","iopub.status.idle":"2025-09-15T10:52:42.485721Z","shell.execute_reply.started":"2025-09-15T10:52:42.341970Z","shell.execute_reply":"2025-09-15T10:52:42.484922Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def main():\n    # Initialising the DataHandler\n    data_handle = DataHandler(root_dir=root_working)\n\n    # Loading the Dataset\n    loaded_train_set = data_handle.load_set(\"train\")\n    loaded_valid_set = data_handle.load_set(\"valid\")\n    \n    # Initial sample sizes\n    print(\"Initial Sample Sizes:\")\n    train_count = Counter(loaded_train_set.targets)\n    valid_count = Counter(loaded_valid_set.targets)\n    print(f\"Train Set:\\n{train_count}\")\n    print(f\"\\nValid Set:\\n{valid_count}\")\n    print(\"\\n ------- \\n\")\n\n    # Moving Samples\n    train_set_pre_prep, valid_set_pre_prep = data_handle.move_samples_from_train(loaded_train_set, move_percent=0.15)\n    \n    print(\"Moved Sample Sizes:\")\n    train_targets = [loaded_train_set.targets[i] for i in train_set_pre_prep.indices]\n    valid_targets = [loaded_train_set.targets[i] for i in valid_set_pre_prep.indices]\n    train_count = Counter(train_targets)\n    valid_count = Counter(valid_targets)\n    print(f\"Train Set:\\n{train_count}\")\n    print(f\"\\nValid Set:\\n{valid_count}\")\n    print(\"\\n ------- \\n\")\n\n    # Preparing the sets\n    train_set = data_handle.prepare_dataset(train_set_pre_prep)\n    valid_set_combined = ConcatDataset([loaded_valid_set, valid_set_pre_prep])\n    valid_set = data_handle.prepare_dataset(valid_set_combined)\n\n    # Loading the model\n    model_base = VisionTransformer(n_classes=n_classes).to(DEVICE)\n    vit_base_16 = torch.nn.DataParallel(model_base)\n\n    # Loading the Training Loop Handler\n    optimizer = TrainingLoop(learning_rate=1e-4, model=vit_base_16)\n\n    # Training and Validating the model\n    train_losses, valid_losses = optimizer.train_model(30, train_set=train_set, valid_set=valid_set)\n\n    # Plotting the losses\n    plt.figure(figsize=(10, 8))\n    plt.title(\"Learning Curve\")\n    plt.plot(range(1, len(train_losses) + 1), train_losses, c=\"b\", ls=\"-\", label=\"Train Loss\")\n    plt.plot(range(1, len(valid_losses) + 1), valid_losses, c=\"g\", ls=\"-.\", label=\"Valid Loss\")\n    plt.legend(loc=\"upper right\")\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(\"Loss\")\n\n    # Storing the Learning Curve\n    if not ASSET_PATH.exists():\n        ASSET_PATH.mkdir()\n    plt.savefig(ASSET_PATH / \"learning_curve.png\")\n\n    # Rendering the plot\n    plt.show()\n\n\n# ==== Driver Code ====\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T10:52:42.486491Z","iopub.execute_input":"2025-09-15T10:52:42.486738Z","iopub.status.idle":"2025-09-15T10:58:32.711495Z","shell.execute_reply.started":"2025-09-15T10:52:42.486710Z","shell.execute_reply":"2025-09-15T10:58:32.710685Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Clearing Train and Valid in Working\nif root_working.exists():\n    shutil.rmtree(root_working)\n\nprint(\"Cleared Working Path!!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T10:58:32.712654Z","iopub.execute_input":"2025-09-15T10:58:32.712969Z","iopub.status.idle":"2025-09-15T10:58:33.039477Z","shell.execute_reply.started":"2025-09-15T10:58:32.712936Z","shell.execute_reply":"2025-09-15T10:58:33.038875Z"}},"outputs":[],"execution_count":null}]}