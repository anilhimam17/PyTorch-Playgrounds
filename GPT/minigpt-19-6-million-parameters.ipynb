{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef6aa32e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T21:11:30.542875Z",
     "iopub.status.busy": "2025-08-31T21:11:30.542667Z",
     "iopub.status.idle": "2025-08-31T21:11:35.616155Z",
     "shell.execute_reply": "2025-08-31T21:11:35.615579Z"
    },
    "papermill": {
     "duration": 5.078717,
     "end_time": "2025-08-31T21:11:35.617619",
     "exception": false,
     "start_time": "2025-08-31T21:11:30.538902",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "import time\n",
    "from typing import Generator\n",
    "from pathlib import Path\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11ce87ca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T21:11:35.624419Z",
     "iopub.status.busy": "2025-08-31T21:11:35.623697Z",
     "iopub.status.idle": "2025-08-31T21:11:35.630522Z",
     "shell.execute_reply": "2025-08-31T21:11:35.629836Z"
    },
    "papermill": {
     "duration": 0.010953,
     "end_time": "2025-08-31T21:11:35.631605",
     "exception": false,
     "start_time": "2025-08-31T21:11:35.620652",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "DATASET_PATH = Path(\"/kaggle/input/imdb-50k-movie-reviews-test-your-bert\")\n",
    "\n",
    "\n",
    "class IMDBMovieReview:\n",
    "    \"\"\"Class implements all the dataset loading, handling and metrics.\"\"\"\n",
    "    def __init__(self) -> None:\n",
    "        \n",
    "        self.dataframe = pd.read_csv(\n",
    "            filepath_or_buffer=DATASET_PATH / \"train.csv\",\n",
    "            encoding=\"utf-8\"\n",
    "        )\n",
    "\n",
    "    def __iter__(self) -> Generator[str, str, None]:\n",
    "        for i in range(10):\n",
    "            yield self.dataframe[\"text\"].loc[i]\n",
    "\n",
    "    def refine_structure(self) -> str:\n",
    "        \"\"\"Focuses the entire dataframe to only the text removing other cols.\"\"\"\n",
    "        if \"sentiment\" in self.dataframe.columns:\n",
    "            self.dataframe = self.dataframe.drop([\"sentiment\"], axis=1)\n",
    "        self.dataset_string = \"\\n\".join(self.dataframe[\"text\"])\n",
    "        \n",
    "        # Cleaning up the string\n",
    "        self.dataset_string = re.sub(r'[^\\x00-\\x7F]+', \" \", self.dataset_string)\n",
    "        self.dataset_string = re.sub(r'[\\U00010000-\\U0010ffff]+', \" \", self.dataset_string)\n",
    "        self.dataset_string = re.sub(r'[\\x08\\x10#\\$%&\\*\\+<=>@\\[\\\\\\]\\^_`\\{\\|\\}~]', \" \", self.dataset_string)\n",
    "        \n",
    "        # Generating the vocabulary for the dataset string.\n",
    "        self.vocab = sorted(list(set(self.dataset_string)))\n",
    "\n",
    "        return self.dataset_string\n",
    "    \n",
    "    def datastring_metrics(self) -> None:\n",
    "        \"\"\"Provides metrics for the dataset string.\"\"\"\n",
    "\n",
    "        print(f\"Length of the Dataset: {len(self.dataset_string)}\\n\")\n",
    "        print(f\"First 1000 Chars:\\n{self.dataset_string[:1000]}\\n\")\n",
    "        print(f\"Vocabulary Size: {len(self.vocab)}\\n\")\n",
    "        print(f\"Vocabulary:\\n{''.join(self.vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1cd0508",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T21:11:35.637135Z",
     "iopub.status.busy": "2025-08-31T21:11:35.636926Z",
     "iopub.status.idle": "2025-08-31T21:11:35.643647Z",
     "shell.execute_reply": "2025-08-31T21:11:35.642950Z"
    },
    "papermill": {
     "duration": 0.01071,
     "end_time": "2025-08-31T21:11:35.644717",
     "exception": false,
     "start_time": "2025-08-31T21:11:35.634007",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SelfAttentionHead(torch.nn.Module):\n",
    "    \"\"\"Class implements a single self-attention head.\"\"\"\n",
    "    def __init__(self, head_size: int, input_features: int, block_size: int, dropout_rate: float = 0.2):\n",
    "        super().__init__()\n",
    "\n",
    "        # Attention Matrices\n",
    "        self.queries = torch.nn.Linear(input_features, head_size, bias=False)\n",
    "        self.keys = torch.nn.Linear(input_features, head_size, bias=False)\n",
    "        self.values = torch.nn.Linear(input_features, head_size, bias=False)\n",
    "        self.register_buffer(\"tril\", torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        # Dropout\n",
    "        self.dropout = torch.nn.Dropout(p=dropout_rate)\n",
    "\n",
    "    def forward(self, X: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Implements the forward propagation of the Self Attention Head.\"\"\"\n",
    "        B, T, C = X.shape\n",
    "\n",
    "        # Calculating the Queries, Keys and Values as Linear Projections\n",
    "        queries: torch.Tensor = self.queries(X)\n",
    "        keys: torch.Tensor = self.keys(X)\n",
    "        values: torch.Tensor = self.values(X)\n",
    "\n",
    "        # Calculating the Dot Product of the Queries and Keys for the Attention Pattern\n",
    "        attention_pattern: torch.Tensor = queries @ keys.transpose(-2, -1) * C ** -0.5\n",
    "        attention_pattern = attention_pattern.masked_fill(self.tril[:T, :T] == 0, -torch.inf)  # type: ignore\n",
    "        attention_pattern = torch.nn.functional.softmax(attention_pattern, dim=-1)\n",
    "\n",
    "        # Regularization of the Attention Patterns\n",
    "        reg_attention_pattern = self.dropout(attention_pattern)\n",
    "\n",
    "        # Weighted Sum\n",
    "        output_attended_embeddings = reg_attention_pattern @ values\n",
    "        return output_attended_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25abb8f7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T21:11:35.650324Z",
     "iopub.status.busy": "2025-08-31T21:11:35.650117Z",
     "iopub.status.idle": "2025-08-31T21:11:35.655548Z",
     "shell.execute_reply": "2025-08-31T21:11:35.654840Z"
    },
    "papermill": {
     "duration": 0.009556,
     "end_time": "2025-08-31T21:11:35.656705",
     "exception": false,
     "start_time": "2025-08-31T21:11:35.647149",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(torch.nn.Module):\n",
    "    \"\"\"Implements a Multi Headed Attention Module.\n",
    "\n",
    "    Where each attention head is a parallel implementation of a distinct Self Attention Head.\n",
    "    The Multi Headed Attention Module runs several Self Attention Heads in parallel and concatenates \n",
    "    the final output generated by each head on the Channel Dimension.\"\"\"\n",
    "\n",
    "    def __init__(self, num_heads: int, head_size: int, block_size: int, n_embd: int, dropout_rate: float = 0.2) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # Parallel Self Attention Heads\n",
    "        self.multiple_attention_heads = torch.nn.ModuleList(\n",
    "            [\n",
    "                SelfAttentionHead(head_size=head_size, input_features=n_embd, block_size=block_size)\n",
    "                for _ in range(num_heads)\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        # Dropout Layer\n",
    "        self.dropout = torch.nn.Dropout(p=dropout_rate)\n",
    "\n",
    "    def forward(self, X) -> torch.Tensor:\n",
    "        \"\"\"Implements the forward propagation of the MultiHead Attention Layer.\"\"\"\n",
    "        \n",
    "        multihead_attention_pattern = torch.cat(\n",
    "            [head(X) for head in self.multiple_attention_heads], dim=-1\n",
    "        )\n",
    "        out_attention = self.dropout(multihead_attention_pattern)\n",
    "\n",
    "        return out_attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26ab2842",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T21:11:35.662629Z",
     "iopub.status.busy": "2025-08-31T21:11:35.661872Z",
     "iopub.status.idle": "2025-08-31T21:11:35.666600Z",
     "shell.execute_reply": "2025-08-31T21:11:35.665890Z"
    },
    "papermill": {
     "duration": 0.008672,
     "end_time": "2025-08-31T21:11:35.667656",
     "exception": false,
     "start_time": "2025-08-31T21:11:35.658984",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FeedForward(torch.nn.Module):\n",
    "    \"\"\"Implements a simple sequential feedforward \n",
    "    network to decide the next token based on attention.\"\"\"\n",
    "\n",
    "    def __init__(self, n_embd: int = 64, dropout_rate: float = 0.2) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.sequential = torch.nn.Sequential(\n",
    "            torch.nn.Linear(in_features=n_embd, out_features=4*n_embd),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(4*n_embd, n_embd),\n",
    "            torch.nn.Dropout(p=dropout_rate)\n",
    "        )\n",
    "\n",
    "    def forward(self, X) -> torch.Tensor:\n",
    "        \"\"\"Implements the forward propagation of the feedforward model.\"\"\"\n",
    "        return self.sequential(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b76cb0a4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T21:11:35.673331Z",
     "iopub.status.busy": "2025-08-31T21:11:35.672740Z",
     "iopub.status.idle": "2025-08-31T21:11:35.677593Z",
     "shell.execute_reply": "2025-08-31T21:11:35.677081Z"
    },
    "papermill": {
     "duration": 0.008793,
     "end_time": "2025-08-31T21:11:35.678698",
     "exception": false,
     "start_time": "2025-08-31T21:11:35.669905",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class GPTDecoderBlock(torch.nn.Module):\n",
    "    \"\"\"Implements a single GPT decoder block comprising the Attention Mechanism and the FeedForward Network.\"\"\"\n",
    "\n",
    "    def __init__(self, num_heads: int, n_embd: int, block_size: int) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # Multihead Attention Block\n",
    "        self.multihead_attention = MultiHeadAttention(\n",
    "            num_heads=num_heads, \n",
    "            head_size=n_embd//num_heads, \n",
    "            block_size=block_size,\n",
    "            n_embd=n_embd\n",
    "        )\n",
    "        # Feedforward Network Block\n",
    "        self.ffwd = FeedForward(n_embd=n_embd)\n",
    "\n",
    "        # Layer Normalisation Blocks\n",
    "        self.ln1 = torch.nn.LayerNorm(n_embd)\n",
    "        self.ln2 = torch.nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, X: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Implements the forward propagation of a single GPT decoder block.\"\"\"\n",
    "        \n",
    "        attention_out = self.multihead_attention(self.ln1(X))\n",
    "        x = attention_out + X\n",
    "        ffwd_out = self.ffwd(self.ln2(x))\n",
    "        residual_scores = ffwd_out + x\n",
    "        return residual_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f227f4b8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T21:11:35.684027Z",
     "iopub.status.busy": "2025-08-31T21:11:35.683826Z",
     "iopub.status.idle": "2025-08-31T21:11:35.694150Z",
     "shell.execute_reply": "2025-08-31T21:11:35.693475Z"
    },
    "papermill": {
     "duration": 0.014232,
     "end_time": "2025-08-31T21:11:35.695191",
     "exception": false,
     "start_time": "2025-08-31T21:11:35.680959",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MyGPT(torch.nn.Module):\n",
    "    \"\"\"Class implments the Generatively Pretrained Transformer from scratch using PyTorch.\"\"\"\n",
    "    def __init__(self, vocab: list[str] = [], decoder_layers: int = 6, n_embd: int = 64, block_size: int = 128, attention_heads: int = 4) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # Hyperparameters\n",
    "        self.vocab_size = len(vocab)\n",
    "        self.block_size = block_size\n",
    "\n",
    "        # Tokenization Maps\n",
    "        self.encode_map = {ch: i for i, ch in enumerate(vocab)}\n",
    "        self.decode_map = {i: ch for i, ch in enumerate(vocab)}\n",
    "\n",
    "        # Embedding Layers\n",
    "        self.token_embed = torch.nn.Embedding(self.vocab_size, n_embd)\n",
    "        self.position_embed = torch.nn.Embedding(self.block_size, n_embd)\n",
    "\n",
    "        # GPT Decoder Blocks\n",
    "        self.decoder_blocks = torch.nn.Sequential(\n",
    "            *[\n",
    "                GPTDecoderBlock(num_heads=attention_heads, n_embd=n_embd, block_size=block_size)\n",
    "                for _ in range(decoder_layers)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Final Normalization Layer\n",
    "        self.final_ln = torch.nn.LayerNorm(n_embd)\n",
    "\n",
    "        # Linear Layers\n",
    "        self.last_linear = torch.nn.Linear(n_embd, self.vocab_size)\n",
    "\n",
    "    def encode(self, input_string: str = \"\") -> list[int]:\n",
    "        \"\"\"Encode operation for the simple character level tokenizer.\"\"\"\n",
    "        return [self.encode_map[ch] for ch in input_string]\n",
    "\n",
    "    def decode(self, input_seq: list[int] = []) -> str:\n",
    "        \"\"\"Invert operation for the simple character level tokenizer.\"\"\"\n",
    "        return \"\".join([self.decode_map[token] for token in input_seq])\n",
    "    \n",
    "    def forward(self, X: torch.Tensor, y: torch.Tensor | None = None) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Implements the forward propagation of the model.\"\"\"\n",
    "\n",
    "        B, T = X.shape\n",
    "\n",
    "        # Initial Scores\n",
    "        embed_score = self.token_embed(X)\n",
    "        pos_score = self.position_embed(torch.arange(T, device=torch.accelerator.current_accelerator()))\n",
    "        x = embed_score + pos_score\n",
    "\n",
    "        # GPT Decoder Blocks\n",
    "        block_scores = self.decoder_blocks(x)\n",
    "\n",
    "        # Final Normalization\n",
    "        block_scores_norm = self.final_ln(block_scores)\n",
    "\n",
    "        # Deeper Layers\n",
    "        logits = self.last_linear(block_scores_norm)\n",
    "\n",
    "        loss = torch.tensor([])\n",
    "        if y is not None:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view((B * T), C)\n",
    "            y = y.view(B * T)\n",
    "            loss = torch.nn.functional.cross_entropy(input=logits, target=y)\n",
    "\n",
    "        return loss, logits\n",
    "    \n",
    "    def generate(self, previous_tokens: torch.Tensor, max_tokens: int) -> torch.Tensor:\n",
    "        \"\"\"Generates novel tokens based on the previous context.\"\"\"\n",
    "\n",
    "        for i in range(max_tokens):\n",
    "            \n",
    "            # Clipping the generations to the last block size tokens\n",
    "            last_block_size = previous_tokens[:, -self.block_size:]\n",
    "\n",
    "            # Generating the next token\n",
    "            _, logits = self(last_block_size)\n",
    "\n",
    "            # Taking the consideration of only the last token\n",
    "            logits = logits[:, -1, :]\n",
    "\n",
    "            # Getting the probabilities of the words\n",
    "            probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, 1)\n",
    "\n",
    "            previous_tokens = torch.cat((previous_tokens, idx_next), dim=1)\n",
    "\n",
    "        return previous_tokens\n",
    "    \n",
    "    def save_model(self) -> None:\n",
    "        \"\"\"Saves the weights of the trained model.\"\"\"\n",
    "\n",
    "        torch.save(self.state_dict(), \"my_gpt_weights.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7cef3a31",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T21:11:35.700215Z",
     "iopub.status.busy": "2025-08-31T21:11:35.700041Z",
     "iopub.status.idle": "2025-08-31T21:11:35.706889Z",
     "shell.execute_reply": "2025-08-31T21:11:35.706398Z"
    },
    "papermill": {
     "duration": 0.010531,
     "end_time": "2025-08-31T21:11:35.707963",
     "exception": false,
     "start_time": "2025-08-31T21:11:35.697432",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DataPreprocessor:\n",
    "    \"\"\"Class handles all of the data pre-processing necessary for the dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, data_string: str, device: str = \"\") -> None:\n",
    "        self.data_string = data_string\n",
    "        self.device = device if device else None\n",
    "\n",
    "    def create_data_tensor(self, model: MyGPT) -> None:\n",
    "        \"\"\"Create the data tensor on accelerator device.\"\"\"\n",
    "        \n",
    "        token_list = model.module.encode(self.data_string)\n",
    "        self.data_tensor = torch.tensor(\n",
    "            data=token_list,\n",
    "            dtype=torch.long\n",
    "        )\n",
    "        self.tensor_size = self.data_tensor.size()[0]\n",
    "    \n",
    "    def train_valid_test(self, valid_percentage: float = 0.1, test_percentage: float = 0.05) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Peforms the split on the data tensor.\"\"\"\n",
    "\n",
    "        size_valid = int(self.tensor_size * valid_percentage)\n",
    "        size_test = int(self.tensor_size * test_percentage)\n",
    "        size_train = self.tensor_size - (size_valid + size_test)\n",
    "\n",
    "        train_set = self.data_tensor[:size_train].clone().to(self.device)\n",
    "        valid_set = self.data_tensor[size_train : size_train + size_valid].clone().to(self.device)\n",
    "        test_set = self.data_tensor[size_train + size_valid :].clone().to(self.device)\n",
    "\n",
    "        return train_set, valid_set, test_set\n",
    "    \n",
    "    def get_batch(self, set_: torch.Tensor, batch_size: int, block_size: int) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Batches the data_tensors and provides the pointer to the result.\"\"\"\n",
    "\n",
    "        # Creating random batches\n",
    "        ix = torch.randint(0, set_.size()[0] - block_size, (batch_size,))\n",
    "        X = torch.stack([set_[i : i + block_size] for i in ix])\n",
    "        y = torch.stack([set_[i + 1 : i + 1 + block_size] for i in ix])\n",
    "\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd539b2a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T21:11:35.713291Z",
     "iopub.status.busy": "2025-08-31T21:11:35.713098Z",
     "iopub.status.idle": "2025-08-31T21:11:35.719691Z",
     "shell.execute_reply": "2025-08-31T21:11:35.719194Z"
    },
    "papermill": {
     "duration": 0.010352,
     "end_time": "2025-08-31T21:11:35.720680",
     "exception": false,
     "start_time": "2025-08-31T21:11:35.710328",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class OptimizationLoop:\n",
    "    \"\"\"Class implements the train-valid and test loops.\"\"\"\n",
    "    def __init__(self, preprocessor: DataPreprocessor, model: MyGPT, learning_rate: float) -> tuple[list[float], list[float]]:\n",
    "        self.preprocessor = preprocessor\n",
    "        self.model = model\n",
    "        self.optimizer = torch.optim.AdamW(self.model.parameters(), lr=learning_rate)\n",
    "\n",
    "    def train(\n",
    "            self, epochs: int, train_set: torch.Tensor, valid_set: torch.Tensor,\n",
    "            batch_size: int, block_size: int) -> None:\n",
    "        \"\"\"Implements the PyTorch Training Loop for the model.\"\"\"\n",
    "\n",
    "        # Mean Loss Variables\n",
    "        mean_train_loss = 0\n",
    "        mean_valid_loss = 0\n",
    "        mean_time = 0\n",
    "        train_losses = []\n",
    "        valid_losses = []\n",
    "\n",
    "        # Training Loop\n",
    "        for i in range(epochs):\n",
    "            sample_train_X, sample_train_y = self.preprocessor.get_batch(train_set, batch_size, block_size)\n",
    "            sample_valid_X, sample_valid_y = self.preprocessor.get_batch(valid_set, batch_size, block_size)\n",
    "\n",
    "            # Timing the execution\n",
    "            start = time.time()\n",
    "\n",
    "            # Training Step\n",
    "            loss_train, _ = self.model(sample_train_X, sample_train_y)\n",
    "            loss_train = loss_train.sum()\n",
    "            self.optimizer.zero_grad()\n",
    "            loss_train.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            # Stop Time\n",
    "            stop = time.time()\n",
    "\n",
    "            # Validation Step\n",
    "            with torch.no_grad():\n",
    "                loss_valid, _ = self.model(sample_valid_X, sample_valid_y)\n",
    "                loss_valid = loss_valid.sum()\n",
    "\n",
    "            mean_train_loss += loss_train.item()\n",
    "            mean_valid_loss += loss_valid.item()\n",
    "            mean_time += stop - start\n",
    "            if (i + 1) % 100 == 0:\n",
    "                mean_train_loss /= 100\n",
    "                mean_valid_loss /= 100\n",
    "                mean_time /= 100\n",
    "                print(f\"Loss at {i + 1}th Epoch -> Train Set: {mean_train_loss:.4f} | Valid Set: {mean_valid_loss:.4f} | Avg Step-Time: {mean_time:.3f} secs\")\n",
    "                \n",
    "                train_losses.append(mean_train_loss)\n",
    "                valid_losses.append(mean_valid_loss)\n",
    "                mean_train_loss, mean_valid_loss, mean_time = 0, 0, 0\n",
    "\n",
    "        # Returning the Tracking of Losses for Plotting\n",
    "        return train_losses, valid_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0a792994",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T21:11:35.725952Z",
     "iopub.status.busy": "2025-08-31T21:11:35.725760Z",
     "iopub.status.idle": "2025-08-31T21:11:35.832375Z",
     "shell.execute_reply": "2025-08-31T21:11:35.831617Z"
    },
    "papermill": {
     "duration": 0.110644,
     "end_time": "2025-08-31T21:11:35.833588",
     "exception": false,
     "start_time": "2025-08-31T21:11:35.722944",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accelerator Available: cuda\n",
      "Units: 2\n"
     ]
    }
   ],
   "source": [
    "# Accelerator Device\n",
    "acc_device = torch.accelerator.current_accelerator()\n",
    "print(f\"Accelerator Available: {acc_device}\")\n",
    "print(f\"Units: {torch.cuda.device_count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2098fab1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T21:11:35.839743Z",
     "iopub.status.busy": "2025-08-31T21:11:35.839109Z",
     "iopub.status.idle": "2025-09-01T06:37:51.649067Z",
     "shell.execute_reply": "2025-09-01T06:37:51.648356Z"
    },
    "papermill": {
     "duration": 33975.821907,
     "end_time": "2025-09-01T06:37:51.658051",
     "exception": false,
     "start_time": "2025-08-31T21:11:35.836144",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datastring Metrics after loading the dataset on Vocab:\n",
      "Length of the Dataset: 33151655\n",
      "\n",
      "First 1000 Chars:\n",
      "Now, I won't deny that when I purchased this off eBay, I had high expectations. This was an incredible out-of-print work from the master of comedy that I so enjoy. However, I was soon to be disappointed. Apologies to those who enjoyed it, but I just found the Compleat Al to be very difficult to watch. I got a few smiles, sure, but the majority of the funny came from the music videos (which I've got on DVD) and the rest was basically filler. You could tell that this was not Al's greatest video achievement (that honor goes to UHF). Honestly, I doubt if this will ever make the jump to DVD, so if you're an ultra-hardcore Al fan and just HAVE to own everything, buy the tape off eBay. Just don't pay too much for it.\n",
      "The saddest thing about this \"tribute\" is that almost all the singers (including the otherwise incredibly talented Nick Cave) seem to have missed the whole point where Cohen's intensity lies: by delivering his lines in an almost tuneless poise, Cohen transmits the full extent of \n",
      "\n",
      "Vocabulary Size: 77\n",
      "\n",
      "Vocabulary:\n",
      "\t\n",
      " !\"'(),-./0123456789:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "\n",
      "-----------\n",
      "\n",
      "Model Vocabulary from the Character-Level Tokenizer:\n",
      " ['\\t', '\\n', ' ', '!', '\"', \"'\", '(', ')', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "\n",
      "-----------\n",
      "\n",
      "Training the Model:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at 100th Epoch -> Train Set: 5.5996 | Valid Set: 5.5615 | Avg Step-Time: 0.884 secs\n",
      "Loss at 200th Epoch -> Train Set: 5.0267 | Valid Set: 5.0212 | Avg Step-Time: 0.953 secs\n",
      "Loss at 300th Epoch -> Train Set: 4.9421 | Valid Set: 4.9345 | Avg Step-Time: 0.965 secs\n",
      "Loss at 400th Epoch -> Train Set: 4.7910 | Valid Set: 4.7915 | Avg Step-Time: 0.960 secs\n",
      "Loss at 500th Epoch -> Train Set: 4.4059 | Valid Set: 4.4035 | Avg Step-Time: 0.962 secs\n",
      "Loss at 600th Epoch -> Train Set: 3.9741 | Valid Set: 3.9725 | Avg Step-Time: 0.963 secs\n",
      "Loss at 700th Epoch -> Train Set: 3.6667 | Valid Set: 3.6691 | Avg Step-Time: 0.964 secs\n",
      "Loss at 800th Epoch -> Train Set: 3.4585 | Valid Set: 3.4583 | Avg Step-Time: 0.960 secs\n",
      "Loss at 900th Epoch -> Train Set: 3.2986 | Valid Set: 3.3037 | Avg Step-Time: 0.962 secs\n",
      "Loss at 1000th Epoch -> Train Set: 3.1829 | Valid Set: 3.1858 | Avg Step-Time: 0.961 secs\n",
      "Loss at 1100th Epoch -> Train Set: 3.0897 | Valid Set: 3.0970 | Avg Step-Time: 0.963 secs\n",
      "Loss at 1200th Epoch -> Train Set: 3.0212 | Valid Set: 3.0283 | Avg Step-Time: 0.956 secs\n",
      "Loss at 1300th Epoch -> Train Set: 2.9567 | Valid Set: 2.9699 | Avg Step-Time: 0.959 secs\n",
      "Loss at 1400th Epoch -> Train Set: 2.9100 | Valid Set: 2.9193 | Avg Step-Time: 0.958 secs\n",
      "Loss at 1500th Epoch -> Train Set: 2.8688 | Valid Set: 2.8782 | Avg Step-Time: 0.961 secs\n",
      "Loss at 1600th Epoch -> Train Set: 2.8294 | Valid Set: 2.8422 | Avg Step-Time: 0.961 secs\n",
      "Loss at 1700th Epoch -> Train Set: 2.7926 | Valid Set: 2.8040 | Avg Step-Time: 0.959 secs\n",
      "Loss at 1800th Epoch -> Train Set: 2.7667 | Valid Set: 2.7755 | Avg Step-Time: 0.961 secs\n",
      "Loss at 1900th Epoch -> Train Set: 2.7472 | Valid Set: 2.7526 | Avg Step-Time: 0.955 secs\n",
      "Loss at 2000th Epoch -> Train Set: 2.7142 | Valid Set: 2.7329 | Avg Step-Time: 0.962 secs\n",
      "Loss at 2100th Epoch -> Train Set: 2.6902 | Valid Set: 2.7121 | Avg Step-Time: 0.959 secs\n",
      "Loss at 2200th Epoch -> Train Set: 2.6778 | Valid Set: 2.6886 | Avg Step-Time: 0.958 secs\n",
      "Loss at 2300th Epoch -> Train Set: 2.6563 | Valid Set: 2.6721 | Avg Step-Time: 0.963 secs\n",
      "Loss at 2400th Epoch -> Train Set: 2.6350 | Valid Set: 2.6547 | Avg Step-Time: 0.962 secs\n",
      "Loss at 2500th Epoch -> Train Set: 2.6219 | Valid Set: 2.6369 | Avg Step-Time: 0.958 secs\n",
      "Loss at 2600th Epoch -> Train Set: 2.6077 | Valid Set: 2.6264 | Avg Step-Time: 0.964 secs\n",
      "Loss at 2700th Epoch -> Train Set: 2.5881 | Valid Set: 2.6071 | Avg Step-Time: 0.961 secs\n",
      "Loss at 2800th Epoch -> Train Set: 2.5804 | Valid Set: 2.5918 | Avg Step-Time: 0.961 secs\n",
      "Loss at 2900th Epoch -> Train Set: 2.5670 | Valid Set: 2.5868 | Avg Step-Time: 0.958 secs\n",
      "Loss at 3000th Epoch -> Train Set: 2.5549 | Valid Set: 2.5776 | Avg Step-Time: 0.957 secs\n",
      "Loss at 3100th Epoch -> Train Set: 2.5486 | Valid Set: 2.5625 | Avg Step-Time: 0.963 secs\n",
      "Loss at 3200th Epoch -> Train Set: 2.5344 | Valid Set: 2.5485 | Avg Step-Time: 0.960 secs\n",
      "Loss at 3300th Epoch -> Train Set: 2.5219 | Valid Set: 2.5384 | Avg Step-Time: 0.958 secs\n",
      "Loss at 3400th Epoch -> Train Set: 2.5107 | Valid Set: 2.5318 | Avg Step-Time: 0.956 secs\n",
      "Loss at 3500th Epoch -> Train Set: 2.5043 | Valid Set: 2.5263 | Avg Step-Time: 0.957 secs\n",
      "Loss at 3600th Epoch -> Train Set: 2.4955 | Valid Set: 2.5089 | Avg Step-Time: 0.957 secs\n",
      "Loss at 3700th Epoch -> Train Set: 2.4869 | Valid Set: 2.5085 | Avg Step-Time: 0.960 secs\n",
      "Loss at 3800th Epoch -> Train Set: 2.4770 | Valid Set: 2.5012 | Avg Step-Time: 0.962 secs\n",
      "Loss at 3900th Epoch -> Train Set: 2.4736 | Valid Set: 2.4915 | Avg Step-Time: 0.958 secs\n",
      "Loss at 4000th Epoch -> Train Set: 2.4655 | Valid Set: 2.4848 | Avg Step-Time: 0.962 secs\n",
      "Loss at 4100th Epoch -> Train Set: 2.4502 | Valid Set: 2.4802 | Avg Step-Time: 0.957 secs\n",
      "Loss at 4200th Epoch -> Train Set: 2.4502 | Valid Set: 2.4734 | Avg Step-Time: 0.958 secs\n",
      "Loss at 4300th Epoch -> Train Set: 2.4327 | Valid Set: 2.4682 | Avg Step-Time: 0.959 secs\n",
      "Loss at 4400th Epoch -> Train Set: 2.4339 | Valid Set: 2.4572 | Avg Step-Time: 0.956 secs\n",
      "Loss at 4500th Epoch -> Train Set: 2.4279 | Valid Set: 2.4497 | Avg Step-Time: 0.957 secs\n",
      "Loss at 4600th Epoch -> Train Set: 2.4198 | Valid Set: 2.4471 | Avg Step-Time: 0.955 secs\n",
      "Loss at 4700th Epoch -> Train Set: 2.4128 | Valid Set: 2.4459 | Avg Step-Time: 0.961 secs\n",
      "Loss at 4800th Epoch -> Train Set: 2.4091 | Valid Set: 2.4342 | Avg Step-Time: 0.959 secs\n",
      "Loss at 4900th Epoch -> Train Set: 2.3990 | Valid Set: 2.4323 | Avg Step-Time: 0.960 secs\n",
      "Loss at 5000th Epoch -> Train Set: 2.3977 | Valid Set: 2.4347 | Avg Step-Time: 0.954 secs\n",
      "Loss at 5100th Epoch -> Train Set: 2.3915 | Valid Set: 2.4208 | Avg Step-Time: 0.957 secs\n",
      "Loss at 5200th Epoch -> Train Set: 2.3876 | Valid Set: 2.4216 | Avg Step-Time: 0.952 secs\n",
      "Loss at 5300th Epoch -> Train Set: 2.3826 | Valid Set: 2.4157 | Avg Step-Time: 0.956 secs\n",
      "Loss at 5400th Epoch -> Train Set: 2.3815 | Valid Set: 2.4071 | Avg Step-Time: 0.961 secs\n",
      "Loss at 5500th Epoch -> Train Set: 2.3694 | Valid Set: 2.4036 | Avg Step-Time: 0.957 secs\n",
      "Loss at 5600th Epoch -> Train Set: 2.3716 | Valid Set: 2.4032 | Avg Step-Time: 0.951 secs\n",
      "Loss at 5700th Epoch -> Train Set: 2.3639 | Valid Set: 2.3967 | Avg Step-Time: 0.957 secs\n",
      "Loss at 5800th Epoch -> Train Set: 2.3545 | Valid Set: 2.3938 | Avg Step-Time: 0.955 secs\n",
      "Loss at 5900th Epoch -> Train Set: 2.3570 | Valid Set: 2.3837 | Avg Step-Time: 0.955 secs\n",
      "Loss at 6000th Epoch -> Train Set: 2.3514 | Valid Set: 2.3838 | Avg Step-Time: 0.955 secs\n",
      "Loss at 6100th Epoch -> Train Set: 2.3450 | Valid Set: 2.3769 | Avg Step-Time: 0.956 secs\n",
      "Loss at 6200th Epoch -> Train Set: 2.3390 | Valid Set: 2.3799 | Avg Step-Time: 0.960 secs\n",
      "Loss at 6300th Epoch -> Train Set: 2.3346 | Valid Set: 2.3718 | Avg Step-Time: 0.952 secs\n",
      "Loss at 6400th Epoch -> Train Set: 2.3362 | Valid Set: 2.3690 | Avg Step-Time: 0.957 secs\n",
      "Loss at 6500th Epoch -> Train Set: 2.3302 | Valid Set: 2.3685 | Avg Step-Time: 0.955 secs\n",
      "Loss at 6600th Epoch -> Train Set: 2.3250 | Valid Set: 2.3680 | Avg Step-Time: 0.961 secs\n",
      "Loss at 6700th Epoch -> Train Set: 2.3255 | Valid Set: 2.3617 | Avg Step-Time: 0.960 secs\n",
      "Loss at 6800th Epoch -> Train Set: 2.3179 | Valid Set: 2.3595 | Avg Step-Time: 0.962 secs\n",
      "Loss at 6900th Epoch -> Train Set: 2.3182 | Valid Set: 2.3576 | Avg Step-Time: 0.962 secs\n",
      "Loss at 7000th Epoch -> Train Set: 2.3111 | Valid Set: 2.3476 | Avg Step-Time: 0.962 secs\n",
      "Loss at 7100th Epoch -> Train Set: 2.3076 | Valid Set: 2.3465 | Avg Step-Time: 0.963 secs\n",
      "Loss at 7200th Epoch -> Train Set: 2.3096 | Valid Set: 2.3478 | Avg Step-Time: 0.960 secs\n",
      "Loss at 7300th Epoch -> Train Set: 2.3021 | Valid Set: 2.3405 | Avg Step-Time: 0.959 secs\n",
      "Loss at 7400th Epoch -> Train Set: 2.3017 | Valid Set: 2.3383 | Avg Step-Time: 0.959 secs\n",
      "Loss at 7500th Epoch -> Train Set: 2.2948 | Valid Set: 2.3373 | Avg Step-Time: 0.963 secs\n",
      "Loss at 7600th Epoch -> Train Set: 2.2920 | Valid Set: 2.3345 | Avg Step-Time: 0.959 secs\n",
      "Loss at 7700th Epoch -> Train Set: 2.2863 | Valid Set: 2.3293 | Avg Step-Time: 0.961 secs\n",
      "Loss at 7800th Epoch -> Train Set: 2.2860 | Valid Set: 2.3307 | Avg Step-Time: 0.962 secs\n",
      "Loss at 7900th Epoch -> Train Set: 2.2822 | Valid Set: 2.3265 | Avg Step-Time: 0.958 secs\n",
      "Loss at 8000th Epoch -> Train Set: 2.2754 | Valid Set: 2.3224 | Avg Step-Time: 0.962 secs\n",
      "Loss at 8100th Epoch -> Train Set: 2.2764 | Valid Set: 2.3211 | Avg Step-Time: 0.957 secs\n",
      "Loss at 8200th Epoch -> Train Set: 2.2770 | Valid Set: 2.3169 | Avg Step-Time: 0.963 secs\n",
      "Loss at 8300th Epoch -> Train Set: 2.2753 | Valid Set: 2.3175 | Avg Step-Time: 0.958 secs\n",
      "Loss at 8400th Epoch -> Train Set: 2.2711 | Valid Set: 2.3130 | Avg Step-Time: 0.956 secs\n",
      "Loss at 8500th Epoch -> Train Set: 2.2616 | Valid Set: 2.3114 | Avg Step-Time: 0.957 secs\n",
      "Loss at 8600th Epoch -> Train Set: 2.2626 | Valid Set: 2.3075 | Avg Step-Time: 0.960 secs\n",
      "Loss at 8700th Epoch -> Train Set: 2.2606 | Valid Set: 2.3046 | Avg Step-Time: 0.955 secs\n",
      "Loss at 8800th Epoch -> Train Set: 2.2552 | Valid Set: 2.3064 | Avg Step-Time: 0.958 secs\n",
      "Loss at 8900th Epoch -> Train Set: 2.2555 | Valid Set: 2.2995 | Avg Step-Time: 0.951 secs\n",
      "Loss at 9000th Epoch -> Train Set: 2.2500 | Valid Set: 2.2990 | Avg Step-Time: 0.953 secs\n",
      "Loss at 9100th Epoch -> Train Set: 2.2501 | Valid Set: 2.3012 | Avg Step-Time: 0.955 secs\n",
      "Loss at 9200th Epoch -> Train Set: 2.2470 | Valid Set: 2.2967 | Avg Step-Time: 0.952 secs\n",
      "Loss at 9300th Epoch -> Train Set: 2.2423 | Valid Set: 2.2941 | Avg Step-Time: 0.954 secs\n",
      "Loss at 9400th Epoch -> Train Set: 2.2393 | Valid Set: 2.2927 | Avg Step-Time: 0.957 secs\n",
      "Loss at 9500th Epoch -> Train Set: 2.2411 | Valid Set: 2.2932 | Avg Step-Time: 0.955 secs\n",
      "Loss at 9600th Epoch -> Train Set: 2.2372 | Valid Set: 2.2930 | Avg Step-Time: 0.957 secs\n",
      "Loss at 9700th Epoch -> Train Set: 2.2335 | Valid Set: 2.2894 | Avg Step-Time: 0.957 secs\n",
      "Loss at 9800th Epoch -> Train Set: 2.2315 | Valid Set: 2.2833 | Avg Step-Time: 0.954 secs\n",
      "Loss at 9900th Epoch -> Train Set: 2.2287 | Valid Set: 2.2848 | Avg Step-Time: 0.956 secs\n",
      "Loss at 10000th Epoch -> Train Set: 2.2282 | Valid Set: 2.2828 | Avg Step-Time: 0.952 secs\n",
      "Loss at 10100th Epoch -> Train Set: 2.2290 | Valid Set: 2.2764 | Avg Step-Time: 0.954 secs\n",
      "Loss at 10200th Epoch -> Train Set: 2.2177 | Valid Set: 2.2729 | Avg Step-Time: 0.961 secs\n",
      "Loss at 10300th Epoch -> Train Set: 2.2238 | Valid Set: 2.2753 | Avg Step-Time: 0.956 secs\n",
      "Loss at 10400th Epoch -> Train Set: 2.2181 | Valid Set: 2.2746 | Avg Step-Time: 0.952 secs\n",
      "Loss at 10500th Epoch -> Train Set: 2.2169 | Valid Set: 2.2767 | Avg Step-Time: 0.957 secs\n",
      "Loss at 10600th Epoch -> Train Set: 2.2114 | Valid Set: 2.2710 | Avg Step-Time: 0.956 secs\n",
      "Loss at 10700th Epoch -> Train Set: 2.2115 | Valid Set: 2.2719 | Avg Step-Time: 0.953 secs\n",
      "Loss at 10800th Epoch -> Train Set: 2.2100 | Valid Set: 2.2670 | Avg Step-Time: 0.953 secs\n",
      "Loss at 10900th Epoch -> Train Set: 2.2082 | Valid Set: 2.2654 | Avg Step-Time: 0.956 secs\n",
      "Loss at 11000th Epoch -> Train Set: 2.2056 | Valid Set: 2.2675 | Avg Step-Time: 0.953 secs\n",
      "Loss at 11100th Epoch -> Train Set: 2.2060 | Valid Set: 2.2644 | Avg Step-Time: 0.956 secs\n",
      "Loss at 11200th Epoch -> Train Set: 2.2003 | Valid Set: 2.2639 | Avg Step-Time: 0.953 secs\n",
      "Loss at 11300th Epoch -> Train Set: 2.1990 | Valid Set: 2.2639 | Avg Step-Time: 0.956 secs\n",
      "Loss at 11400th Epoch -> Train Set: 2.2007 | Valid Set: 2.2623 | Avg Step-Time: 0.956 secs\n",
      "Loss at 11500th Epoch -> Train Set: 2.1972 | Valid Set: 2.2591 | Avg Step-Time: 0.959 secs\n",
      "Loss at 11600th Epoch -> Train Set: 2.1978 | Valid Set: 2.2570 | Avg Step-Time: 0.957 secs\n",
      "Loss at 11700th Epoch -> Train Set: 2.1923 | Valid Set: 2.2553 | Avg Step-Time: 0.959 secs\n",
      "Loss at 11800th Epoch -> Train Set: 2.1939 | Valid Set: 2.2556 | Avg Step-Time: 0.956 secs\n",
      "Loss at 11900th Epoch -> Train Set: 2.1914 | Valid Set: 2.2517 | Avg Step-Time: 0.956 secs\n",
      "Loss at 12000th Epoch -> Train Set: 2.1873 | Valid Set: 2.2568 | Avg Step-Time: 0.958 secs\n",
      "Loss at 12100th Epoch -> Train Set: 2.1857 | Valid Set: 2.2522 | Avg Step-Time: 0.954 secs\n",
      "Loss at 12200th Epoch -> Train Set: 2.1857 | Valid Set: 2.2494 | Avg Step-Time: 0.953 secs\n",
      "Loss at 12300th Epoch -> Train Set: 2.1828 | Valid Set: 2.2491 | Avg Step-Time: 0.956 secs\n",
      "Loss at 12400th Epoch -> Train Set: 2.1796 | Valid Set: 2.2470 | Avg Step-Time: 0.955 secs\n",
      "Loss at 12500th Epoch -> Train Set: 2.1801 | Valid Set: 2.2461 | Avg Step-Time: 0.954 secs\n",
      "Loss at 12600th Epoch -> Train Set: 2.1773 | Valid Set: 2.2406 | Avg Step-Time: 0.955 secs\n",
      "Loss at 12700th Epoch -> Train Set: 2.1754 | Valid Set: 2.2474 | Avg Step-Time: 0.955 secs\n",
      "Loss at 12800th Epoch -> Train Set: 2.1751 | Valid Set: 2.2352 | Avg Step-Time: 0.955 secs\n",
      "Loss at 12900th Epoch -> Train Set: 2.1735 | Valid Set: 2.2397 | Avg Step-Time: 0.956 secs\n",
      "Loss at 13000th Epoch -> Train Set: 2.1706 | Valid Set: 2.2466 | Avg Step-Time: 0.952 secs\n",
      "Loss at 13100th Epoch -> Train Set: 2.1713 | Valid Set: 2.2384 | Avg Step-Time: 0.955 secs\n",
      "Loss at 13200th Epoch -> Train Set: 2.1724 | Valid Set: 2.2409 | Avg Step-Time: 0.956 secs\n",
      "Loss at 13300th Epoch -> Train Set: 2.1662 | Valid Set: 2.2417 | Avg Step-Time: 0.952 secs\n",
      "Loss at 13400th Epoch -> Train Set: 2.1712 | Valid Set: 2.2392 | Avg Step-Time: 0.954 secs\n",
      "Loss at 13500th Epoch -> Train Set: 2.1647 | Valid Set: 2.2302 | Avg Step-Time: 0.953 secs\n",
      "Loss at 13600th Epoch -> Train Set: 2.1688 | Valid Set: 2.2374 | Avg Step-Time: 0.961 secs\n",
      "Loss at 13700th Epoch -> Train Set: 2.1643 | Valid Set: 2.2346 | Avg Step-Time: 0.955 secs\n",
      "Loss at 13800th Epoch -> Train Set: 2.1598 | Valid Set: 2.2330 | Avg Step-Time: 0.951 secs\n",
      "Loss at 13900th Epoch -> Train Set: 2.1597 | Valid Set: 2.2279 | Avg Step-Time: 0.955 secs\n",
      "Loss at 14000th Epoch -> Train Set: 2.1585 | Valid Set: 2.2298 | Avg Step-Time: 0.956 secs\n",
      "Loss at 14100th Epoch -> Train Set: 2.1548 | Valid Set: 2.2297 | Avg Step-Time: 0.952 secs\n",
      "Loss at 14200th Epoch -> Train Set: 2.1559 | Valid Set: 2.2310 | Avg Step-Time: 0.957 secs\n",
      "Loss at 14300th Epoch -> Train Set: 2.1561 | Valid Set: 2.2282 | Avg Step-Time: 0.953 secs\n",
      "Loss at 14400th Epoch -> Train Set: 2.1554 | Valid Set: 2.2294 | Avg Step-Time: 0.952 secs\n",
      "Loss at 14500th Epoch -> Train Set: 2.1492 | Valid Set: 2.2238 | Avg Step-Time: 0.957 secs\n",
      "Loss at 14600th Epoch -> Train Set: 2.1529 | Valid Set: 2.2232 | Avg Step-Time: 0.953 secs\n",
      "Loss at 14700th Epoch -> Train Set: 2.1476 | Valid Set: 2.2237 | Avg Step-Time: 0.954 secs\n",
      "Loss at 14800th Epoch -> Train Set: 2.1507 | Valid Set: 2.2201 | Avg Step-Time: 0.954 secs\n",
      "Loss at 14900th Epoch -> Train Set: 2.1444 | Valid Set: 2.2207 | Avg Step-Time: 0.954 secs\n",
      "Loss at 15000th Epoch -> Train Set: 2.1481 | Valid Set: 2.2183 | Avg Step-Time: 0.954 secs\n",
      "Loss at 15100th Epoch -> Train Set: 2.1419 | Valid Set: 2.2176 | Avg Step-Time: 0.959 secs\n",
      "Loss at 15200th Epoch -> Train Set: 2.1464 | Valid Set: 2.2194 | Avg Step-Time: 0.953 secs\n",
      "Loss at 15300th Epoch -> Train Set: 2.1429 | Valid Set: 2.2137 | Avg Step-Time: 0.953 secs\n",
      "Loss at 15400th Epoch -> Train Set: 2.1401 | Valid Set: 2.2214 | Avg Step-Time: 0.952 secs\n",
      "Loss at 15500th Epoch -> Train Set: 2.1426 | Valid Set: 2.2121 | Avg Step-Time: 0.955 secs\n",
      "Loss at 15600th Epoch -> Train Set: 2.1416 | Valid Set: 2.2149 | Avg Step-Time: 0.954 secs\n",
      "Loss at 15700th Epoch -> Train Set: 2.1415 | Valid Set: 2.2160 | Avg Step-Time: 0.955 secs\n",
      "Loss at 15800th Epoch -> Train Set: 2.1345 | Valid Set: 2.2157 | Avg Step-Time: 0.957 secs\n",
      "Loss at 15900th Epoch -> Train Set: 2.1384 | Valid Set: 2.2182 | Avg Step-Time: 0.956 secs\n",
      "Loss at 16000th Epoch -> Train Set: 2.1326 | Valid Set: 2.2125 | Avg Step-Time: 0.955 secs\n",
      "Loss at 16100th Epoch -> Train Set: 2.1354 | Valid Set: 2.2099 | Avg Step-Time: 0.956 secs\n",
      "Loss at 16200th Epoch -> Train Set: 2.1317 | Valid Set: 2.2118 | Avg Step-Time: 0.954 secs\n",
      "Loss at 16300th Epoch -> Train Set: 2.1286 | Valid Set: 2.2122 | Avg Step-Time: 0.954 secs\n",
      "Loss at 16400th Epoch -> Train Set: 2.1263 | Valid Set: 2.2140 | Avg Step-Time: 0.956 secs\n",
      "Loss at 16500th Epoch -> Train Set: 2.1310 | Valid Set: 2.2082 | Avg Step-Time: 0.956 secs\n",
      "Loss at 16600th Epoch -> Train Set: 2.1254 | Valid Set: 2.2050 | Avg Step-Time: 0.954 secs\n",
      "Loss at 16700th Epoch -> Train Set: 2.1244 | Valid Set: 2.2074 | Avg Step-Time: 0.956 secs\n",
      "Loss at 16800th Epoch -> Train Set: 2.1257 | Valid Set: 2.2017 | Avg Step-Time: 0.955 secs\n",
      "Loss at 16900th Epoch -> Train Set: 2.1213 | Valid Set: 2.2105 | Avg Step-Time: 0.954 secs\n",
      "Loss at 17000th Epoch -> Train Set: 2.1187 | Valid Set: 2.2046 | Avg Step-Time: 0.959 secs\n",
      "Loss at 17100th Epoch -> Train Set: 2.1217 | Valid Set: 2.2095 | Avg Step-Time: 0.956 secs\n",
      "Loss at 17200th Epoch -> Train Set: 2.1176 | Valid Set: 2.2041 | Avg Step-Time: 0.956 secs\n",
      "Loss at 17300th Epoch -> Train Set: 2.1235 | Valid Set: 2.2017 | Avg Step-Time: 0.953 secs\n",
      "Loss at 17400th Epoch -> Train Set: 2.1206 | Valid Set: 2.2051 | Avg Step-Time: 0.952 secs\n",
      "Loss at 17500th Epoch -> Train Set: 2.1188 | Valid Set: 2.2005 | Avg Step-Time: 0.953 secs\n",
      "Loss at 17600th Epoch -> Train Set: 2.1150 | Valid Set: 2.2064 | Avg Step-Time: 0.953 secs\n",
      "Loss at 17700th Epoch -> Train Set: 2.1150 | Valid Set: 2.1995 | Avg Step-Time: 0.951 secs\n",
      "Loss at 17800th Epoch -> Train Set: 2.1124 | Valid Set: 2.1980 | Avg Step-Time: 0.952 secs\n",
      "Loss at 17900th Epoch -> Train Set: 2.1153 | Valid Set: 2.1990 | Avg Step-Time: 0.954 secs\n",
      "Loss at 18000th Epoch -> Train Set: 2.1113 | Valid Set: 2.1992 | Avg Step-Time: 0.953 secs\n",
      "Loss at 18100th Epoch -> Train Set: 2.1106 | Valid Set: 2.2015 | Avg Step-Time: 0.951 secs\n",
      "Loss at 18200th Epoch -> Train Set: 2.1117 | Valid Set: 2.1993 | Avg Step-Time: 0.954 secs\n",
      "Loss at 18300th Epoch -> Train Set: 2.1117 | Valid Set: 2.1973 | Avg Step-Time: 0.953 secs\n",
      "Loss at 18400th Epoch -> Train Set: 2.1159 | Valid Set: 2.1936 | Avg Step-Time: 0.956 secs\n",
      "Loss at 18500th Epoch -> Train Set: 2.1075 | Valid Set: 2.1946 | Avg Step-Time: 0.955 secs\n",
      "Loss at 18600th Epoch -> Train Set: 2.1102 | Valid Set: 2.1961 | Avg Step-Time: 0.953 secs\n",
      "Loss at 18700th Epoch -> Train Set: 2.1055 | Valid Set: 2.1967 | Avg Step-Time: 0.958 secs\n",
      "Loss at 18800th Epoch -> Train Set: 2.1047 | Valid Set: 2.1946 | Avg Step-Time: 0.956 secs\n",
      "Loss at 18900th Epoch -> Train Set: 2.1038 | Valid Set: 2.1948 | Avg Step-Time: 0.956 secs\n",
      "Loss at 19000th Epoch -> Train Set: 2.1055 | Valid Set: 2.1969 | Avg Step-Time: 0.950 secs\n",
      "Loss at 19100th Epoch -> Train Set: 2.1047 | Valid Set: 2.1909 | Avg Step-Time: 0.949 secs\n",
      "Loss at 19200th Epoch -> Train Set: 2.1040 | Valid Set: 2.1921 | Avg Step-Time: 0.952 secs\n",
      "Loss at 19300th Epoch -> Train Set: 2.0978 | Valid Set: 2.1915 | Avg Step-Time: 0.949 secs\n",
      "Loss at 19400th Epoch -> Train Set: 2.0997 | Valid Set: 2.1913 | Avg Step-Time: 0.954 secs\n",
      "Loss at 19500th Epoch -> Train Set: 2.0979 | Valid Set: 2.1911 | Avg Step-Time: 0.949 secs\n",
      "Loss at 19600th Epoch -> Train Set: 2.0975 | Valid Set: 2.1894 | Avg Step-Time: 0.957 secs\n",
      "Loss at 19700th Epoch -> Train Set: 2.0986 | Valid Set: 2.1895 | Avg Step-Time: 0.955 secs\n",
      "Loss at 19800th Epoch -> Train Set: 2.0985 | Valid Set: 2.1870 | Avg Step-Time: 0.953 secs\n",
      "Loss at 19900th Epoch -> Train Set: 2.0971 | Valid Set: 2.1949 | Avg Step-Time: 0.953 secs\n",
      "Loss at 20000th Epoch -> Train Set: 2.0962 | Valid Set: 2.1897 | Avg Step-Time: 0.953 secs\n",
      "\n",
      "-----------\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Novel Text on seed:\n",
      "\n",
      "\tit slowed downloads not on but it defined to the \"Tables\".  br /  br / People in \"Dick\" in the attitude of this movie (that could be made for non-Septe this movie on Simpsons, pretty even half during the film, yet wannabes it on or \"the Fury it's too long as well cut the stylish Dickens of Sline.\" br /  br / I didn't know how much has magazine centering this film either \"sexasonal buses\" for its enjoyment instead of \"Saturday night and return\", that really no associate home. br /  br / There's also a serious humor in this sloppy town in every possible equation and whose real weesetness is that producers so convincingly though should try and accept that that humor of cinema. This also comes up with the them all that is. But with many of the local video movies, that all of the sequence regarding a viewer to brash, and reveal a degree in my mind blame in a movie. You have been dusted into the Kong movie announced with everything. br /  br / Seeing similar to homeless films I have seen man\n",
      "\n",
      "-----------\n",
      "\n",
      "Generating based on seed: This movie had amazing action sequences\n",
      "\n",
      "This movie had amazing action sequences can be flashed out there. The story is totally ignorant in the rest of the movie, and there is a ragtage of unexpectedly introducing people so well. There are things that make the movie come in and if the movie deals with characters life in Italy and hints in the final figure. The only music scene in this movie when somewhat big appeal that had to learn its look and writing it's score. Fighting with the post-waddly execution of \"The Bathroom Shinin's Livings\", which apparently refers to its action and general, which is supposed to be acceptante if one it's day of idiotentian disregard. br /  br / Amazing, bitter marketedly, but the guy is just about any determination of prower lover about is ultimately important for his wife as the way of love attack. Lovepoven and the girl have no measure about any of which. But not as much as I'm sure) either. That was a very different part as the women, or masochistic heart humor may be punched out. Many people have defined the girls laws in the ti\n",
      "\n",
      "-----------\n",
      "\n",
      "Generating based on seed: The screenplay and direction of this movie was its saviour given the dismal\n",
      "\n",
      "The screenplay and direction of this movie was its saviour given the dismal, it was the terrible ideal every given so offencive, that does have much to believe a raw comedies at the 1984 CSA.\n",
      "Making your attention from start to finish \"Stephen Adventure\", the damn entertainment description of 'Blade Runner\", \"Repulsion!\" this may have been one of those films that fail to capture this moment.  br /  br / The damned problem here might unravel the nace story are good in which they have so much called it. The film's Elementary reaction of filth. 8) is better than not only a fantastic female. br /  br / While Tarzan is one of the greatest city ever. She really is a natural. It does have some great scenes (more for the admissation of life), but this wonders all brings to a purple cartoon (another is the dull, silly, exploitative work for that Evel would have), a prodigious Evans film. Are so obvious, a great family moving up of chaos making the \"those super-explain killings\" that are deeply different after a panache but we only get those native biggest effects alon\n",
      "\n",
      "-----------\n",
      "\n",
      "Generating based on seed: I absolutely enjoyed this movie it was\n",
      "\n",
      "I absolutely enjoyed this movie it was a genre I truly adored my advice with watching it if one instead I just had to walk away with giving it. I could honestly tell you. I appreciate poor Footail it appeared in an entire film, and thus also instantly beats his deadly brain at by performing his jestem. Paul Mc Kennedy's never went to a 'few' this isn't a show anymore but it's still subtly keeps going green either; Sadly Aunts in a beat it's beating his teeth shoot him so little that he wants to deal with each hiss. br /  br / My recommended favorite film was that the mess being made about \"small toon\" Sprina was incredible, and while Reeni is obviously young and sinister with the ships of robot will nod get her sour. Trust me no hum. br /  br / Now which is so unabsorbed. br /  br / Next I get all those support and a story worse than REWINDS of character, it could be appealing non adventures of Dead Hope movie before being asked of such a wonderful director. Blink Flood Surf has been poured off some exception and more unmi\n",
      "\n",
      "-----------\n",
      "\n",
      "Model Architecture:\n",
      "\n",
      " MyGPT(\n",
      "  (token_embed): Embedding(77, 384)\n",
      "  (position_embed): Embedding(256, 384)\n",
      "  (decoder_blocks): Sequential(\n",
      "    (0): GPTDecoderBlock(\n",
      "      (multihead_attention): MultiHeadAttention(\n",
      "        (multiple_attention_heads): ModuleList(\n",
      "          (0-7): 8 x SelfAttentionHead(\n",
      "            (queries): Linear(in_features=384, out_features=48, bias=False)\n",
      "            (keys): Linear(in_features=384, out_features=48, bias=False)\n",
      "            (values): Linear(in_features=384, out_features=48, bias=False)\n",
      "            (dropout): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (dropout): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "      (ffwd): FeedForward(\n",
      "        (sequential): Sequential(\n",
      "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "          (3): Dropout(p=0.2, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (1): GPTDecoderBlock(\n",
      "      (multihead_attention): MultiHeadAttention(\n",
      "        (multiple_attention_heads): ModuleList(\n",
      "          (0-7): 8 x SelfAttentionHead(\n",
      "            (queries): Linear(in_features=384, out_features=48, bias=False)\n",
      "            (keys): Linear(in_features=384, out_features=48, bias=False)\n",
      "            (values): Linear(in_features=384, out_features=48, bias=False)\n",
      "            (dropout): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (dropout): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "      (ffwd): FeedForward(\n",
      "        (sequential): Sequential(\n",
      "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "          (3): Dropout(p=0.2, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (2): GPTDecoderBlock(\n",
      "      (multihead_attention): MultiHeadAttention(\n",
      "        (multiple_attention_heads): ModuleList(\n",
      "          (0-7): 8 x SelfAttentionHead(\n",
      "            (queries): Linear(in_features=384, out_features=48, bias=False)\n",
      "            (keys): Linear(in_features=384, out_features=48, bias=False)\n",
      "            (values): Linear(in_features=384, out_features=48, bias=False)\n",
      "            (dropout): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (dropout): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "      (ffwd): FeedForward(\n",
      "        (sequential): Sequential(\n",
      "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "          (3): Dropout(p=0.2, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (3): GPTDecoderBlock(\n",
      "      (multihead_attention): MultiHeadAttention(\n",
      "        (multiple_attention_heads): ModuleList(\n",
      "          (0-7): 8 x SelfAttentionHead(\n",
      "            (queries): Linear(in_features=384, out_features=48, bias=False)\n",
      "            (keys): Linear(in_features=384, out_features=48, bias=False)\n",
      "            (values): Linear(in_features=384, out_features=48, bias=False)\n",
      "            (dropout): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (dropout): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "      (ffwd): FeedForward(\n",
      "        (sequential): Sequential(\n",
      "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "          (3): Dropout(p=0.2, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (4): GPTDecoderBlock(\n",
      "      (multihead_attention): MultiHeadAttention(\n",
      "        (multiple_attention_heads): ModuleList(\n",
      "          (0-7): 8 x SelfAttentionHead(\n",
      "            (queries): Linear(in_features=384, out_features=48, bias=False)\n",
      "            (keys): Linear(in_features=384, out_features=48, bias=False)\n",
      "            (values): Linear(in_features=384, out_features=48, bias=False)\n",
      "            (dropout): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (dropout): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "      (ffwd): FeedForward(\n",
      "        (sequential): Sequential(\n",
      "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "          (3): Dropout(p=0.2, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (5): GPTDecoderBlock(\n",
      "      (multihead_attention): MultiHeadAttention(\n",
      "        (multiple_attention_heads): ModuleList(\n",
      "          (0-7): 8 x SelfAttentionHead(\n",
      "            (queries): Linear(in_features=384, out_features=48, bias=False)\n",
      "            (keys): Linear(in_features=384, out_features=48, bias=False)\n",
      "            (values): Linear(in_features=384, out_features=48, bias=False)\n",
      "            (dropout): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (dropout): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "      (ffwd): FeedForward(\n",
      "        (sequential): Sequential(\n",
      "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "          (3): Dropout(p=0.2, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (6): GPTDecoderBlock(\n",
      "      (multihead_attention): MultiHeadAttention(\n",
      "        (multiple_attention_heads): ModuleList(\n",
      "          (0-7): 8 x SelfAttentionHead(\n",
      "            (queries): Linear(in_features=384, out_features=48, bias=False)\n",
      "            (keys): Linear(in_features=384, out_features=48, bias=False)\n",
      "            (values): Linear(in_features=384, out_features=48, bias=False)\n",
      "            (dropout): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (dropout): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "      (ffwd): FeedForward(\n",
      "        (sequential): Sequential(\n",
      "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "          (3): Dropout(p=0.2, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (7): GPTDecoderBlock(\n",
      "      (multihead_attention): MultiHeadAttention(\n",
      "        (multiple_attention_heads): ModuleList(\n",
      "          (0-7): 8 x SelfAttentionHead(\n",
      "            (queries): Linear(in_features=384, out_features=48, bias=False)\n",
      "            (keys): Linear(in_features=384, out_features=48, bias=False)\n",
      "            (values): Linear(in_features=384, out_features=48, bias=False)\n",
      "            (dropout): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (dropout): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "      (ffwd): FeedForward(\n",
      "        (sequential): Sequential(\n",
      "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "          (3): Dropout(p=0.2, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (8): GPTDecoderBlock(\n",
      "      (multihead_attention): MultiHeadAttention(\n",
      "        (multiple_attention_heads): ModuleList(\n",
      "          (0-7): 8 x SelfAttentionHead(\n",
      "            (queries): Linear(in_features=384, out_features=48, bias=False)\n",
      "            (keys): Linear(in_features=384, out_features=48, bias=False)\n",
      "            (values): Linear(in_features=384, out_features=48, bias=False)\n",
      "            (dropout): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (dropout): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "      (ffwd): FeedForward(\n",
      "        (sequential): Sequential(\n",
      "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "          (3): Dropout(p=0.2, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (9): GPTDecoderBlock(\n",
      "      (multihead_attention): MultiHeadAttention(\n",
      "        (multiple_attention_heads): ModuleList(\n",
      "          (0-7): 8 x SelfAttentionHead(\n",
      "            (queries): Linear(in_features=384, out_features=48, bias=False)\n",
      "            (keys): Linear(in_features=384, out_features=48, bias=False)\n",
      "            (values): Linear(in_features=384, out_features=48, bias=False)\n",
      "            (dropout): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (dropout): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "      (ffwd): FeedForward(\n",
      "        (sequential): Sequential(\n",
      "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "          (3): Dropout(p=0.2, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (10): GPTDecoderBlock(\n",
      "      (multihead_attention): MultiHeadAttention(\n",
      "        (multiple_attention_heads): ModuleList(\n",
      "          (0-7): 8 x SelfAttentionHead(\n",
      "            (queries): Linear(in_features=384, out_features=48, bias=False)\n",
      "            (keys): Linear(in_features=384, out_features=48, bias=False)\n",
      "            (values): Linear(in_features=384, out_features=48, bias=False)\n",
      "            (dropout): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (dropout): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "      (ffwd): FeedForward(\n",
      "        (sequential): Sequential(\n",
      "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "          (3): Dropout(p=0.2, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (11): GPTDecoderBlock(\n",
      "      (multihead_attention): MultiHeadAttention(\n",
      "        (multiple_attention_heads): ModuleList(\n",
      "          (0-7): 8 x SelfAttentionHead(\n",
      "            (queries): Linear(in_features=384, out_features=48, bias=False)\n",
      "            (keys): Linear(in_features=384, out_features=48, bias=False)\n",
      "            (values): Linear(in_features=384, out_features=48, bias=False)\n",
      "            (dropout): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (dropout): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "      (ffwd): FeedForward(\n",
      "        (sequential): Sequential(\n",
      "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "          (3): Dropout(p=0.2, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (final_ln): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "  (last_linear): Linear(in_features=384, out_features=77, bias=True)\n",
      ")\n",
      "\n",
      "-----------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The Main Function\n",
    "def main():\n",
    "\n",
    "    # Downloading and Loading the Dataset\n",
    "    imdb_review_dataset = IMDBMovieReview()\n",
    "\n",
    "    # Filtering the columns of the Dataset\n",
    "    review_string = imdb_review_dataset.refine_structure()\n",
    "\n",
    "    # Loading the GPT Model Class\n",
    "    gpt_model = MyGPT(imdb_review_dataset.vocab, n_embd=384, block_size=256, decoder_layers=12, attention_heads=8)\n",
    "    gpt_model = torch.nn.DataParallel(gpt_model)\n",
    "    gpt_model.to(device=acc_device)\n",
    "\n",
    "    # Creating the data tensor\n",
    "    data_preprocessor = DataPreprocessor(review_string, device=acc_device)\n",
    "    data_preprocessor.create_data_tensor(gpt_model)\n",
    "    \n",
    "    # Splitting the data tensor\n",
    "    train_set, valid_set, _ = data_preprocessor.train_valid_test()\n",
    "\n",
    "    # Creating the optimization loop\n",
    "    optim_handle = OptimizationLoop(data_preprocessor, gpt_model, learning_rate=5e-4)\n",
    "\n",
    "    # Datastring Metrics\n",
    "    print(\"Datastring Metrics after loading the dataset on Vocab:\")\n",
    "    imdb_review_dataset.datastring_metrics()\n",
    "    print(\"\\n-----------\\n\")\n",
    "\n",
    "    # Vocabulary used by the model\n",
    "    print(\"Model Vocabulary from the Character-Level Tokenizer:\\n\", imdb_review_dataset.vocab)\n",
    "    print(\"\\n-----------\\n\")\n",
    "    \n",
    "    # Training the model\n",
    "    print(\"Training the Model:\\n\")\n",
    "    train_losses, valid_losses = optim_handle.train(20000, train_set, valid_set, batch_size=128, block_size=256)\n",
    "    print(\"\\n-----------\\n\")\n",
    "\n",
    "    # Plotting the Losses\n",
    "    plt.plot(range(1, len(train_losses) + 1), train_losses, label=\"Training Loss\", c=\"g\", ls=\"-\")\n",
    "    plt.plot(range(1, len(valid_losses) + 1), valid_losses, label=\"Validation Loss\", c=\"b\", ls=\"-.\")\n",
    "    plt.title(\"Learning Curve\")\n",
    "    plt.xlabel(\"Epochs in (100x)\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.show()\n",
    "\n",
    "    # Generating Text with the model\n",
    "    print(\"Generating Novel Text on seed:\\n\")\n",
    "    idx = torch.zeros((1, 1), dtype=torch.long, device=acc_device)\n",
    "    output_tokens = gpt_model.module.generate(previous_tokens=idx, max_tokens=1000)[0].tolist()\n",
    "    output_sentence = gpt_model.module.decode(output_tokens)\n",
    "    print(output_sentence)\n",
    "    print(\"\\n-----------\\n\")\n",
    "\n",
    "    print(\"Generating based on seed: This movie had amazing action sequences\\n\")\n",
    "    action_prompt = torch.tensor(gpt_model.module.encode(\"This movie had amazing action sequences\"), device=acc_device)\n",
    "    action_prompt = torch.unsqueeze(action_prompt, 0)\n",
    "    output_tokens = gpt_model.module.generate(previous_tokens=action_prompt, max_tokens=1000)[0].tolist()\n",
    "    output_sentence = gpt_model.module.decode(output_tokens)\n",
    "    print(output_sentence)\n",
    "    print(\"\\n-----------\\n\")\n",
    "\n",
    "    print(\"Generating based on seed: The screenplay and direction of this movie was its saviour given the dismal\\n\")\n",
    "    drama_prompt = torch.tensor(gpt_model.module.encode(\"The screenplay and direction of this movie was its saviour given the dismal\"), device=acc_device)\n",
    "    drama_prompt = torch.unsqueeze(drama_prompt, 0)\n",
    "    output_tokens = gpt_model.module.generate(previous_tokens=drama_prompt, max_tokens=1000)[0].tolist()\n",
    "    output_sentence = gpt_model.module.decode(output_tokens)\n",
    "    print(output_sentence)\n",
    "    print(\"\\n-----------\\n\")\n",
    "\n",
    "    print(\"Generating based on seed: I absolutely enjoyed this movie it was\\n\")\n",
    "    excited_prompt = torch.tensor(gpt_model.module.encode(\"I absolutely enjoyed this movie it was\"), device=acc_device)\n",
    "    excited_prompt = torch.unsqueeze(excited_prompt, 0)\n",
    "    output_tokens = gpt_model.module.generate(previous_tokens=excited_prompt, max_tokens=1000)[0].tolist()\n",
    "    output_sentence = gpt_model.module.decode(output_tokens)\n",
    "    print(output_sentence)\n",
    "    print(\"\\n-----------\\n\")\n",
    "\n",
    "    # Storing the weights of the model\n",
    "    gpt_model.module.save_model()\n",
    "\n",
    "    # Model Architecture\n",
    "    print(\"Model Architecture:\\n\\n\", gpt_model.module)\n",
    "    print(\"\\n-----------\\n\")\n",
    "\n",
    "    del gpt_model\n",
    "\n",
    "\n",
    "# Driver code\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adce905d",
   "metadata": {
    "papermill": {
     "duration": 0.010797,
     "end_time": "2025-09-01T06:37:51.680908",
     "exception": false,
     "start_time": "2025-09-01T06:37:51.670111",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 447516,
     "sourceId": 849658,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 33987.374269,
   "end_time": "2025-09-01T06:37:53.896405",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-08-31T21:11:26.522136",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
