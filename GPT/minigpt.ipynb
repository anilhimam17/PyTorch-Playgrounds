{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46f4c1cf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T10:52:14.077782Z",
     "iopub.status.busy": "2025-08-31T10:52:14.077494Z",
     "iopub.status.idle": "2025-08-31T10:52:18.880581Z",
     "shell.execute_reply": "2025-08-31T10:52:18.879949Z"
    },
    "papermill": {
     "duration": 4.808449,
     "end_time": "2025-08-31T10:52:18.881978",
     "exception": false,
     "start_time": "2025-08-31T10:52:14.073529",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "import time\n",
    "from typing import Generator\n",
    "from pathlib import Path\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29e5aa65",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T10:52:18.888563Z",
     "iopub.status.busy": "2025-08-31T10:52:18.888219Z",
     "iopub.status.idle": "2025-08-31T10:52:18.895051Z",
     "shell.execute_reply": "2025-08-31T10:52:18.894312Z"
    },
    "papermill": {
     "duration": 0.011115,
     "end_time": "2025-08-31T10:52:18.896184",
     "exception": false,
     "start_time": "2025-08-31T10:52:18.885069",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "DATASET_PATH = Path(\"/kaggle/input/imdb-50k-movie-reviews-test-your-bert\")\n",
    "\n",
    "\n",
    "class IMDBMovieReview:\n",
    "    \"\"\"Class implements all the dataset loading, handling and metrics.\"\"\"\n",
    "    def __init__(self) -> None:\n",
    "        \n",
    "        self.dataframe = pd.read_csv(\n",
    "            filepath_or_buffer=DATASET_PATH / \"train.csv\",\n",
    "            encoding=\"utf-8\"\n",
    "        )\n",
    "\n",
    "    def __iter__(self) -> Generator[str, str, None]:\n",
    "        for i in range(10):\n",
    "            yield self.dataframe[\"text\"].loc[i]\n",
    "\n",
    "    def refine_structure(self) -> str:\n",
    "        \"\"\"Focuses the entire dataframe to only the text removing other cols.\"\"\"\n",
    "        if \"sentiment\" in self.dataframe.columns:\n",
    "            self.dataframe = self.dataframe.drop([\"sentiment\"], axis=1)\n",
    "        self.dataset_string = \"\\n\".join(self.dataframe[\"text\"])\n",
    "        \n",
    "        # Cleaning up the string\n",
    "        self.dataset_string = re.sub(r'[^\\x00-\\x7F]+', \" \", self.dataset_string)\n",
    "        self.dataset_string = re.sub(r'[\\U00010000-\\U0010ffff]+', \" \", self.dataset_string)\n",
    "        self.dataset_string = re.sub(r'[\\x08\\x10#\\$%&\\*\\+<=>@\\[\\\\\\]\\^_`\\{\\|\\}~]', \" \", self.dataset_string)\n",
    "        \n",
    "        # Generating the vocabulary for the dataset string.\n",
    "        self.vocab = sorted(list(set(self.dataset_string)))\n",
    "\n",
    "        return self.dataset_string\n",
    "    \n",
    "    def datastring_metrics(self) -> None:\n",
    "        \"\"\"Provides metrics for the dataset string.\"\"\"\n",
    "\n",
    "        print(f\"Length of the Dataset: {len(self.dataset_string)}\\n\")\n",
    "        print(f\"First 1000 Chars:\\n{self.dataset_string[:1000]}\\n\")\n",
    "        print(f\"Vocabulary Size: {len(self.vocab)}\\n\")\n",
    "        print(f\"Vocabulary:\\n{''.join(self.vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ceb318b0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T10:52:18.901875Z",
     "iopub.status.busy": "2025-08-31T10:52:18.901666Z",
     "iopub.status.idle": "2025-08-31T10:52:18.907772Z",
     "shell.execute_reply": "2025-08-31T10:52:18.907121Z"
    },
    "papermill": {
     "duration": 0.010087,
     "end_time": "2025-08-31T10:52:18.908775",
     "exception": false,
     "start_time": "2025-08-31T10:52:18.898688",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SelfAttentionHead(torch.nn.Module):\n",
    "    \"\"\"Class implements a single self-attention head.\"\"\"\n",
    "    def __init__(self, head_size: int, input_features: int, block_size: int, dropout_rate: float = 0.2):\n",
    "        super().__init__()\n",
    "\n",
    "        # Attention Matrices\n",
    "        self.queries = torch.nn.Linear(input_features, head_size, bias=False)\n",
    "        self.keys = torch.nn.Linear(input_features, head_size, bias=False)\n",
    "        self.values = torch.nn.Linear(input_features, head_size, bias=False)\n",
    "        self.register_buffer(\"tril\", torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        # Dropout\n",
    "        self.dropout = torch.nn.Dropout(p=dropout_rate)\n",
    "\n",
    "    def forward(self, X: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Implements the forward propagation of the Self Attention Head.\"\"\"\n",
    "        B, T, C = X.shape\n",
    "\n",
    "        # Calculating the Queries, Keys and Values as Linear Projections\n",
    "        queries: torch.Tensor = self.queries(X)\n",
    "        keys: torch.Tensor = self.keys(X)\n",
    "        values: torch.Tensor = self.values(X)\n",
    "\n",
    "        # Calculating the Dot Product of the Queries and Keys for the Attention Pattern\n",
    "        attention_pattern: torch.Tensor = queries @ keys.transpose(-2, -1) * C ** -0.5\n",
    "        attention_pattern = attention_pattern.masked_fill(self.tril[:T, :T] == 0, -torch.inf)  # type: ignore\n",
    "        attention_pattern = torch.nn.functional.softmax(attention_pattern, dim=-1)\n",
    "\n",
    "        # Regularization of the Attention Patterns\n",
    "        reg_attention_pattern = self.dropout(attention_pattern)\n",
    "\n",
    "        # Weighted Sum\n",
    "        output_attended_embeddings = reg_attention_pattern @ values\n",
    "        return output_attended_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3070b68d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T10:52:18.914167Z",
     "iopub.status.busy": "2025-08-31T10:52:18.913977Z",
     "iopub.status.idle": "2025-08-31T10:52:18.919422Z",
     "shell.execute_reply": "2025-08-31T10:52:18.918752Z"
    },
    "papermill": {
     "duration": 0.00935,
     "end_time": "2025-08-31T10:52:18.920459",
     "exception": false,
     "start_time": "2025-08-31T10:52:18.911109",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(torch.nn.Module):\n",
    "    \"\"\"Implements a Multi Headed Attention Module.\n",
    "\n",
    "    Where each attention head is a parallel implementation of a distinct Self Attention Head.\n",
    "    The Multi Headed Attention Module runs several Self Attention Heads in parallel and concatenates \n",
    "    the final output generated by each head on the Channel Dimension.\"\"\"\n",
    "\n",
    "    def __init__(self, num_heads: int, head_size: int, block_size: int, n_embd: int, dropout_rate: float = 0.2) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # Parallel Self Attention Heads\n",
    "        self.multiple_attention_heads = torch.nn.ModuleList(\n",
    "            [\n",
    "                SelfAttentionHead(head_size=head_size, input_features=n_embd, block_size=block_size)\n",
    "                for _ in range(num_heads)\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        # Dropout Layer\n",
    "        self.dropout = torch.nn.Dropout(p=dropout_rate)\n",
    "\n",
    "    def forward(self, X) -> torch.Tensor:\n",
    "        \"\"\"Implements the forward propagation of the MultiHead Attention Layer.\"\"\"\n",
    "        \n",
    "        multihead_attention_pattern = torch.cat(\n",
    "            [head(X) for head in self.multiple_attention_heads], dim=-1\n",
    "        )\n",
    "        out_attention = self.dropout(multihead_attention_pattern)\n",
    "\n",
    "        return out_attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16e48913",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T10:52:18.925717Z",
     "iopub.status.busy": "2025-08-31T10:52:18.925531Z",
     "iopub.status.idle": "2025-08-31T10:52:18.930055Z",
     "shell.execute_reply": "2025-08-31T10:52:18.929366Z"
    },
    "papermill": {
     "duration": 0.008383,
     "end_time": "2025-08-31T10:52:18.931142",
     "exception": false,
     "start_time": "2025-08-31T10:52:18.922759",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FeedForward(torch.nn.Module):\n",
    "    \"\"\"Implements a simple sequential feedforward \n",
    "    network to decide the next token based on attention.\"\"\"\n",
    "\n",
    "    def __init__(self, n_embd: int = 64, dropout_rate: float = 0.2) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.sequential = torch.nn.Sequential(\n",
    "            torch.nn.Linear(in_features=n_embd, out_features=4*n_embd),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(4*n_embd, n_embd),\n",
    "            torch.nn.Dropout(p=dropout_rate)\n",
    "        )\n",
    "\n",
    "    def forward(self, X) -> torch.Tensor:\n",
    "        \"\"\"Implements the forward propagation of the feedforward model.\"\"\"\n",
    "        return self.sequential(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a4af403",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T10:52:18.936648Z",
     "iopub.status.busy": "2025-08-31T10:52:18.936219Z",
     "iopub.status.idle": "2025-08-31T10:52:18.941184Z",
     "shell.execute_reply": "2025-08-31T10:52:18.940533Z"
    },
    "papermill": {
     "duration": 0.008718,
     "end_time": "2025-08-31T10:52:18.942164",
     "exception": false,
     "start_time": "2025-08-31T10:52:18.933446",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class GPTDecoderBlock(torch.nn.Module):\n",
    "    \"\"\"Implements a single GPT decoder block comprising the Attention Mechanism and the FeedForward Network.\"\"\"\n",
    "\n",
    "    def __init__(self, num_heads: int, n_embd: int, block_size: int) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # Multihead Attention Block\n",
    "        self.multihead_attention = MultiHeadAttention(\n",
    "            num_heads=num_heads, \n",
    "            head_size=n_embd//num_heads, \n",
    "            block_size=block_size,\n",
    "            n_embd=n_embd\n",
    "        )\n",
    "        # Feedforward Network Block\n",
    "        self.ffwd = FeedForward(n_embd=n_embd)\n",
    "\n",
    "        # Layer Normalisation Blocks\n",
    "        self.ln1 = torch.nn.LayerNorm(n_embd)\n",
    "        self.ln2 = torch.nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, X: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Implements the forward propagation of a single GPT decoder block.\"\"\"\n",
    "        \n",
    "        attention_out = self.multihead_attention(self.ln1(X))\n",
    "        x = attention_out + X\n",
    "        ffwd_out = self.ffwd(self.ln2(x))\n",
    "        residual_scores = ffwd_out + x\n",
    "        return residual_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e5542f8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T10:52:18.947796Z",
     "iopub.status.busy": "2025-08-31T10:52:18.947591Z",
     "iopub.status.idle": "2025-08-31T10:52:18.957537Z",
     "shell.execute_reply": "2025-08-31T10:52:18.956870Z"
    },
    "papermill": {
     "duration": 0.014206,
     "end_time": "2025-08-31T10:52:18.958753",
     "exception": false,
     "start_time": "2025-08-31T10:52:18.944547",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MyGPT(torch.nn.Module):\n",
    "    \"\"\"Class implments the Generatively Pretrained Transformer from scratch using PyTorch.\"\"\"\n",
    "    def __init__(self, vocab: list[str] = [], decoder_layers: int = 6, n_embd: int = 64, block_size: int = 128, attention_heads: int = 4) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # Hyperparameters\n",
    "        self.vocab_size = len(vocab)\n",
    "        self.block_size = block_size\n",
    "\n",
    "        # Tokenization Maps\n",
    "        self.encode_map = {ch: i for i, ch in enumerate(vocab)}\n",
    "        self.decode_map = {i: ch for i, ch in enumerate(vocab)}\n",
    "\n",
    "        # Embedding Layers\n",
    "        self.token_embed = torch.nn.Embedding(self.vocab_size, n_embd)\n",
    "        self.position_embed = torch.nn.Embedding(self.block_size, n_embd)\n",
    "\n",
    "        # GPT Decoder Blocks\n",
    "        self.decoder_blocks = torch.nn.Sequential(\n",
    "            *[\n",
    "                GPTDecoderBlock(num_heads=attention_heads, n_embd=n_embd, block_size=block_size)\n",
    "                for _ in range(decoder_layers)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Final Normalization Layer\n",
    "        self.final_ln = torch.nn.LayerNorm(n_embd)\n",
    "\n",
    "        # Linear Layers\n",
    "        self.last_linear = torch.nn.Linear(n_embd, self.vocab_size)\n",
    "\n",
    "    def encode(self, input_string: str = \"\") -> list[int]:\n",
    "        \"\"\"Encode operation for the simple character level tokenizer.\"\"\"\n",
    "        return [self.encode_map[ch] for ch in input_string]\n",
    "\n",
    "    def decode(self, input_seq: list[int] = []) -> str:\n",
    "        \"\"\"Invert operation for the simple character level tokenizer.\"\"\"\n",
    "        return \"\".join([self.decode_map[token] for token in input_seq])\n",
    "    \n",
    "    def forward(self, X: torch.Tensor, y: torch.Tensor | None = None) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Implements the forward propagation of the model.\"\"\"\n",
    "\n",
    "        B, T = X.shape\n",
    "\n",
    "        # Initial Scores\n",
    "        embed_score = self.token_embed(X)\n",
    "        pos_score = self.position_embed(torch.arange(T, device=torch.accelerator.current_accelerator()))\n",
    "        x = embed_score + pos_score\n",
    "\n",
    "        # GPT Decoder Blocks\n",
    "        block_scores = self.decoder_blocks(x)\n",
    "\n",
    "        # Final Normalization\n",
    "        block_scores_norm = self.final_ln(block_scores)\n",
    "\n",
    "        # Deeper Layers\n",
    "        logits = self.last_linear(block_scores_norm)\n",
    "\n",
    "        loss = torch.tensor([])\n",
    "        if y is not None:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view((B * T), C)\n",
    "            y = y.view(B * T)\n",
    "            loss = torch.nn.functional.cross_entropy(input=logits, target=y)\n",
    "\n",
    "        return loss, logits\n",
    "    \n",
    "    def generate(self, previous_tokens: torch.Tensor, max_tokens: int) -> torch.Tensor:\n",
    "        \"\"\"Generates novel tokens based on the previous context.\"\"\"\n",
    "\n",
    "        for i in range(max_tokens):\n",
    "            \n",
    "            # Clipping the generations to the last block size tokens\n",
    "            last_block_size = previous_tokens[:, -self.block_size:]\n",
    "\n",
    "            # Generating the next token\n",
    "            _, logits = self(last_block_size)\n",
    "\n",
    "            # Taking the consideration of only the last token\n",
    "            logits = logits[:, -1, :]\n",
    "\n",
    "            # Getting the probabilities of the words\n",
    "            probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, 1)\n",
    "\n",
    "            previous_tokens = torch.cat((previous_tokens, idx_next), dim=1)\n",
    "\n",
    "        return previous_tokens\n",
    "    \n",
    "    def save_model(self) -> None:\n",
    "        \"\"\"Saves the weights of the trained model.\"\"\"\n",
    "\n",
    "        torch.save(self.state_dict(), \"my_gpt_weights.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a23de35d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T10:52:18.964057Z",
     "iopub.status.busy": "2025-08-31T10:52:18.963850Z",
     "iopub.status.idle": "2025-08-31T10:52:18.970701Z",
     "shell.execute_reply": "2025-08-31T10:52:18.970017Z"
    },
    "papermill": {
     "duration": 0.010791,
     "end_time": "2025-08-31T10:52:18.971814",
     "exception": false,
     "start_time": "2025-08-31T10:52:18.961023",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DataPreprocessor:\n",
    "    \"\"\"Class handles all of the data pre-processing necessary for the dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, data_string: str, device: str = \"\") -> None:\n",
    "        self.data_string = data_string\n",
    "        self.device = device if device else None\n",
    "\n",
    "    def create_data_tensor(self, model: MyGPT) -> None:\n",
    "        \"\"\"Create the data tensor on accelerator device.\"\"\"\n",
    "        \n",
    "        token_list = model.module.encode(self.data_string)\n",
    "        self.data_tensor = torch.tensor(\n",
    "            data=token_list,\n",
    "            dtype=torch.long\n",
    "        )\n",
    "        self.tensor_size = self.data_tensor.size()[0]\n",
    "    \n",
    "    def train_valid_test(self, valid_percentage: float = 0.1, test_percentage: float = 0.05) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Peforms the split on the data tensor.\"\"\"\n",
    "\n",
    "        size_valid = int(self.tensor_size * valid_percentage)\n",
    "        size_test = int(self.tensor_size * test_percentage)\n",
    "        size_train = self.tensor_size - (size_valid + size_test)\n",
    "\n",
    "        train_set = self.data_tensor[:size_train].clone().to(self.device)\n",
    "        valid_set = self.data_tensor[size_train : size_train + size_valid].clone().to(self.device)\n",
    "        test_set = self.data_tensor[size_train + size_valid :].clone().to(self.device)\n",
    "\n",
    "        return train_set, valid_set, test_set\n",
    "    \n",
    "    def get_batch(self, set_: torch.Tensor, batch_size: int, block_size: int) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Batches the data_tensors and provides the pointer to the result.\"\"\"\n",
    "\n",
    "        # Creating random batches\n",
    "        ix = torch.randint(0, set_.size()[0] - block_size, (batch_size,))\n",
    "        X = torch.stack([set_[i : i + block_size] for i in ix])\n",
    "        y = torch.stack([set_[i + 1 : i + 1 + block_size] for i in ix])\n",
    "\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c0aebce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T10:52:18.977632Z",
     "iopub.status.busy": "2025-08-31T10:52:18.977110Z",
     "iopub.status.idle": "2025-08-31T10:52:18.984071Z",
     "shell.execute_reply": "2025-08-31T10:52:18.983408Z"
    },
    "papermill": {
     "duration": 0.011028,
     "end_time": "2025-08-31T10:52:18.985191",
     "exception": false,
     "start_time": "2025-08-31T10:52:18.974163",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class OptimizationLoop:\n",
    "    \"\"\"Class implements the train-valid and test loops.\"\"\"\n",
    "    def __init__(self, preprocessor: DataPreprocessor, model: MyGPT, learning_rate: float) -> tuple[list[float], list[float]]:\n",
    "        self.preprocessor = preprocessor\n",
    "        self.model = model\n",
    "        self.optimizer = torch.optim.AdamW(self.model.parameters(), lr=learning_rate)\n",
    "\n",
    "    def train(\n",
    "            self, epochs: int, train_set: torch.Tensor, valid_set: torch.Tensor,\n",
    "            batch_size: int, block_size: int) -> None:\n",
    "        \"\"\"Implements the PyTorch Training Loop for the model.\"\"\"\n",
    "\n",
    "        # Mean Loss Variables\n",
    "        mean_train_loss = 0\n",
    "        mean_valid_loss = 0\n",
    "        mean_time = 0\n",
    "        train_losses = []\n",
    "        valid_losses = []\n",
    "\n",
    "        # Training Loop\n",
    "        for i in range(epochs):\n",
    "            sample_train_X, sample_train_y = self.preprocessor.get_batch(train_set, batch_size, block_size)\n",
    "            sample_valid_X, sample_valid_y = self.preprocessor.get_batch(valid_set, batch_size, block_size)\n",
    "\n",
    "            # Timing the execution\n",
    "            start = time.time()\n",
    "\n",
    "            # Training Step\n",
    "            loss_train, _ = self.model(sample_train_X, sample_train_y)\n",
    "            loss_train = loss_train.sum()\n",
    "            self.optimizer.zero_grad()\n",
    "            loss_train.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            # Stop Time\n",
    "            stop = time.time()\n",
    "\n",
    "            # Validation Step\n",
    "            with torch.no_grad():\n",
    "                loss_valid, _ = self.model(sample_valid_X, sample_valid_y)\n",
    "                loss_valid = loss_valid.sum()\n",
    "\n",
    "            mean_train_loss += loss_train.item()\n",
    "            mean_valid_loss += loss_valid.item()\n",
    "            mean_time += stop - start\n",
    "            if (i + 1) % 100 == 0:\n",
    "                mean_train_loss /= 100\n",
    "                mean_valid_loss /= 100\n",
    "                mean_time /= 100\n",
    "                print(f\"Loss at {i + 1}th Epoch -> Train Set: {mean_train_loss:.4f} | Valid Set: {mean_valid_loss:.4f} | Avg Step-Time: {mean_time:.3f} secs\")\n",
    "                \n",
    "                train_losses.append(mean_train_loss)\n",
    "                valid_losses.append(mean_valid_loss)\n",
    "                mean_train_loss, mean_valid_loss, mean_time = 0, 0, 0\n",
    "\n",
    "        # Returning the Tracking of Losses for Plotting\n",
    "        return train_losses, valid_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "69b3807b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T10:52:18.990676Z",
     "iopub.status.busy": "2025-08-31T10:52:18.990240Z",
     "iopub.status.idle": "2025-08-31T10:52:19.095648Z",
     "shell.execute_reply": "2025-08-31T10:52:19.094825Z"
    },
    "papermill": {
     "duration": 0.109162,
     "end_time": "2025-08-31T10:52:19.096690",
     "exception": false,
     "start_time": "2025-08-31T10:52:18.987528",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accelerator Available: cuda\n",
      "Units: 2\n"
     ]
    }
   ],
   "source": [
    "# Accelerator Device\n",
    "acc_device = torch.accelerator.current_accelerator()\n",
    "print(f\"Accelerator Available: {acc_device}\")\n",
    "print(f\"Units: {torch.cuda.device_count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "adcfc343",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T10:52:19.102609Z",
     "iopub.status.busy": "2025-08-31T10:52:19.102348Z",
     "iopub.status.idle": "2025-08-31T18:00:42.940161Z",
     "shell.execute_reply": "2025-08-31T18:00:42.939359Z"
    },
    "papermill": {
     "duration": 25703.857138,
     "end_time": "2025-08-31T18:00:42.956357",
     "exception": false,
     "start_time": "2025-08-31T10:52:19.099219",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datastring Metrics after loading the dataset on Vocab:\n",
      "Length of the Dataset: 33151655\n",
      "\n",
      "First 1000 Chars:\n",
      "Now, I won't deny that when I purchased this off eBay, I had high expectations. This was an incredible out-of-print work from the master of comedy that I so enjoy. However, I was soon to be disappointed. Apologies to those who enjoyed it, but I just found the Compleat Al to be very difficult to watch. I got a few smiles, sure, but the majority of the funny came from the music videos (which I've got on DVD) and the rest was basically filler. You could tell that this was not Al's greatest video achievement (that honor goes to UHF). Honestly, I doubt if this will ever make the jump to DVD, so if you're an ultra-hardcore Al fan and just HAVE to own everything, buy the tape off eBay. Just don't pay too much for it.\n",
      "The saddest thing about this \"tribute\" is that almost all the singers (including the otherwise incredibly talented Nick Cave) seem to have missed the whole point where Cohen's intensity lies: by delivering his lines in an almost tuneless poise, Cohen transmits the full extent of \n",
      "\n",
      "Vocabulary Size: 77\n",
      "\n",
      "Vocabulary:\n",
      "\t\n",
      " !\"'(),-./0123456789:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "\n",
      "-----------\n",
      "\n",
      "Model Vocabulary from the Character-Level Tokenizer:\n",
      " ['\\t', '\\n', ' ', '!', '\"', \"'\", '(', ')', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "\n",
      "-----------\n",
      "\n",
      "Training the Model:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at 100th Epoch -> Train Set: 5.4574 | Valid Set: 5.4197 | Avg Step-Time: 0.268 secs\n",
      "Loss at 200th Epoch -> Train Set: 4.9984 | Valid Set: 4.9976 | Avg Step-Time: 0.292 secs\n",
      "Loss at 300th Epoch -> Train Set: 4.8764 | Valid Set: 4.8762 | Avg Step-Time: 0.292 secs\n",
      "Loss at 400th Epoch -> Train Set: 4.5353 | Valid Set: 4.5330 | Avg Step-Time: 0.292 secs\n",
      "Loss at 500th Epoch -> Train Set: 4.1008 | Valid Set: 4.0980 | Avg Step-Time: 0.293 secs\n",
      "Loss at 600th Epoch -> Train Set: 3.7964 | Valid Set: 3.7929 | Avg Step-Time: 0.291 secs\n",
      "Loss at 700th Epoch -> Train Set: 3.5827 | Valid Set: 3.5807 | Avg Step-Time: 0.291 secs\n",
      "Loss at 800th Epoch -> Train Set: 3.4170 | Valid Set: 3.4223 | Avg Step-Time: 0.291 secs\n",
      "Loss at 900th Epoch -> Train Set: 3.2856 | Valid Set: 3.2963 | Avg Step-Time: 0.291 secs\n",
      "Loss at 1000th Epoch -> Train Set: 3.1917 | Valid Set: 3.1969 | Avg Step-Time: 0.291 secs\n",
      "Loss at 1100th Epoch -> Train Set: 3.1076 | Valid Set: 3.1155 | Avg Step-Time: 0.291 secs\n",
      "Loss at 1200th Epoch -> Train Set: 3.0473 | Valid Set: 3.0521 | Avg Step-Time: 0.294 secs\n",
      "Loss at 1300th Epoch -> Train Set: 2.9873 | Valid Set: 2.9945 | Avg Step-Time: 0.290 secs\n",
      "Loss at 1400th Epoch -> Train Set: 2.9324 | Valid Set: 2.9464 | Avg Step-Time: 0.290 secs\n",
      "Loss at 1500th Epoch -> Train Set: 2.8926 | Valid Set: 2.9053 | Avg Step-Time: 0.290 secs\n",
      "Loss at 1600th Epoch -> Train Set: 2.8626 | Valid Set: 2.8717 | Avg Step-Time: 0.290 secs\n",
      "Loss at 1700th Epoch -> Train Set: 2.8252 | Valid Set: 2.8351 | Avg Step-Time: 0.292 secs\n",
      "Loss at 1800th Epoch -> Train Set: 2.7977 | Valid Set: 2.8092 | Avg Step-Time: 0.292 secs\n",
      "Loss at 1900th Epoch -> Train Set: 2.7790 | Valid Set: 2.7867 | Avg Step-Time: 0.293 secs\n",
      "Loss at 2000th Epoch -> Train Set: 2.7532 | Valid Set: 2.7652 | Avg Step-Time: 0.293 secs\n",
      "Loss at 2100th Epoch -> Train Set: 2.7284 | Valid Set: 2.7411 | Avg Step-Time: 0.290 secs\n",
      "Loss at 2200th Epoch -> Train Set: 2.7144 | Valid Set: 2.7219 | Avg Step-Time: 0.290 secs\n",
      "Loss at 2300th Epoch -> Train Set: 2.6966 | Valid Set: 2.7079 | Avg Step-Time: 0.291 secs\n",
      "Loss at 2400th Epoch -> Train Set: 2.6785 | Valid Set: 2.6870 | Avg Step-Time: 0.290 secs\n",
      "Loss at 2500th Epoch -> Train Set: 2.6595 | Valid Set: 2.6725 | Avg Step-Time: 0.292 secs\n",
      "Loss at 2600th Epoch -> Train Set: 2.6392 | Valid Set: 2.6641 | Avg Step-Time: 0.293 secs\n",
      "Loss at 2700th Epoch -> Train Set: 2.6343 | Valid Set: 2.6513 | Avg Step-Time: 0.293 secs\n",
      "Loss at 2800th Epoch -> Train Set: 2.6172 | Valid Set: 2.6384 | Avg Step-Time: 0.291 secs\n",
      "Loss at 2900th Epoch -> Train Set: 2.6067 | Valid Set: 2.6303 | Avg Step-Time: 0.290 secs\n",
      "Loss at 3000th Epoch -> Train Set: 2.6004 | Valid Set: 2.6134 | Avg Step-Time: 0.290 secs\n",
      "Loss at 3100th Epoch -> Train Set: 2.5903 | Valid Set: 2.5995 | Avg Step-Time: 0.290 secs\n",
      "Loss at 3200th Epoch -> Train Set: 2.5819 | Valid Set: 2.5999 | Avg Step-Time: 0.290 secs\n",
      "Loss at 3300th Epoch -> Train Set: 2.5696 | Valid Set: 2.5823 | Avg Step-Time: 0.290 secs\n",
      "Loss at 3400th Epoch -> Train Set: 2.5626 | Valid Set: 2.5794 | Avg Step-Time: 0.289 secs\n",
      "Loss at 3500th Epoch -> Train Set: 2.5495 | Valid Set: 2.5683 | Avg Step-Time: 0.292 secs\n",
      "Loss at 3600th Epoch -> Train Set: 2.5444 | Valid Set: 2.5614 | Avg Step-Time: 0.286 secs\n",
      "Loss at 3700th Epoch -> Train Set: 2.5353 | Valid Set: 2.5599 | Avg Step-Time: 0.287 secs\n",
      "Loss at 3800th Epoch -> Train Set: 2.5312 | Valid Set: 2.5532 | Avg Step-Time: 0.287 secs\n",
      "Loss at 3900th Epoch -> Train Set: 2.5186 | Valid Set: 2.5436 | Avg Step-Time: 0.288 secs\n",
      "Loss at 4000th Epoch -> Train Set: 2.5140 | Valid Set: 2.5311 | Avg Step-Time: 0.288 secs\n",
      "Loss at 4100th Epoch -> Train Set: 2.5040 | Valid Set: 2.5281 | Avg Step-Time: 0.289 secs\n",
      "Loss at 4200th Epoch -> Train Set: 2.4998 | Valid Set: 2.5238 | Avg Step-Time: 0.290 secs\n",
      "Loss at 4300th Epoch -> Train Set: 2.4961 | Valid Set: 2.5167 | Avg Step-Time: 0.291 secs\n",
      "Loss at 4400th Epoch -> Train Set: 2.4912 | Valid Set: 2.5099 | Avg Step-Time: 0.287 secs\n",
      "Loss at 4500th Epoch -> Train Set: 2.4807 | Valid Set: 2.5078 | Avg Step-Time: 0.287 secs\n",
      "Loss at 4600th Epoch -> Train Set: 2.4785 | Valid Set: 2.4991 | Avg Step-Time: 0.288 secs\n",
      "Loss at 4700th Epoch -> Train Set: 2.4767 | Valid Set: 2.4946 | Avg Step-Time: 0.286 secs\n",
      "Loss at 4800th Epoch -> Train Set: 2.4684 | Valid Set: 2.4936 | Avg Step-Time: 0.288 secs\n",
      "Loss at 4900th Epoch -> Train Set: 2.4563 | Valid Set: 2.4823 | Avg Step-Time: 0.290 secs\n",
      "Loss at 5000th Epoch -> Train Set: 2.4525 | Valid Set: 2.4849 | Avg Step-Time: 0.290 secs\n",
      "Loss at 5100th Epoch -> Train Set: 2.4547 | Valid Set: 2.4738 | Avg Step-Time: 0.290 secs\n",
      "Loss at 5200th Epoch -> Train Set: 2.4477 | Valid Set: 2.4777 | Avg Step-Time: 0.288 secs\n",
      "Loss at 5300th Epoch -> Train Set: 2.4410 | Valid Set: 2.4674 | Avg Step-Time: 0.288 secs\n",
      "Loss at 5400th Epoch -> Train Set: 2.4401 | Valid Set: 2.4653 | Avg Step-Time: 0.290 secs\n",
      "Loss at 5500th Epoch -> Train Set: 2.4361 | Valid Set: 2.4598 | Avg Step-Time: 0.290 secs\n",
      "Loss at 5600th Epoch -> Train Set: 2.4312 | Valid Set: 2.4549 | Avg Step-Time: 0.290 secs\n",
      "Loss at 5700th Epoch -> Train Set: 2.4287 | Valid Set: 2.4508 | Avg Step-Time: 0.288 secs\n",
      "Loss at 5800th Epoch -> Train Set: 2.4253 | Valid Set: 2.4529 | Avg Step-Time: 0.291 secs\n",
      "Loss at 5900th Epoch -> Train Set: 2.4189 | Valid Set: 2.4458 | Avg Step-Time: 0.288 secs\n",
      "Loss at 6000th Epoch -> Train Set: 2.4127 | Valid Set: 2.4403 | Avg Step-Time: 0.287 secs\n",
      "Loss at 6100th Epoch -> Train Set: 2.4072 | Valid Set: 2.4397 | Avg Step-Time: 0.290 secs\n",
      "Loss at 6200th Epoch -> Train Set: 2.4072 | Valid Set: 2.4353 | Avg Step-Time: 0.290 secs\n",
      "Loss at 6300th Epoch -> Train Set: 2.4044 | Valid Set: 2.4314 | Avg Step-Time: 0.289 secs\n",
      "Loss at 6400th Epoch -> Train Set: 2.4030 | Valid Set: 2.4301 | Avg Step-Time: 0.289 secs\n",
      "Loss at 6500th Epoch -> Train Set: 2.4025 | Valid Set: 2.4258 | Avg Step-Time: 0.291 secs\n",
      "Loss at 6600th Epoch -> Train Set: 2.3941 | Valid Set: 2.4235 | Avg Step-Time: 0.292 secs\n",
      "Loss at 6700th Epoch -> Train Set: 2.3943 | Valid Set: 2.4254 | Avg Step-Time: 0.289 secs\n",
      "Loss at 6800th Epoch -> Train Set: 2.3858 | Valid Set: 2.4222 | Avg Step-Time: 0.289 secs\n",
      "Loss at 6900th Epoch -> Train Set: 2.3886 | Valid Set: 2.4150 | Avg Step-Time: 0.289 secs\n",
      "Loss at 7000th Epoch -> Train Set: 2.3863 | Valid Set: 2.4120 | Avg Step-Time: 0.289 secs\n",
      "Loss at 7100th Epoch -> Train Set: 2.3784 | Valid Set: 2.4117 | Avg Step-Time: 0.290 secs\n",
      "Loss at 7200th Epoch -> Train Set: 2.3774 | Valid Set: 2.4081 | Avg Step-Time: 0.291 secs\n",
      "Loss at 7300th Epoch -> Train Set: 2.3778 | Valid Set: 2.4068 | Avg Step-Time: 0.293 secs\n",
      "Loss at 7400th Epoch -> Train Set: 2.3717 | Valid Set: 2.4038 | Avg Step-Time: 0.290 secs\n",
      "Loss at 7500th Epoch -> Train Set: 2.3670 | Valid Set: 2.4011 | Avg Step-Time: 0.288 secs\n",
      "Loss at 7600th Epoch -> Train Set: 2.3635 | Valid Set: 2.3961 | Avg Step-Time: 0.289 secs\n",
      "Loss at 7700th Epoch -> Train Set: 2.3588 | Valid Set: 2.3957 | Avg Step-Time: 0.290 secs\n",
      "Loss at 7800th Epoch -> Train Set: 2.3605 | Valid Set: 2.3911 | Avg Step-Time: 0.291 secs\n",
      "Loss at 7900th Epoch -> Train Set: 2.3596 | Valid Set: 2.3913 | Avg Step-Time: 0.289 secs\n",
      "Loss at 8000th Epoch -> Train Set: 2.3585 | Valid Set: 2.3896 | Avg Step-Time: 0.289 secs\n",
      "Loss at 8100th Epoch -> Train Set: 2.3510 | Valid Set: 2.3842 | Avg Step-Time: 0.293 secs\n",
      "Loss at 8200th Epoch -> Train Set: 2.3478 | Valid Set: 2.3812 | Avg Step-Time: 0.290 secs\n",
      "Loss at 8300th Epoch -> Train Set: 2.3499 | Valid Set: 2.3803 | Avg Step-Time: 0.290 secs\n",
      "Loss at 8400th Epoch -> Train Set: 2.3413 | Valid Set: 2.3771 | Avg Step-Time: 0.287 secs\n",
      "Loss at 8500th Epoch -> Train Set: 2.3426 | Valid Set: 2.3760 | Avg Step-Time: 0.288 secs\n",
      "Loss at 8600th Epoch -> Train Set: 2.3367 | Valid Set: 2.3769 | Avg Step-Time: 0.290 secs\n",
      "Loss at 8700th Epoch -> Train Set: 2.3362 | Valid Set: 2.3739 | Avg Step-Time: 0.290 secs\n",
      "Loss at 8800th Epoch -> Train Set: 2.3362 | Valid Set: 2.3642 | Avg Step-Time: 0.292 secs\n",
      "Loss at 8900th Epoch -> Train Set: 2.3303 | Valid Set: 2.3674 | Avg Step-Time: 0.291 secs\n",
      "Loss at 9000th Epoch -> Train Set: 2.3272 | Valid Set: 2.3641 | Avg Step-Time: 0.289 secs\n",
      "Loss at 9100th Epoch -> Train Set: 2.3277 | Valid Set: 2.3642 | Avg Step-Time: 0.288 secs\n",
      "Loss at 9200th Epoch -> Train Set: 2.3178 | Valid Set: 2.3546 | Avg Step-Time: 0.289 secs\n",
      "Loss at 9300th Epoch -> Train Set: 2.3249 | Valid Set: 2.3597 | Avg Step-Time: 0.289 secs\n",
      "Loss at 9400th Epoch -> Train Set: 2.3217 | Valid Set: 2.3611 | Avg Step-Time: 0.290 secs\n",
      "Loss at 9500th Epoch -> Train Set: 2.3199 | Valid Set: 2.3580 | Avg Step-Time: 0.292 secs\n",
      "Loss at 9600th Epoch -> Train Set: 2.3155 | Valid Set: 2.3557 | Avg Step-Time: 0.292 secs\n",
      "Loss at 9700th Epoch -> Train Set: 2.3144 | Valid Set: 2.3521 | Avg Step-Time: 0.291 secs\n",
      "Loss at 9800th Epoch -> Train Set: 2.3127 | Valid Set: 2.3530 | Avg Step-Time: 0.290 secs\n",
      "Loss at 9900th Epoch -> Train Set: 2.3153 | Valid Set: 2.3496 | Avg Step-Time: 0.289 secs\n",
      "Loss at 10000th Epoch -> Train Set: 2.3123 | Valid Set: 2.3473 | Avg Step-Time: 0.289 secs\n",
      "Loss at 10100th Epoch -> Train Set: 2.3110 | Valid Set: 2.3470 | Avg Step-Time: 0.291 secs\n",
      "Loss at 10200th Epoch -> Train Set: 2.3064 | Valid Set: 2.3454 | Avg Step-Time: 0.291 secs\n",
      "Loss at 10300th Epoch -> Train Set: 2.2994 | Valid Set: 2.3458 | Avg Step-Time: 0.291 secs\n",
      "Loss at 10400th Epoch -> Train Set: 2.3005 | Valid Set: 2.3402 | Avg Step-Time: 0.294 secs\n",
      "Loss at 10500th Epoch -> Train Set: 2.3015 | Valid Set: 2.3372 | Avg Step-Time: 0.290 secs\n",
      "Loss at 10600th Epoch -> Train Set: 2.3004 | Valid Set: 2.3401 | Avg Step-Time: 0.290 secs\n",
      "Loss at 10700th Epoch -> Train Set: 2.2971 | Valid Set: 2.3433 | Avg Step-Time: 0.290 secs\n",
      "Loss at 10800th Epoch -> Train Set: 2.2976 | Valid Set: 2.3307 | Avg Step-Time: 0.290 secs\n",
      "Loss at 10900th Epoch -> Train Set: 2.2959 | Valid Set: 2.3315 | Avg Step-Time: 0.291 secs\n",
      "Loss at 11000th Epoch -> Train Set: 2.2937 | Valid Set: 2.3281 | Avg Step-Time: 0.291 secs\n",
      "Loss at 11100th Epoch -> Train Set: 2.2885 | Valid Set: 2.3329 | Avg Step-Time: 0.292 secs\n",
      "Loss at 11200th Epoch -> Train Set: 2.2887 | Valid Set: 2.3362 | Avg Step-Time: 0.292 secs\n",
      "Loss at 11300th Epoch -> Train Set: 2.2851 | Valid Set: 2.3231 | Avg Step-Time: 0.290 secs\n",
      "Loss at 11400th Epoch -> Train Set: 2.2851 | Valid Set: 2.3296 | Avg Step-Time: 0.290 secs\n",
      "Loss at 11500th Epoch -> Train Set: 2.2792 | Valid Set: 2.3254 | Avg Step-Time: 0.290 secs\n",
      "Loss at 11600th Epoch -> Train Set: 2.2817 | Valid Set: 2.3232 | Avg Step-Time: 0.289 secs\n",
      "Loss at 11700th Epoch -> Train Set: 2.2822 | Valid Set: 2.3260 | Avg Step-Time: 0.289 secs\n",
      "Loss at 11800th Epoch -> Train Set: 2.2819 | Valid Set: 2.3171 | Avg Step-Time: 0.291 secs\n",
      "Loss at 11900th Epoch -> Train Set: 2.2784 | Valid Set: 2.3266 | Avg Step-Time: 0.292 secs\n",
      "Loss at 12000th Epoch -> Train Set: 2.2786 | Valid Set: 2.3213 | Avg Step-Time: 0.291 secs\n",
      "Loss at 12100th Epoch -> Train Set: 2.2769 | Valid Set: 2.3171 | Avg Step-Time: 0.290 secs\n",
      "Loss at 12200th Epoch -> Train Set: 2.2812 | Valid Set: 2.3144 | Avg Step-Time: 0.289 secs\n",
      "Loss at 12300th Epoch -> Train Set: 2.2720 | Valid Set: 2.3192 | Avg Step-Time: 0.289 secs\n",
      "Loss at 12400th Epoch -> Train Set: 2.2732 | Valid Set: 2.3208 | Avg Step-Time: 0.291 secs\n",
      "Loss at 12500th Epoch -> Train Set: 2.2732 | Valid Set: 2.3149 | Avg Step-Time: 0.291 secs\n",
      "Loss at 12600th Epoch -> Train Set: 2.2654 | Valid Set: 2.3149 | Avg Step-Time: 0.291 secs\n",
      "Loss at 12700th Epoch -> Train Set: 2.2691 | Valid Set: 2.3134 | Avg Step-Time: 0.294 secs\n",
      "Loss at 12800th Epoch -> Train Set: 2.2630 | Valid Set: 2.3114 | Avg Step-Time: 0.290 secs\n",
      "Loss at 12900th Epoch -> Train Set: 2.2663 | Valid Set: 2.3080 | Avg Step-Time: 0.289 secs\n",
      "Loss at 13000th Epoch -> Train Set: 2.2615 | Valid Set: 2.3152 | Avg Step-Time: 0.290 secs\n",
      "Loss at 13100th Epoch -> Train Set: 2.2603 | Valid Set: 2.3122 | Avg Step-Time: 0.289 secs\n",
      "Loss at 13200th Epoch -> Train Set: 2.2572 | Valid Set: 2.3065 | Avg Step-Time: 0.291 secs\n",
      "Loss at 13300th Epoch -> Train Set: 2.2591 | Valid Set: 2.3049 | Avg Step-Time: 0.291 secs\n",
      "Loss at 13400th Epoch -> Train Set: 2.2602 | Valid Set: 2.3070 | Avg Step-Time: 0.292 secs\n",
      "Loss at 13500th Epoch -> Train Set: 2.2558 | Valid Set: 2.3035 | Avg Step-Time: 0.292 secs\n",
      "Loss at 13600th Epoch -> Train Set: 2.2567 | Valid Set: 2.2976 | Avg Step-Time: 0.290 secs\n",
      "Loss at 13700th Epoch -> Train Set: 2.2544 | Valid Set: 2.3043 | Avg Step-Time: 0.290 secs\n",
      "Loss at 13800th Epoch -> Train Set: 2.2551 | Valid Set: 2.3039 | Avg Step-Time: 0.289 secs\n",
      "Loss at 13900th Epoch -> Train Set: 2.2557 | Valid Set: 2.3039 | Avg Step-Time: 0.290 secs\n",
      "Loss at 14000th Epoch -> Train Set: 2.2550 | Valid Set: 2.3014 | Avg Step-Time: 0.291 secs\n",
      "Loss at 14100th Epoch -> Train Set: 2.2450 | Valid Set: 2.2988 | Avg Step-Time: 0.292 secs\n",
      "Loss at 14200th Epoch -> Train Set: 2.2455 | Valid Set: 2.2946 | Avg Step-Time: 0.292 secs\n",
      "Loss at 14300th Epoch -> Train Set: 2.2468 | Valid Set: 2.2967 | Avg Step-Time: 0.291 secs\n",
      "Loss at 14400th Epoch -> Train Set: 2.2484 | Valid Set: 2.2996 | Avg Step-Time: 0.290 secs\n",
      "Loss at 14500th Epoch -> Train Set: 2.2434 | Valid Set: 2.2921 | Avg Step-Time: 0.290 secs\n",
      "Loss at 14600th Epoch -> Train Set: 2.2402 | Valid Set: 2.2934 | Avg Step-Time: 0.289 secs\n",
      "Loss at 14700th Epoch -> Train Set: 2.2482 | Valid Set: 2.2978 | Avg Step-Time: 0.289 secs\n",
      "Loss at 14800th Epoch -> Train Set: 2.2409 | Valid Set: 2.2918 | Avg Step-Time: 0.290 secs\n",
      "Loss at 14900th Epoch -> Train Set: 2.2430 | Valid Set: 2.2960 | Avg Step-Time: 0.291 secs\n",
      "Loss at 15000th Epoch -> Train Set: 2.2410 | Valid Set: 2.2869 | Avg Step-Time: 0.294 secs\n",
      "Loss at 15100th Epoch -> Train Set: 2.2410 | Valid Set: 2.2841 | Avg Step-Time: 0.290 secs\n",
      "Loss at 15200th Epoch -> Train Set: 2.2367 | Valid Set: 2.2904 | Avg Step-Time: 0.290 secs\n",
      "Loss at 15300th Epoch -> Train Set: 2.2364 | Valid Set: 2.2897 | Avg Step-Time: 0.290 secs\n",
      "Loss at 15400th Epoch -> Train Set: 2.2396 | Valid Set: 2.2872 | Avg Step-Time: 0.288 secs\n",
      "Loss at 15500th Epoch -> Train Set: 2.2351 | Valid Set: 2.2895 | Avg Step-Time: 0.289 secs\n",
      "Loss at 15600th Epoch -> Train Set: 2.2332 | Valid Set: 2.2846 | Avg Step-Time: 0.290 secs\n",
      "Loss at 15700th Epoch -> Train Set: 2.2322 | Valid Set: 2.2817 | Avg Step-Time: 0.292 secs\n",
      "Loss at 15800th Epoch -> Train Set: 2.2319 | Valid Set: 2.2836 | Avg Step-Time: 0.292 secs\n",
      "Loss at 15900th Epoch -> Train Set: 2.2341 | Valid Set: 2.2814 | Avg Step-Time: 0.287 secs\n",
      "Loss at 16000th Epoch -> Train Set: 2.2332 | Valid Set: 2.2877 | Avg Step-Time: 0.289 secs\n",
      "Loss at 16100th Epoch -> Train Set: 2.2278 | Valid Set: 2.2819 | Avg Step-Time: 0.289 secs\n",
      "Loss at 16200th Epoch -> Train Set: 2.2303 | Valid Set: 2.2792 | Avg Step-Time: 0.290 secs\n",
      "Loss at 16300th Epoch -> Train Set: 2.2292 | Valid Set: 2.2793 | Avg Step-Time: 0.291 secs\n",
      "Loss at 16400th Epoch -> Train Set: 2.2266 | Valid Set: 2.2822 | Avg Step-Time: 0.290 secs\n",
      "Loss at 16500th Epoch -> Train Set: 2.2249 | Valid Set: 2.2773 | Avg Step-Time: 0.291 secs\n",
      "Loss at 16600th Epoch -> Train Set: 2.2240 | Valid Set: 2.2826 | Avg Step-Time: 0.289 secs\n",
      "Loss at 16700th Epoch -> Train Set: 2.2262 | Valid Set: 2.2765 | Avg Step-Time: 0.288 secs\n",
      "Loss at 16800th Epoch -> Train Set: 2.2279 | Valid Set: 2.2766 | Avg Step-Time: 0.289 secs\n",
      "Loss at 16900th Epoch -> Train Set: 2.2268 | Valid Set: 2.2806 | Avg Step-Time: 0.288 secs\n",
      "Loss at 17000th Epoch -> Train Set: 2.2231 | Valid Set: 2.2759 | Avg Step-Time: 0.288 secs\n",
      "Loss at 17100th Epoch -> Train Set: 2.2202 | Valid Set: 2.2771 | Avg Step-Time: 0.289 secs\n",
      "Loss at 17200th Epoch -> Train Set: 2.2184 | Valid Set: 2.2759 | Avg Step-Time: 0.290 secs\n",
      "Loss at 17300th Epoch -> Train Set: 2.2173 | Valid Set: 2.2754 | Avg Step-Time: 0.292 secs\n",
      "Loss at 17400th Epoch -> Train Set: 2.2186 | Valid Set: 2.2714 | Avg Step-Time: 0.291 secs\n",
      "Loss at 17500th Epoch -> Train Set: 2.2175 | Valid Set: 2.2761 | Avg Step-Time: 0.289 secs\n",
      "Loss at 17600th Epoch -> Train Set: 2.2142 | Valid Set: 2.2696 | Avg Step-Time: 0.286 secs\n",
      "Loss at 17700th Epoch -> Train Set: 2.2159 | Valid Set: 2.2711 | Avg Step-Time: 0.287 secs\n",
      "Loss at 17800th Epoch -> Train Set: 2.2151 | Valid Set: 2.2711 | Avg Step-Time: 0.290 secs\n",
      "Loss at 17900th Epoch -> Train Set: 2.2111 | Valid Set: 2.2732 | Avg Step-Time: 0.288 secs\n",
      "Loss at 18000th Epoch -> Train Set: 2.2199 | Valid Set: 2.2716 | Avg Step-Time: 0.288 secs\n",
      "Loss at 18100th Epoch -> Train Set: 2.2148 | Valid Set: 2.2653 | Avg Step-Time: 0.288 secs\n",
      "Loss at 18200th Epoch -> Train Set: 2.2120 | Valid Set: 2.2646 | Avg Step-Time: 0.285 secs\n",
      "Loss at 18300th Epoch -> Train Set: 2.2127 | Valid Set: 2.2649 | Avg Step-Time: 0.286 secs\n",
      "Loss at 18400th Epoch -> Train Set: 2.2094 | Valid Set: 2.2605 | Avg Step-Time: 0.286 secs\n",
      "Loss at 18500th Epoch -> Train Set: 2.2078 | Valid Set: 2.2625 | Avg Step-Time: 0.287 secs\n",
      "Loss at 18600th Epoch -> Train Set: 2.2077 | Valid Set: 2.2684 | Avg Step-Time: 0.289 secs\n",
      "Loss at 18700th Epoch -> Train Set: 2.2065 | Valid Set: 2.2691 | Avg Step-Time: 0.292 secs\n",
      "Loss at 18800th Epoch -> Train Set: 2.2123 | Valid Set: 2.2642 | Avg Step-Time: 0.291 secs\n",
      "Loss at 18900th Epoch -> Train Set: 2.2051 | Valid Set: 2.2599 | Avg Step-Time: 0.289 secs\n",
      "Loss at 19000th Epoch -> Train Set: 2.2042 | Valid Set: 2.2560 | Avg Step-Time: 0.288 secs\n",
      "Loss at 19100th Epoch -> Train Set: 2.2060 | Valid Set: 2.2624 | Avg Step-Time: 0.288 secs\n",
      "Loss at 19200th Epoch -> Train Set: 2.2048 | Valid Set: 2.2667 | Avg Step-Time: 0.288 secs\n",
      "Loss at 19300th Epoch -> Train Set: 2.2033 | Valid Set: 2.2653 | Avg Step-Time: 0.289 secs\n",
      "Loss at 19400th Epoch -> Train Set: 2.2063 | Valid Set: 2.2593 | Avg Step-Time: 0.289 secs\n",
      "Loss at 19500th Epoch -> Train Set: 2.1996 | Valid Set: 2.2669 | Avg Step-Time: 0.289 secs\n",
      "Loss at 19600th Epoch -> Train Set: 2.2007 | Valid Set: 2.2613 | Avg Step-Time: 0.290 secs\n",
      "Loss at 19700th Epoch -> Train Set: 2.2021 | Valid Set: 2.2584 | Avg Step-Time: 0.289 secs\n",
      "Loss at 19800th Epoch -> Train Set: 2.2020 | Valid Set: 2.2624 | Avg Step-Time: 0.288 secs\n",
      "Loss at 19900th Epoch -> Train Set: 2.2027 | Valid Set: 2.2593 | Avg Step-Time: 0.287 secs\n",
      "Loss at 20000th Epoch -> Train Set: 2.1980 | Valid Set: 2.2577 | Avg Step-Time: 0.288 secs\n",
      "Loss at 20100th Epoch -> Train Set: 2.1995 | Valid Set: 2.2552 | Avg Step-Time: 0.289 secs\n",
      "Loss at 20200th Epoch -> Train Set: 2.2032 | Valid Set: 2.2519 | Avg Step-Time: 0.289 secs\n",
      "Loss at 20300th Epoch -> Train Set: 2.1958 | Valid Set: 2.2609 | Avg Step-Time: 0.290 secs\n",
      "Loss at 20400th Epoch -> Train Set: 2.1955 | Valid Set: 2.2492 | Avg Step-Time: 0.290 secs\n",
      "Loss at 20500th Epoch -> Train Set: 2.1953 | Valid Set: 2.2561 | Avg Step-Time: 0.287 secs\n",
      "Loss at 20600th Epoch -> Train Set: 2.1951 | Valid Set: 2.2482 | Avg Step-Time: 0.287 secs\n",
      "Loss at 20700th Epoch -> Train Set: 2.1956 | Valid Set: 2.2540 | Avg Step-Time: 0.287 secs\n",
      "Loss at 20800th Epoch -> Train Set: 2.1952 | Valid Set: 2.2483 | Avg Step-Time: 0.287 secs\n",
      "Loss at 20900th Epoch -> Train Set: 2.1911 | Valid Set: 2.2507 | Avg Step-Time: 0.288 secs\n",
      "Loss at 21000th Epoch -> Train Set: 2.1941 | Valid Set: 2.2535 | Avg Step-Time: 0.291 secs\n",
      "Loss at 21100th Epoch -> Train Set: 2.1913 | Valid Set: 2.2487 | Avg Step-Time: 0.291 secs\n",
      "Loss at 21200th Epoch -> Train Set: 2.1896 | Valid Set: 2.2527 | Avg Step-Time: 0.289 secs\n",
      "Loss at 21300th Epoch -> Train Set: 2.1926 | Valid Set: 2.2469 | Avg Step-Time: 0.287 secs\n",
      "Loss at 21400th Epoch -> Train Set: 2.1892 | Valid Set: 2.2520 | Avg Step-Time: 0.287 secs\n",
      "Loss at 21500th Epoch -> Train Set: 2.1929 | Valid Set: 2.2469 | Avg Step-Time: 0.287 secs\n",
      "Loss at 21600th Epoch -> Train Set: 2.1922 | Valid Set: 2.2503 | Avg Step-Time: 0.288 secs\n",
      "Loss at 21700th Epoch -> Train Set: 2.1878 | Valid Set: 2.2411 | Avg Step-Time: 0.288 secs\n",
      "Loss at 21800th Epoch -> Train Set: 2.1895 | Valid Set: 2.2486 | Avg Step-Time: 0.288 secs\n",
      "Loss at 21900th Epoch -> Train Set: 2.1888 | Valid Set: 2.2422 | Avg Step-Time: 0.289 secs\n",
      "Loss at 22000th Epoch -> Train Set: 2.1872 | Valid Set: 2.2439 | Avg Step-Time: 0.287 secs\n",
      "Loss at 22100th Epoch -> Train Set: 2.1846 | Valid Set: 2.2453 | Avg Step-Time: 0.289 secs\n",
      "Loss at 22200th Epoch -> Train Set: 2.1829 | Valid Set: 2.2482 | Avg Step-Time: 0.287 secs\n",
      "Loss at 22300th Epoch -> Train Set: 2.1842 | Valid Set: 2.2447 | Avg Step-Time: 0.286 secs\n",
      "Loss at 22400th Epoch -> Train Set: 2.1823 | Valid Set: 2.2476 | Avg Step-Time: 0.288 secs\n",
      "Loss at 22500th Epoch -> Train Set: 2.1855 | Valid Set: 2.2472 | Avg Step-Time: 0.288 secs\n",
      "Loss at 22600th Epoch -> Train Set: 2.1807 | Valid Set: 2.2431 | Avg Step-Time: 0.290 secs\n",
      "Loss at 22700th Epoch -> Train Set: 2.1791 | Valid Set: 2.2461 | Avg Step-Time: 0.290 secs\n",
      "Loss at 22800th Epoch -> Train Set: 2.1836 | Valid Set: 2.2467 | Avg Step-Time: 0.287 secs\n",
      "Loss at 22900th Epoch -> Train Set: 2.1826 | Valid Set: 2.2425 | Avg Step-Time: 0.286 secs\n",
      "Loss at 23000th Epoch -> Train Set: 2.1812 | Valid Set: 2.2429 | Avg Step-Time: 0.286 secs\n",
      "Loss at 23100th Epoch -> Train Set: 2.1810 | Valid Set: 2.2430 | Avg Step-Time: 0.286 secs\n",
      "Loss at 23200th Epoch -> Train Set: 2.1786 | Valid Set: 2.2436 | Avg Step-Time: 0.288 secs\n",
      "Loss at 23300th Epoch -> Train Set: 2.1737 | Valid Set: 2.2382 | Avg Step-Time: 0.290 secs\n",
      "Loss at 23400th Epoch -> Train Set: 2.1784 | Valid Set: 2.2409 | Avg Step-Time: 0.290 secs\n",
      "Loss at 23500th Epoch -> Train Set: 2.1779 | Valid Set: 2.2453 | Avg Step-Time: 0.290 secs\n",
      "Loss at 23600th Epoch -> Train Set: 2.1753 | Valid Set: 2.2408 | Avg Step-Time: 0.289 secs\n",
      "Loss at 23700th Epoch -> Train Set: 2.1759 | Valid Set: 2.2367 | Avg Step-Time: 0.289 secs\n",
      "Loss at 23800th Epoch -> Train Set: 2.1764 | Valid Set: 2.2429 | Avg Step-Time: 0.289 secs\n",
      "Loss at 23900th Epoch -> Train Set: 2.1736 | Valid Set: 2.2414 | Avg Step-Time: 0.290 secs\n",
      "Loss at 24000th Epoch -> Train Set: 2.1668 | Valid Set: 2.2371 | Avg Step-Time: 0.291 secs\n",
      "Loss at 24100th Epoch -> Train Set: 2.1678 | Valid Set: 2.2349 | Avg Step-Time: 0.289 secs\n",
      "Loss at 24200th Epoch -> Train Set: 2.1777 | Valid Set: 2.2335 | Avg Step-Time: 0.292 secs\n",
      "Loss at 24300th Epoch -> Train Set: 2.1674 | Valid Set: 2.2404 | Avg Step-Time: 0.292 secs\n",
      "Loss at 24400th Epoch -> Train Set: 2.1702 | Valid Set: 2.2365 | Avg Step-Time: 0.291 secs\n",
      "Loss at 24500th Epoch -> Train Set: 2.1757 | Valid Set: 2.2388 | Avg Step-Time: 0.290 secs\n",
      "Loss at 24600th Epoch -> Train Set: 2.1734 | Valid Set: 2.2325 | Avg Step-Time: 0.290 secs\n",
      "Loss at 24700th Epoch -> Train Set: 2.1691 | Valid Set: 2.2350 | Avg Step-Time: 0.292 secs\n",
      "Loss at 24800th Epoch -> Train Set: 2.1683 | Valid Set: 2.2330 | Avg Step-Time: 0.291 secs\n",
      "Loss at 24900th Epoch -> Train Set: 2.1702 | Valid Set: 2.2374 | Avg Step-Time: 0.292 secs\n",
      "Loss at 25000th Epoch -> Train Set: 2.1713 | Valid Set: 2.2299 | Avg Step-Time: 0.293 secs\n",
      "Loss at 25100th Epoch -> Train Set: 2.1672 | Valid Set: 2.2298 | Avg Step-Time: 0.290 secs\n",
      "Loss at 25200th Epoch -> Train Set: 2.1624 | Valid Set: 2.2377 | Avg Step-Time: 0.290 secs\n",
      "Loss at 25300th Epoch -> Train Set: 2.1658 | Valid Set: 2.2297 | Avg Step-Time: 0.290 secs\n",
      "Loss at 25400th Epoch -> Train Set: 2.1617 | Valid Set: 2.2267 | Avg Step-Time: 0.290 secs\n",
      "Loss at 25500th Epoch -> Train Set: 2.1708 | Valid Set: 2.2343 | Avg Step-Time: 0.292 secs\n",
      "Loss at 25600th Epoch -> Train Set: 2.1682 | Valid Set: 2.2307 | Avg Step-Time: 0.293 secs\n",
      "Loss at 25700th Epoch -> Train Set: 2.1662 | Valid Set: 2.2309 | Avg Step-Time: 0.293 secs\n",
      "Loss at 25800th Epoch -> Train Set: 2.1652 | Valid Set: 2.2304 | Avg Step-Time: 0.292 secs\n",
      "Loss at 25900th Epoch -> Train Set: 2.1597 | Valid Set: 2.2281 | Avg Step-Time: 0.291 secs\n",
      "Loss at 26000th Epoch -> Train Set: 2.1657 | Valid Set: 2.2349 | Avg Step-Time: 0.291 secs\n",
      "Loss at 26100th Epoch -> Train Set: 2.1606 | Valid Set: 2.2303 | Avg Step-Time: 0.291 secs\n",
      "Loss at 26200th Epoch -> Train Set: 2.1617 | Valid Set: 2.2283 | Avg Step-Time: 0.292 secs\n",
      "Loss at 26300th Epoch -> Train Set: 2.1621 | Valid Set: 2.2289 | Avg Step-Time: 0.292 secs\n",
      "Loss at 26400th Epoch -> Train Set: 2.1611 | Valid Set: 2.2273 | Avg Step-Time: 0.292 secs\n",
      "Loss at 26500th Epoch -> Train Set: 2.1618 | Valid Set: 2.2306 | Avg Step-Time: 0.293 secs\n",
      "Loss at 26600th Epoch -> Train Set: 2.1650 | Valid Set: 2.2295 | Avg Step-Time: 0.291 secs\n",
      "Loss at 26700th Epoch -> Train Set: 2.1618 | Valid Set: 2.2276 | Avg Step-Time: 0.290 secs\n",
      "Loss at 26800th Epoch -> Train Set: 2.1645 | Valid Set: 2.2300 | Avg Step-Time: 0.290 secs\n",
      "Loss at 26900th Epoch -> Train Set: 2.1640 | Valid Set: 2.2311 | Avg Step-Time: 0.290 secs\n",
      "Loss at 27000th Epoch -> Train Set: 2.1630 | Valid Set: 2.2246 | Avg Step-Time: 0.291 secs\n",
      "Loss at 27100th Epoch -> Train Set: 2.1637 | Valid Set: 2.2238 | Avg Step-Time: 0.291 secs\n",
      "Loss at 27200th Epoch -> Train Set: 2.1528 | Valid Set: 2.2301 | Avg Step-Time: 0.293 secs\n",
      "Loss at 27300th Epoch -> Train Set: 2.1573 | Valid Set: 2.2273 | Avg Step-Time: 0.292 secs\n",
      "Loss at 27400th Epoch -> Train Set: 2.1574 | Valid Set: 2.2257 | Avg Step-Time: 0.290 secs\n",
      "Loss at 27500th Epoch -> Train Set: 2.1591 | Valid Set: 2.2223 | Avg Step-Time: 0.290 secs\n",
      "Loss at 27600th Epoch -> Train Set: 2.1534 | Valid Set: 2.2245 | Avg Step-Time: 0.290 secs\n",
      "Loss at 27700th Epoch -> Train Set: 2.1542 | Valid Set: 2.2263 | Avg Step-Time: 0.290 secs\n",
      "Loss at 27800th Epoch -> Train Set: 2.1526 | Valid Set: 2.2248 | Avg Step-Time: 0.291 secs\n",
      "Loss at 27900th Epoch -> Train Set: 2.1553 | Valid Set: 2.2238 | Avg Step-Time: 0.292 secs\n",
      "Loss at 28000th Epoch -> Train Set: 2.1557 | Valid Set: 2.2280 | Avg Step-Time: 0.291 secs\n",
      "Loss at 28100th Epoch -> Train Set: 2.1541 | Valid Set: 2.2200 | Avg Step-Time: 0.292 secs\n",
      "Loss at 28200th Epoch -> Train Set: 2.1524 | Valid Set: 2.2258 | Avg Step-Time: 0.290 secs\n",
      "Loss at 28300th Epoch -> Train Set: 2.1536 | Valid Set: 2.2192 | Avg Step-Time: 0.289 secs\n",
      "Loss at 28400th Epoch -> Train Set: 2.1531 | Valid Set: 2.2277 | Avg Step-Time: 0.290 secs\n",
      "Loss at 28500th Epoch -> Train Set: 2.1546 | Valid Set: 2.2211 | Avg Step-Time: 0.292 secs\n",
      "Loss at 28600th Epoch -> Train Set: 2.1570 | Valid Set: 2.2265 | Avg Step-Time: 0.291 secs\n",
      "Loss at 28700th Epoch -> Train Set: 2.1493 | Valid Set: 2.2239 | Avg Step-Time: 0.291 secs\n",
      "Loss at 28800th Epoch -> Train Set: 2.1425 | Valid Set: 2.2310 | Avg Step-Time: 0.292 secs\n",
      "Loss at 28900th Epoch -> Train Set: 2.1482 | Valid Set: 2.2202 | Avg Step-Time: 0.291 secs\n",
      "Loss at 29000th Epoch -> Train Set: 2.1482 | Valid Set: 2.2225 | Avg Step-Time: 0.289 secs\n",
      "Loss at 29100th Epoch -> Train Set: 2.1531 | Valid Set: 2.2237 | Avg Step-Time: 0.289 secs\n",
      "Loss at 29200th Epoch -> Train Set: 2.1491 | Valid Set: 2.2243 | Avg Step-Time: 0.290 secs\n",
      "Loss at 29300th Epoch -> Train Set: 2.1483 | Valid Set: 2.2198 | Avg Step-Time: 0.291 secs\n",
      "Loss at 29400th Epoch -> Train Set: 2.1449 | Valid Set: 2.2170 | Avg Step-Time: 0.291 secs\n",
      "Loss at 29500th Epoch -> Train Set: 2.1544 | Valid Set: 2.2197 | Avg Step-Time: 0.292 secs\n",
      "Loss at 29600th Epoch -> Train Set: 2.1503 | Valid Set: 2.2214 | Avg Step-Time: 0.292 secs\n",
      "Loss at 29700th Epoch -> Train Set: 2.1446 | Valid Set: 2.2207 | Avg Step-Time: 0.290 secs\n",
      "Loss at 29800th Epoch -> Train Set: 2.1473 | Valid Set: 2.2165 | Avg Step-Time: 0.289 secs\n",
      "Loss at 29900th Epoch -> Train Set: 2.1488 | Valid Set: 2.2170 | Avg Step-Time: 0.290 secs\n",
      "Loss at 30000th Epoch -> Train Set: 2.1464 | Valid Set: 2.2215 | Avg Step-Time: 0.290 secs\n",
      "\n",
      "-----------\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Novel Text on seed:\n",
      "\n",
      "\t Kate Beatty's lead cameo a deal of strange, and he thus Ruth offerishes per little misfortune. Grayson may be excellent and I can't little anothers of FAST through police. I'm sure that Lumier doesn't act. br /  br / Sky finally puts a bat on a life pomenally going, but doesn't want to put a thrill for her and Skywalker is awful, but so her emotion sucks. Her at least is about that point, awful people getting shouting mostly for me with the songs helpers. br /  br / Obviously well. This gets something to be missing down: \"If you're going to Campbell, you don't mind at all the ugly horror. Clark Focker\" is a bit one of the worst movie ever, and I still want to make a complete attention song to learn.\n",
      "And Zelda is picky, it's a retypical TV movie calone about the King assassin Jet, and the Enterprise of its original Shakti this otherwise the lead. However, I saw a number of the original three films - not that introducing this acting a hipschool trough to the match. My fellows: My god sa\n",
      "\n",
      "-----------\n",
      "\n",
      "Generating based on seed: This movie had amazing action sequences\n",
      "\n",
      "This movie had amazing action sequences and had the nasty turtle of it complete well) enough for an insult johns. The animation is awving, and isn't even very haphalf. I would not write a book on Edi Baker. I cant an average South character claw, but looking it simply stuck together instead. I think I know Woody were's a dramatic cololate language. Whether an author of Montgomery or Montfield on ANY HALF-NOTHING would consider Christopher Malkovin.\n",
      "The series was just not even considered special offensiveness, but I found the cops to justify waste . The Monsters with each other lurid, or because make the work of what different categories and employees or the mutlent first following or yeong gets more so than the work in the (yes, it was behind the box-with noisy sound tracks).  br /  br / Both info:-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "-----------\n",
      "\n",
      "Generating based on seed: The screenplay and direction of this movie was its saviour given the dismal\n",
      "\n",
      "The screenplay and direction of this movie was its saviour given the dismal campaign of screen for near teenage janitor Annie Greenwhils', and just flash-backs in the most neighbourgeois and just one comment baiting for a woman's eyes. For example, gentlemen \"Orica wind under the screen school\", the police with the cold bad jos, eyes they disposed her as one of those of the worst of the few hits are, looming band, shockers, etc. br /  br / Unlook at all for a Goreaus spat-calicing pub natical famine performance more.  br /  br / What soon wants to be? Taken off a character that should keep you thinking and turgent on is being pondecaptured here, and you don't get to do into and change this garbage. This praising to the traditional studios is flop in that orginal, this fact probably direction it does in the extreme movie, and the dire just seaoid sun throwing up there and the introration of an island to infamination. But aliens on probably would be on hands and a fool going in windows. Maybe the sun doesn't hurt on women alive. Some would have forgiven it. Thi\n",
      "\n",
      "-----------\n",
      "\n",
      "Generating based on seed: I absolutely enjoyed this movie it was\n",
      "\n",
      "I absolutely enjoyed this movie it was garbage. I cannot buy it that Happy Garbo has made 10 stairs. It is like the bunch of warnings of the horror music films by the crew (note that requires the versatility and limitations), but cast after their own private nostalgia tour extras (his voice is just glad-jaw of them.). The film relies only to a star daring tomb, and saying just on the adventure. br /  br / Robert Dm Fellinson did an excelemental job of preventing, but the film all surprises, with the bit of me and I have seen the plot before-- it was superb. Reading the book by the book in a somewhat off-crash ending, that's other stuff in which itless speaking turkey back seat would usually run through the background that the young side of a character, has where you are never be, not walking around with him. br /  br / a movie won thus be that Horneground film that is not commented too often this with Sanatosh Kurtight as a Fred Delay, for themselves it also marks: Unfortunately it was going to promote the less great excit\n",
      "\n",
      "-----------\n",
      "\n",
      "Model Architecture:\n",
      "\n",
      " MyGPT(\n",
      "  (token_embed): Embedding(77, 384)\n",
      "  (position_embed): Embedding(256, 384)\n",
      "  (decoder_blocks): Sequential(\n",
      "    (0): GPTDecoderBlock(\n",
      "      (multihead_attention): MultiHeadAttention(\n",
      "        (multiple_attention_heads): ModuleList(\n",
      "          (0-5): 6 x SelfAttentionHead(\n",
      "            (queries): Linear(in_features=384, out_features=64, bias=False)\n",
      "            (keys): Linear(in_features=384, out_features=64, bias=False)\n",
      "            (values): Linear(in_features=384, out_features=64, bias=False)\n",
      "            (dropout): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (dropout): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "      (ffwd): FeedForward(\n",
      "        (sequential): Sequential(\n",
      "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "          (3): Dropout(p=0.2, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (1): GPTDecoderBlock(\n",
      "      (multihead_attention): MultiHeadAttention(\n",
      "        (multiple_attention_heads): ModuleList(\n",
      "          (0-5): 6 x SelfAttentionHead(\n",
      "            (queries): Linear(in_features=384, out_features=64, bias=False)\n",
      "            (keys): Linear(in_features=384, out_features=64, bias=False)\n",
      "            (values): Linear(in_features=384, out_features=64, bias=False)\n",
      "            (dropout): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (dropout): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "      (ffwd): FeedForward(\n",
      "        (sequential): Sequential(\n",
      "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "          (3): Dropout(p=0.2, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (2): GPTDecoderBlock(\n",
      "      (multihead_attention): MultiHeadAttention(\n",
      "        (multiple_attention_heads): ModuleList(\n",
      "          (0-5): 6 x SelfAttentionHead(\n",
      "            (queries): Linear(in_features=384, out_features=64, bias=False)\n",
      "            (keys): Linear(in_features=384, out_features=64, bias=False)\n",
      "            (values): Linear(in_features=384, out_features=64, bias=False)\n",
      "            (dropout): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (dropout): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "      (ffwd): FeedForward(\n",
      "        (sequential): Sequential(\n",
      "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "          (3): Dropout(p=0.2, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (3): GPTDecoderBlock(\n",
      "      (multihead_attention): MultiHeadAttention(\n",
      "        (multiple_attention_heads): ModuleList(\n",
      "          (0-5): 6 x SelfAttentionHead(\n",
      "            (queries): Linear(in_features=384, out_features=64, bias=False)\n",
      "            (keys): Linear(in_features=384, out_features=64, bias=False)\n",
      "            (values): Linear(in_features=384, out_features=64, bias=False)\n",
      "            (dropout): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (dropout): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "      (ffwd): FeedForward(\n",
      "        (sequential): Sequential(\n",
      "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "          (3): Dropout(p=0.2, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (4): GPTDecoderBlock(\n",
      "      (multihead_attention): MultiHeadAttention(\n",
      "        (multiple_attention_heads): ModuleList(\n",
      "          (0-5): 6 x SelfAttentionHead(\n",
      "            (queries): Linear(in_features=384, out_features=64, bias=False)\n",
      "            (keys): Linear(in_features=384, out_features=64, bias=False)\n",
      "            (values): Linear(in_features=384, out_features=64, bias=False)\n",
      "            (dropout): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (dropout): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "      (ffwd): FeedForward(\n",
      "        (sequential): Sequential(\n",
      "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "          (3): Dropout(p=0.2, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (5): GPTDecoderBlock(\n",
      "      (multihead_attention): MultiHeadAttention(\n",
      "        (multiple_attention_heads): ModuleList(\n",
      "          (0-5): 6 x SelfAttentionHead(\n",
      "            (queries): Linear(in_features=384, out_features=64, bias=False)\n",
      "            (keys): Linear(in_features=384, out_features=64, bias=False)\n",
      "            (values): Linear(in_features=384, out_features=64, bias=False)\n",
      "            (dropout): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (dropout): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "      (ffwd): FeedForward(\n",
      "        (sequential): Sequential(\n",
      "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "          (3): Dropout(p=0.2, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (6): GPTDecoderBlock(\n",
      "      (multihead_attention): MultiHeadAttention(\n",
      "        (multiple_attention_heads): ModuleList(\n",
      "          (0-5): 6 x SelfAttentionHead(\n",
      "            (queries): Linear(in_features=384, out_features=64, bias=False)\n",
      "            (keys): Linear(in_features=384, out_features=64, bias=False)\n",
      "            (values): Linear(in_features=384, out_features=64, bias=False)\n",
      "            (dropout): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (dropout): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "      (ffwd): FeedForward(\n",
      "        (sequential): Sequential(\n",
      "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "          (3): Dropout(p=0.2, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (final_ln): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "  (last_linear): Linear(in_features=384, out_features=77, bias=True)\n",
      ")\n",
      "\n",
      "-----------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The Main Function\n",
    "def main():\n",
    "\n",
    "    # Downloading and Loading the Dataset\n",
    "    imdb_review_dataset = IMDBMovieReview()\n",
    "\n",
    "    # Filtering the columns of the Dataset\n",
    "    review_string = imdb_review_dataset.refine_structure()\n",
    "\n",
    "    # Loading the GPT Model Class\n",
    "    gpt_model = MyGPT(imdb_review_dataset.vocab, n_embd=384, block_size=256, decoder_layers=7, attention_heads=6)\n",
    "    gpt_model = torch.nn.DataParallel(gpt_model)\n",
    "    gpt_model.to(device=acc_device)\n",
    "\n",
    "    # Creating the data tensor\n",
    "    data_preprocessor = DataPreprocessor(review_string, device=acc_device)\n",
    "    data_preprocessor.create_data_tensor(gpt_model)\n",
    "    \n",
    "    # Splitting the data tensor\n",
    "    train_set, valid_set, _ = data_preprocessor.train_valid_test()\n",
    "\n",
    "    # Creating the optimization loop\n",
    "    optim_handle = OptimizationLoop(data_preprocessor, gpt_model, learning_rate=6e-4)\n",
    "\n",
    "    # Datastring Metrics\n",
    "    print(\"Datastring Metrics after loading the dataset on Vocab:\")\n",
    "    imdb_review_dataset.datastring_metrics()\n",
    "    print(\"\\n-----------\\n\")\n",
    "\n",
    "    # Vocabulary used by the model\n",
    "    print(\"Model Vocabulary from the Character-Level Tokenizer:\\n\", imdb_review_dataset.vocab)\n",
    "    print(\"\\n-----------\\n\")\n",
    "    \n",
    "    # Training the model\n",
    "    print(\"Training the Model:\\n\")\n",
    "    train_losses, valid_losses = optim_handle.train(30000, train_set, valid_set, batch_size=128, block_size=256)\n",
    "    print(\"\\n-----------\\n\")\n",
    "\n",
    "    # Plotting the Losses\n",
    "    plt.plot(range(1, len(train_losses) + 1), train_losses, label=\"Training Loss\", c=\"g\", ls=\"-\")\n",
    "    plt.plot(range(1, len(valid_losses) + 1), valid_losses, label=\"Validation Loss\", c=\"b\", ls=\"-.\")\n",
    "    plt.title(\"Learning Curve\")\n",
    "    plt.xlabel(\"Epochs in (100x)\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.show()\n",
    "\n",
    "    # Generating Text with the model\n",
    "    print(\"Generating Novel Text on seed:\\n\")\n",
    "    idx = torch.zeros((1, 1), dtype=torch.long, device=acc_device)\n",
    "    output_tokens = gpt_model.module.generate(previous_tokens=idx, max_tokens=1000)[0].tolist()\n",
    "    output_sentence = gpt_model.module.decode(output_tokens)\n",
    "    print(output_sentence)\n",
    "    print(\"\\n-----------\\n\")\n",
    "\n",
    "    print(\"Generating based on seed: This movie had amazing action sequences\\n\")\n",
    "    action_prompt = torch.tensor(gpt_model.module.encode(\"This movie had amazing action sequences\"), device=acc_device)\n",
    "    action_prompt = torch.unsqueeze(action_prompt, 0)\n",
    "    output_tokens = gpt_model.module.generate(previous_tokens=action_prompt, max_tokens=1000)[0].tolist()\n",
    "    output_sentence = gpt_model.module.decode(output_tokens)\n",
    "    print(output_sentence)\n",
    "    print(\"\\n-----------\\n\")\n",
    "\n",
    "    print(\"Generating based on seed: The screenplay and direction of this movie was its saviour given the dismal\\n\")\n",
    "    drama_prompt = torch.tensor(gpt_model.module.encode(\"The screenplay and direction of this movie was its saviour given the dismal\"), device=acc_device)\n",
    "    drama_prompt = torch.unsqueeze(drama_prompt, 0)\n",
    "    output_tokens = gpt_model.module.generate(previous_tokens=drama_prompt, max_tokens=1000)[0].tolist()\n",
    "    output_sentence = gpt_model.module.decode(output_tokens)\n",
    "    print(output_sentence)\n",
    "    print(\"\\n-----------\\n\")\n",
    "\n",
    "    print(\"Generating based on seed: I absolutely enjoyed this movie it was\\n\")\n",
    "    excited_prompt = torch.tensor(gpt_model.module.encode(\"I absolutely enjoyed this movie it was\"), device=acc_device)\n",
    "    excited_prompt = torch.unsqueeze(excited_prompt, 0)\n",
    "    output_tokens = gpt_model.module.generate(previous_tokens=excited_prompt, max_tokens=1000)[0].tolist()\n",
    "    output_sentence = gpt_model.module.decode(output_tokens)\n",
    "    print(output_sentence)\n",
    "    print(\"\\n-----------\\n\")\n",
    "\n",
    "    # Storing the weights of the model\n",
    "    gpt_model.module.save_model()\n",
    "\n",
    "    # Model Architecture\n",
    "    print(\"Model Architecture:\\n\\n\", gpt_model.module)\n",
    "    print(\"\\n-----------\\n\")\n",
    "\n",
    "    del gpt_model\n",
    "\n",
    "\n",
    "# Driver code\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f99c2c4",
   "metadata": {
    "papermill": {
     "duration": 0.014651,
     "end_time": "2025-08-31T18:00:42.985737",
     "exception": false,
     "start_time": "2025-08-31T18:00:42.971086",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Logs at $5e^{-4}$\n",
    "\n",
    "- Loss at 100th Epoch -> Train Set: 5.3759 | Valid Set: 5.3351 | Avg Step-Time: 0.271 secs\n",
    "- Loss at 200th Epoch -> Train Set: 4.9903 | Valid Set: 4.9943 | Avg Step-Time: 0.266 secs\n",
    "- Loss at 300th Epoch -> Train Set: 4.8890 | Valid Set: 4.8899 | Avg Step-Time: 0.266 secs\n",
    "- Loss at 400th Epoch -> Train Set: 4.6155 | Valid Set: 4.6205 | Avg Step-Time: 0.269 secs\n",
    "- Loss at 500th Epoch -> Train Set: 4.1610 | Valid Set: 4.1612 | Avg Step-Time: 0.268 secs\n",
    "- Loss at 600th Epoch -> Train Set: 3.8432 | Valid Set: 3.8429 | Avg Step-Time: 0.265 secs\n",
    "- Loss at 700th Epoch -> Train Set: 3.6180 | Valid Set: 3.6264 | Avg Step-Time: 0.265 secs\n",
    "- Loss at 800th Epoch -> Train Set: 3.4639 | Valid Set: 3.4642 | Avg Step-Time: 0.265 secs\n",
    "- Loss at 900th Epoch -> Train Set: 3.3412 | Valid Set: 3.3418 | Avg Step-Time: 0.265 secs\n",
    "- Loss at 1000th Epoch -> Train Set: 3.2381 | Valid Set: 3.2455 | Avg Step-Time: 0.267 secs\n",
    "- Loss at 1100th Epoch -> Train Set: 3.1563 | Valid Set: 3.1662 | Avg Step-Time: 0.267 secs\n",
    "- Loss at 1200th Epoch -> Train Set: 3.0901 | Valid Set: 3.0935 | Avg Step-Time: 0.268 secs\n",
    "- Loss at 1300th Epoch -> Train Set: 3.0307 | Valid Set: 3.0413 | Avg Step-Time: 0.269 secs\n",
    "- Loss at 1400th Epoch -> Train Set: 2.9781 | Valid Set: 2.9905 | Avg Step-Time: 0.265 secs\n",
    "- Loss at 1500th Epoch -> Train Set: 2.9406 | Valid Set: 2.9490 | Avg Step-Time: 0.265 secs\n",
    "- Loss at 1600th Epoch -> Train Set: 2.9018 | Valid Set: 2.9056 | Avg Step-Time: 0.265 secs\n",
    "- Loss at 1700th Epoch -> Train Set: 2.8737 | Valid Set: 2.8783 | Avg Step-Time: 0.263 secs\n",
    "- Loss at 1800th Epoch -> Train Set: 2.8390 | Valid Set: 2.8509 | Avg Step-Time: 0.263 secs\n",
    "- Loss at 1900th Epoch -> Train Set: 2.8109 | Valid Set: 2.8230 | Avg Step-Time: 0.266 secs\n",
    "- Loss at 2000th Epoch -> Train Set: 2.7857 | Valid Set: 2.7912 | Avg Step-Time: 0.267 secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27adb30",
   "metadata": {
    "papermill": {
     "duration": 0.014058,
     "end_time": "2025-08-31T18:00:43.013979",
     "exception": false,
     "start_time": "2025-08-31T18:00:42.999921",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 447516,
     "sourceId": 849658,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 25715.133651,
   "end_time": "2025-08-31T18:00:45.349948",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-08-31T10:52:10.216297",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
