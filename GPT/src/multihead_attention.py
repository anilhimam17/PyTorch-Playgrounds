import torch

from src.self_attention import SelfAttentionHead

class MultiHeadAttention(torch.nn.Module):
    """Implements a Multi Headed Attention Module.

    Where each attention head is a parallel implementation of a distinct Self Attention Head.
    The Multi Headed Attention Module runs several Self Attention Heads in parallel and concatenates 
    the final output generated by each head on the Channel Dimension."""

    def __init__(self, num_heads: int, head_size: int, block_size: int, n_embd: int) -> None:
        super().__init__()

        # Parallel Self Attention Heads
        self.multiple_attention_heads = torch.nn.ModuleList(
            [
                SelfAttentionHead(head_size=head_size, input_features=n_embd, block_size=block_size)
                for _ in range(num_heads)
            ]
        )

    def forward(self, X) -> torch.Tensor:
        """Implements the forward propagation of the MultiHead Attention Layer."""
        return torch.cat(
            [head(X) for head in self.multiple_attention_heads], dim=-1
        )