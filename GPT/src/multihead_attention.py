import torch

from src.self_attention import SelfAttentionHead

class MultiHeadAttention(torch.nn.Module):
    """Implements a Multi Headed Attention Module.

    Where each attention head is a parallel implementation of a distinct Self Attention Head.
    The Multi Headed Attention Module runs several Self Attention Heads in parallel and concatenates 
    the final output generated by each head on the Channel Dimension."""

    def __init__(self, num_heads: int, head_size: int, block_size: int, n_embd: int, dropout_rate: float = 0.2) -> None:
        super().__init__()

        # Parallel Self Attention Heads
        self.multiple_attention_heads = torch.nn.ModuleList(
            [
                SelfAttentionHead(head_size=head_size, input_features=n_embd, block_size=block_size)
                for _ in range(num_heads)
            ]
        )
        
        # Dropout Layer
        self.dropout = torch.nn.Dropout(p=dropout_rate)

    def forward(self, X) -> torch.Tensor:
        """Implements the forward propagation of the MultiHead Attention Layer."""
        
        multihead_attention_pattern = torch.cat(
            [head(X) for head in self.multiple_attention_heads], dim=-1
        )
        out_attention = self.dropout(multihead_attention_pattern)

        return out_attention